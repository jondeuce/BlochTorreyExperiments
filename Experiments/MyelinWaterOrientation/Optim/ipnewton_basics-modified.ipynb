{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nonlinear constrained optimization interface in\n",
    "`Optim` assumes that the user can write the optimization\n",
    "problem in the following way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\min_{x\\in\\mathbb{R}^n} f(x) \\quad \\text{such that}\\\\\n",
    "l_x \\leq \\phantom{c(}x\\phantom{)} \\leq u_x \\\\\n",
    "l_c \\leq c(x) \\leq u_c.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For equality constraints on ``x_j`` or ``c(x)_j`` you set those\n",
    "particular entries of bounds to be equal, ``l_j=u_j``.\n",
    "Likewise, setting ``l_j=-\\infty`` or ``u_j=\\infty`` means that the\n",
    "constraint is unbounded from below or above respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim, NLSolversBase #hide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained optimization with `IPNewton`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through examples on how to use the constraints interface\n",
    "with the interior-point Newton optimization algorithm [IPNewton](../../algo/ipnewton.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout these examples we work with the standard Rosenbrock function.\n",
    "The objective and its derivatives are given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "\n",
    "function fun_grad!(g, x)\n",
    "g[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n",
    "g[2] = 200.0 * (x[2] - x[1]^2)\n",
    "end\n",
    "\n",
    "function fun_hess!(h, x)\n",
    "h[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n",
    "h[1, 2] = -400.0 * x[1]\n",
    "h[2, 1] = -400.0 * x[1]\n",
    "h[2, 2] = 200.0\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization interface\n",
    "To solve a constrained optimization problem we call the `optimize`\n",
    "method\n",
    "``` julia\n",
    "optimize(d::AbstractObjective, constraints::AbstractConstraints, initial_x::Tx, method::ConstrainedOptimizer, options::Options)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create instances of `AbstractObjective` and\n",
    "`AbstractConstraints` using the types `TwiceDifferentiable` and\n",
    "`TwiceDifferentiableConstraints` from the package `NLSolversBase.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box minimzation\n",
    "We want to optimize the Rosenbrock function in the box\n",
    "``-0.5 \\leq x \\leq 0.5``, starting from the point ``x_0=(0,0)``.\n",
    "Box constraints are defined using, for example,\n",
    "`TwiceDifferentiableConstraints(lx, ux)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: IPNewton not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: IPNewton not defined\u001b[39m",
      ""
     ]
    }
   ],
   "source": [
    "x0 = [0.0, 0.0]\n",
    "df = TwiceDifferentiable(fun, fun_grad!, fun_hess!, x0)\n",
    "\n",
    "lx = [-0.5, -0.5]; ux = [0.5, 0.5]\n",
    "dfc = TwiceDifferentiableConstraints(lx, ux)\n",
    "\n",
    "res = optimize(df, dfc, x0, IPNewton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only want to set lower bounds, use `ux = fill(Inf, 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Interior Point Newton\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999999998342594,0.9999999996456271]\n",
       " * Minimum: 7.987239e-20\n",
       " * Iterations: 35\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 0.0e+00: false \n",
       "     |x - x'| = 3.54e-10 \n",
       "   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n",
       "     |f(x) - f(x')| = 3.00e+00 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: true \n",
       "     |g(x)| = 8.83e-09 \n",
       "   * Stopped by an increasing objective: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 63\n",
       " * Gradient Calls: 63"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ux = fill(Inf, 2)\n",
    "dfc = TwiceDifferentiableConstraints(lx, ux)\n",
    "\n",
    "clear!(df)\n",
    "res = optimize(df, dfc, x0, IPNewton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining \"unconstrained\" problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An unconstrained problem can be defined either by passing\n",
    "`Inf` bounds or empty arrays.\n",
    "**Note that we must pass the correct type information to the empty `lx` and `ux`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Interior Point Newton\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999999992619217,0.9999999985003628]\n",
       " * Minimum: 5.998937e-19\n",
       " * Iterations: 34\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 0.0e+00: false \n",
       "     |x - x'| = 1.50e-09 \n",
       "   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n",
       "     |f(x) - f(x')| = 3.00e+00 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: true \n",
       "     |g(x)| = 7.92e-09 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 63\n",
       " * Gradient Calls: 63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lx = fill(-Inf, 2); ux = fill(Inf, 2)\n",
    "dfc = TwiceDifferentiableConstraints(lx, ux)\n",
    "\n",
    "clear!(df)\n",
    "res = optimize(df, dfc, x0, IPNewton())\n",
    "\n",
    "lx = Float64[]; ux = Float64[]\n",
    "dfc = TwiceDifferentiableConstraints(lx, ux)\n",
    "\n",
    "clear!(df)\n",
    "res = optimize(df, dfc, x0, IPNewton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic nonlinear constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider the Rosenbrock problem with a constraint on\n",
    "\\begin{equation}\n",
    "   c(x)_1 = x_1^2 + x_2^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the information about the constraints to `optimize`\n",
    "by defining a vector function `c(x)` and its Jacobian `J(x)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian information is treated differently, by considering the\n",
    "Lagrangian of the corresponding slack-variable transformed\n",
    "optimization problem. This is similar to how the [CUTEst\n",
    "library](https://github.com/JuliaSmoothOptimizers/CUTEst.jl) works.\n",
    "Let ``H_j(x)`` represent the Hessian of the ``j``th component\n",
    "``c(x)_j`` of the generic constraints.\n",
    "and ``\\lambda_j`` the corresponding dual variable in the\n",
    "Lagrangian. Then we want the `constraint` object to\n",
    "add the values of ``H_j(x)`` to the Hessian of the objective,\n",
    "weighted by ``\\lambda_j``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Julian form for the supplied function ``c(x)`` and the derivative\n",
    "information is then added in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_c!(c, x) = (c[1] = x[1]^2 + x[2]^2; c)\n",
    "function con_jacobian!(J, x)\n",
    "    J[1,1] = 2*x[1]\n",
    "    J[1,2] = 2*x[2]\n",
    "    J\n",
    "end\n",
    "function con_h!(h, x, λ)\n",
    "    h[1,1] += λ[1]*2\n",
    "    h[2,2] += λ[1]*2\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that `con_h!` adds the `λ`-weighted Hessian value of each\n",
    "element of `c(x)` to the Hessian of `fun`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then optimize the Rosenbrock function inside the ball of radius\n",
    "``0.5``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Interior Point Newton\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.45564896414551875,0.2058737998704899]\n",
       " * Minimum: 2.966216e-01\n",
       " * Iterations: 28\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 0.0e+00: true \n",
       "     |x - x'| = 0.00e+00 \n",
       "   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n",
       "     |f(x) - f(x')| = 0.00e+00 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 7.71e-01 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 109\n",
       " * Gradient Calls: 109"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lx = Float64[]; ux = Float64[]\n",
    "lc = [-Inf]; uc = [0.5^2]\n",
    "dfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n",
    "                                     lx, ux, lc, uc)\n",
    "res = optimize(df, dfc, x0, IPNewton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a lower bound on the constraint, and thus\n",
    "optimize the objective on the annulus with\n",
    "inner and outer radii ``0.1`` and ``0.5`` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Initial guess is not an interior point\n",
      "\n",
      "Stacktrace:\n",
      " [1] initial_state(::Optim.IPNewton{Optim.#backtrack_constrained_grad,Symbol}, ::Optim.Options{Float64,Void}, ::NLSolversBase.TwiceDifferentiable{Float64,Array{Float64,1},Array{Float64,2},Array{Float64,1}}, ::NLSolversBase.TwiceDifferentiableConstraints{##1689.#con_c!,##1689.#con_jacobian!,##1689.#con_h!,Float64}, ::Array{Float64,1}) at /home/travis/.julia/v0.6/Optim/src/multivariate/solvers/constrained/ipnewton/ipnewton.jl:112\n",
      " [2] optimize(::NLSolversBase.TwiceDifferentiable{Float64,Array{Float64,1},Array{Float64,2},Array{Float64,1}}, ::NLSolversBase.TwiceDifferentiableConstraints{##1689.#con_c!,##1689.#con_jacobian!,##1689.#con_h!,Float64}, ::Array{Float64,1}, ::Optim.IPNewton{Optim.#backtrack_constrained_grad,Symbol}) at /home/travis/.julia/v0.6/Optim/src/multivariate/solvers/constrained/ipnewton/interior.jl:196\n",
      " [3] include_string(::String, ::String) at ./loading.jl:522\n",
      " [4] include_string(::Module, ::String) at /home/travis/.julia/v0.6/Compat/src/Compat.jl:90\n",
      " [5] (::Literate.Documenter.##2#3{Literate.##20#21{Module},Base.PipeEndpoint,Base.PipeEndpoint,Pipe,Array{UInt8,1}})() at /home/travis/.julia/v0.6/Literate/src/Documenter.jl:41\n",
      " [6] withoutput(::Literate.##20#21{Module}) at /home/travis/.julia/v0.6/Literate/src/Documenter.jl:39\n",
      " [7] execute_notebook(::Dict{Any,Any}) at /home/travis/.julia/v0.6/Literate/src/Literate.jl:500\n",
      " [8] #15 at /home/travis/.julia/v0.6/Literate/src/Literate.jl:466 [inlined]\n",
      " [9] cd(::Literate.##15#19, ::String) at ./file.jl:70\n",
      " [10] #notebook#11(::Base.#identity, ::Base.#identity, ::Bool, ::Bool, ::Bool, ::String, ::Array{Any,1}, ::Function, ::String, ::String) at /home/travis/.julia/v0.6/Literate/src/Literate.jl:465\n",
      " [11] (::Literate.#kw##notebook)(::Array{Any,1}, ::Literate.#notebook, ::String, ::String) at ./<missing>:0\n",
      " [12] macro expansion at /home/travis/.julia/v0.6/Optim/test/../docs/generate.jl:18 [inlined]\n",
      " [13] anonymous at ./<missing>:?\n",
      " [14] include_from_node1(::String) at ./loading.jl:576\n",
      " [15] include(::String) at ./sysimg.jl:14\n",
      " [16] include_from_node1(::String) at ./loading.jl:576\n",
      " [17] include(::String) at ./sysimg.jl:14\n",
      " [18] include_from_node1(::String) at ./loading.jl:576\n",
      " [19] include(::String) at ./sysimg.jl:14\n",
      " [20] process_options(::Base.JLOptions) at ./client.jl:305\n",
      " [21] _start() at ./client.jl:371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Interior Point Newton\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.45564896414551953,0.20587379987049065]\n",
       " * Minimum: 2.966216e-01\n",
       " * Iterations: 34\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 0.0e+00: true \n",
       "     |x - x'| = 0.00e+00 \n",
       "   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n",
       "     |f(x) - f(x')| = 0.00e+00 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: false \n",
       "     |g(x)| = 7.71e-01 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 158\n",
       " * Gradient Calls: 158"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc = [0.1^2]\n",
    "dfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n",
    "                                     lx, ux, lc, uc)\n",
    "res = optimize(df, dfc, x0, IPNewton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the algorithm warns that the Initial guess is not an\n",
    "interior point.** `IPNewton` can often handle this, however, if the\n",
    "initial guess is such that `c(x) = u_c`, then the algorithm currently\n",
    "fails. We may fix this in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple constraints\n",
    "The following example illustrates how to add an additional constraint.\n",
    "In particular, we add a constraint function\n",
    "\\begin{equation}\n",
    "   c(x)_2 = x_2\\sin(x_1)-x_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function con2_c!(c, x)\n",
    "    c[1] = x[1]^2 + x[2]^2     ## First constraint\n",
    "    c[2] = x[2]*sin(x[1])-x[1] ## Second constraint\n",
    "    c\n",
    "end\n",
    "function con2_jacobian!(J, x)\n",
    "    # First constraint\n",
    "    J[1,1] = 2*x[1]\n",
    "    J[1,2] = 2*x[2]\n",
    "    # Second constraint\n",
    "    J[2,1] = x[2]*cos(x[1])-1.0\n",
    "    J[2,2] = sin(x[1])\n",
    "    J\n",
    "end\n",
    "function con2_h!(h, x, λ)\n",
    "    # First constraint\n",
    "    h[1,1] += λ[1]*2\n",
    "    h[2,2] += λ[1]*2\n",
    "    # Second constraint\n",
    "    h[1,1] += λ[2]*x[2]*-sin(x[1])\n",
    "    h[1,2] += λ[2]*cos(x[1])\n",
    "    # Symmetrize h\n",
    "    h[2,1]  = h[1,2]\n",
    "    h\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the constraint objects and call `IPNewton` with\n",
    "initial guess ``x_0 = (0.25,0.25)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Interior Point Newton\n",
       " * Starting Point: [0.25,0.25]\n",
       " * Minimizer: [-1.595442184049669e-19,-1.9465281383022617e-18, ...]\n",
       " * Minimum: 1.000000e+00\n",
       " * Iterations: 29\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 0.0e+00: false \n",
       "     |x - x'| = 6.90e-10 \n",
       "   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n",
       "     |f(x) - f(x')| = 1.38e-09 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: true \n",
       "     |g(x)| = 2.00e+00 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 215\n",
       " * Gradient Calls: 215"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = [0.25, 0.25]\n",
    "lc = [-Inf, 0.0]; uc = [0.5^2, 0.0]\n",
    "dfc = TwiceDifferentiableConstraints(con2_c!, con2_jacobian!, con2_h!,\n",
    "                                     lx, ux, lc, uc)\n",
    "res = optimize(df, dfc, x0, IPNewton())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.4",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
