# Initialize generator + discriminator + kernel
function make_mmd_cvae_models(phys::PhysicsModel{Float32}, settings::Dict{String,Any}, models = Dict{String, Any}(), derived = Dict{String, Any}())
    n   = nsignal(phys) # input signal length
    nÎ¸  = ntheta(phys) # number of physics variables
    Î¸bd = Î¸bounds(phys)
    k   = settings["arch"]["nlatent"]::Int # number of latent variables Z
    nz  = settings["arch"]["zdim"]::Int # embedding dimension
    Î´   = settings["arch"]["genatr"]["maxcorr"]::Float64
    Ïƒbd = settings["arch"]["genatr"]["noisebounds"]::Vector{Float64} |> bd -> (bd...,)::NTuple{2,Float64}

    #TODO: only works for Latent(*)Corrector family
    RiceGenType = LatentScalarRicianNoiseCorrector{n,k}
    # RiceGenType = LatentVectorRicianNoiseCorrector{n,k}
    # RiceGenType = LatentVectorRicianCorrector{n,k}
    # RiceGenType = VectorRicianCorrector{n,k}

    OutputScale = let
        RiceGenType <: Union{<:VectorRicianCorrector, <:LatentVectorRicianCorrector} ? MMDLearning.CatScale([(-Î´, Î´), Ïƒbd], [n,n]) :
        RiceGenType <: FixedNoiseVectorRicianCorrector ? MMDLearning.CatScale([(-Î´, Î´)], [n]) :
        RiceGenType <: LatentVectorRicianNoiseCorrector ? MMDLearning.CatScale([Ïƒbd], [n]) :
        RiceGenType <: LatentScalarRicianNoiseCorrector ? MMDLearning.CatScale([Ïƒbd], [1]) :
        error("Unsupported corrector type: $RiceGenType")
    end

    # Physics model input variables prior
    get!(models, "theta_prior") do
        hdim = settings["arch"]["genatr"]["hdim"]::Int
        ktheta = settings["arch"]["genatr"]["ktheta"]::Int
        nhidden = settings["arch"]["genatr"]["nhidden"]::Int
        leakyslope = settings["arch"]["genatr"]["leakyslope"]::Float64
        Ïƒinner = leakyslope == 0 ? Flux.relu : eltype(phys)(leakyslope) |> a -> (x -> Flux.leakyrelu(x, a))
        Flux.Chain(
            MMDLearning.MLP(ktheta => nÎ¸, nhidden, hdim, Ïƒinner, tanh)...,
            MMDLearning.CatScale(Î¸bd, ones(Int, nÎ¸)),
        ) |> to32
    end

    # Latent variable prior
    get!(models, "latent_prior") do
        hdim = settings["arch"]["genatr"]["hdim"]::Int
        klatent = settings["arch"]["genatr"]["klatent"]::Int
        nhidden = settings["arch"]["genatr"]["nhidden"]::Int
        leakyslope = settings["arch"]["genatr"]["leakyslope"]::Float64
        Ïƒinner = leakyslope == 0 ? Flux.relu : eltype(phys)(leakyslope) |> a -> (x -> Flux.leakyrelu(x, a))
        Flux.Chain(
            MMDLearning.MLP(klatent => k, nhidden, hdim, Ïƒinner, tanh)...,
            deepcopy(OutputScale),
        ) |> to32
    end

    # Rician generator mapping Z variables from prior space to Rician parameter space
    get!(models, "genatr") do
        if k == 1
            return Flux.Chain(identity) # Latent space outputs noise level directly
        else
            error("nlatent = $k not implemented")
        end

        # #TODO: only works for LatentVectorRicianNoiseCorrector
        # @assert nin == k == nlatent(RiceGenType) && nout == n
        # Flux.Chain(
        #     # position encoding
        #     Z -> vcat(Z, zeros_similar(Z, 1, size(Z,2))),   # [k x b] -> [(k+1) x b]
        #     Z -> repeat(Z, n, 1),                           # [(k+1) x b] -> [(k+1)*n x b]
        #     NotTrainable(Flux.Diagonal(ones((k+1)*n), vec(vcat(zeros(k, n), uniform_range(n)')))),
        #     Z -> reshape(Z, k+1, :),                        # [(k+1)*n x b] -> [(k+1) x n*b]
        #     # position-wise mlp
        #     MMDLearning.MLP(k+1 => 1, nhidden, hdim, Ïƒinner, tanh)..., # [(k+1) x n*b] -> [1 x n*b]
        #     # output scaling
        #     Z -> reshape(Z, n, :),                          # [1 x n*b] -> [n x b]
        #     OutputScale,
        # ) |> to32
    end

    # Wrapped generator produces ð‘^2n outputs parameterizing n Rician distributions
    get!(derived, "ricegen") do
        R = RiceGenType(models["genatr"])
        normalizer = X -> maximum(X; dims = 1) #TODO: normalize by mean? sum? maximum? first echo?
        noisescale = X -> mean(X; dims = 1) #TODO: relative to mean? nothing?
        NormalizedRicianCorrector(R, normalizer, noisescale)
    end

    # Deep prior by physics model
    get!(derived, "prior") do
        default_Î¸prior(x) = sampleÎ¸prior(phys, typeof(x), size(x,2))
        # default_Zprior(x) = randn_similar(x, k, size(x,2))
        default_Zprior(x) = ((lo,hi) = eltype(x).(Ïƒbd); return lo .+ (hi .- lo) .* rand_similar(x, k, size(x,2)))
        deepÎ¸prior = get!(settings["train"], "DeepThetaPrior", false)::Bool
        deepZprior = get!(settings["train"], "DeepLatentPrior", false)::Bool
        ktheta = get!(settings["arch"]["genatr"], "ktheta", 0)::Int
        klatent = get!(settings["arch"]["genatr"], "klatent", 0)::Int
        DeepPriorRicianPhysicsModel{Float32,ktheta,klatent}(
            phys,
            derived["ricegen"],
            !deepÎ¸prior || ktheta == 0 ? default_Î¸prior : models["theta_prior"],
            !deepZprior || klatent == 0 ? default_Zprior : models["latent_prior"],
        )
    end

    # Encoders
    get!(models, "enc1") do
        hdim = settings["arch"]["enc1"]["hdim"]::Int
        nhidden = settings["arch"]["enc1"]["nhidden"]::Int
        psize = settings["arch"]["enc1"]["psize"]::Int
        head = settings["arch"]["enc1"]["head"]::Int
        MMDLearning.MLP(n => 2*nz, nhidden, hdim, Flux.relu, identity) |> to32
        # Transformers.Stack(
        #     Transformers.@nntopo( X : X => H : H => Î¼r ),
        #     TransformerEncoder(; n, psize, head, hdim, nhidden),
        #     MMDLearning.MLP(psize*n => 2*nz, 0, hdim, Flux.relu, identity),
        # ) |> to32
    end

    get!(models, "enc2") do
        hdim = settings["arch"]["enc2"]["hdim"]::Int
        nhidden = settings["arch"]["enc2"]["nhidden"]::Int
        psize = settings["arch"]["enc2"]["psize"]::Int
        head = settings["arch"]["enc2"]["head"]::Int
        MMDLearning.MLP(n + nÎ¸ + k => 2*nz, nhidden, hdim, Flux.relu, identity) |> to32
        # Transformers.Stack(
        #     Transformers.@nntopo( (X,Î¸,Z) : X => H : (H,Î¸,Z) => HÎ¸Z : HÎ¸Z => Î¼q ),
        #     TransformerEncoder(; n, psize, head, hdim, nhidden),
        #     vcat,
        #     MMDLearning.MLP(psize*n + nÎ¸ + k => 2*nz, 0, hdim, Flux.relu, identity),
        # ) |> to32
    end

    # Decoder
    get!(models, "dec") do
        hdim = settings["arch"]["dec"]["hdim"]::Int
        nhidden = settings["arch"]["dec"]["nhidden"]::Int
        Flux.Chain(
            MMDLearning.MLP(n + nz => 2*(nÎ¸ + k), nhidden, hdim, Flux.relu, identity)...,
            MMDLearning.CatScale(eltype(Î¸bd)[Î¸bd; (-1, 1)], [ones(Int, nÎ¸); k + nÎ¸ + k]),
        ) |> to32
    end

    # Discriminator
    get!(models, "discrim") do
        hdim = settings["arch"]["discrim"]["hdim"]::Int
        nhidden = settings["arch"]["discrim"]["nhidden"]::Int
        dropout = settings["arch"]["discrim"]["dropout"]::Float64
        chunk = settings["train"]["transform"]["chunk"]::Int
        order = get!(settings["train"]["augment"], "fdcat", 0)::Int #TODO
        augsizes = Dict{String,Int}(["signal" => n, "gradient" => n-1, "laplacian" => n-2, "encoderspace" => nz, "residuals" => n, "fftcat" => 2*(nÃ·2 + 1), "fftsplit" => 2*(nÃ·2 + 1), "fdcat" => sum(n-i for i in 0:order)])
        nin = sum((s -> ifelse(settings["train"]["augment"][s]::Union{Int,Bool} > 0, min(augsizes[s], chunk), 0)).(keys(augsizes))) #TODO > 0 hack works for both boolean and integer flags
        MMDLearning.MLP(nin => 1, nhidden, hdim, Flux.relu, Flux.sigmoid; dropout) |> to32
    end

    # CVAE
    get!(derived, "cvae") do; CVAE{n,nÎ¸,k,nz}(models["enc1"], models["enc2"], models["dec"]) end

    # Misc. useful operators
    get!(derived, "forwarddiff") do; MMDLearning.ForwardDifferemce() |> to32 end
    get!(derived, "laplacian") do; MMDLearning.Laplacian() |> to32 end
    get!(derived, "fdcat") do
        order = get!(settings["train"]["augment"], "fdcat", 0)::Int #TODO
        A = I(n) |> Matrix{Float64}
        FD = LinearAlgebra.diagm(n-1, n, 0 => -ones(n-1), 1 => ones(n-1))
        A = mapfoldl(vcat, 1:order; init = A) do i
            A = @views FD[1:end-i+1, 1:end-i+1] * A
        end
        NotTrainable(Flux.Dense(A, [0.0])) |> to32
    end
    get!(derived, "encoderspace") do # non-trainable sampling of encoder signal representations
        NotTrainable(flattenchain(Flux.Chain(
            models["enc1"],
            MMDLearning.split_mean_softplus_std,
            MMDLearning.sample_mv_normal,
        )))
    end

    return models, derived
end

function RESCNN(sz::Pair{Int,Int}, Nhid::Int, Dhid::Int, Ïƒhid = Flux.relu, Ïƒout = identity; skip = false)
    Flux.Chain(
        x::AbstractMatrix -> reshape(x, sz[1], 1, 1, size(x,2)),
        Flux.Conv((3,1), 1=>Dhid, identity; pad = Flux.SamePad()),
        mapreduce(vcat, 1:NhidÃ·2) do _
            convlayers = [Flux.Conv((3,1), Dhid=>Dhid, Ïƒhid; pad = Flux.SamePad()) for _ in 1:2]
            skip ? [Flux.SkipConnection(Flux.Chain(convlayers...), +)] : convlayers
        end...,
        Flux.Conv((1,1), Dhid=>1, identity; pad = Flux.SamePad()),
        x::AbstractArray{<:Any,4} -> reshape(x, sz[1], size(x,4)),
        Flux.Dense(sz[1], sz[2], Ïƒout),
    )
end

function TransformerEncoder(; n = 48, psize = 16, head = 8, hdim = 256, nhidden = 2)
    t = Transformers.Stack(
        Transformers.@nntopo(
            X : # Input (n Ã— b)
            X => X : # Reshape (1 Ã— n Ã— b)
            X => pe : # Positional embedding pe (psize Ã— n)
            (X, pe) => E : # Add positional embedding (psize Ã— n Ã— b)
            E => H : # Transformer encoder (psize Ã— n Ã— b)
            H => H # Flatten output (psize*n Ã— b)
        ),
        X -> reshape(X, 1, size(X)...),
        Transformers.Basic.PositionEmbedding(psize, n; trainable = true),
        (X, pe) -> X .+ pe,
        Flux.Chain([Transformers.Basic.Transformer(psize, head, hdim; future = true, act = Flux.relu, pdrop = 0.0) for i = 1:nhidden]...),
        Flux.flatten,
    )
    Flux.fmap(Flux.testmode!, t) # Force dropout layers inactive
end

"""
Conditional variational autoencoder.

Architecture inspired by:
    "Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy"
    https://arxiv.org/abs/1802.08797
"""
struct CVAE{n,nÎ¸,k,nz,E1,E2,D}
    E1 :: E1
    E2 :: E2
    D  :: D
    CVAE{n,nÎ¸,k,nz}(enc1::E1, enc2::E2, dec::D) where {n,nÎ¸,k,nz,E1,E2,D} = new{n,nÎ¸,k,nz,E1,E2,D}(enc1, enc2, dec)
end
Flux.@functor CVAE
Base.show(io::IO, m::CVAE) = model_summary(io, Dict("E1" => m.E1, "E2" => m.E2, "D" => m.D))

nsignal(::CVAE{n,nÎ¸,k,nz}) where {n,nÎ¸,k,nz} = n
ntheta(::CVAE{n,nÎ¸,k,nz}) where {n,nÎ¸,k,nz} = nÎ¸
nlatent(::CVAE{n,nÎ¸,k,nz}) where {n,nÎ¸,k,nz} = k
nembedding(::CVAE{n,nÎ¸,k,nz}) where {n,nÎ¸,k,nz} = nz

####
#### CVAE helpers
####

@inline function split_theta_latent(::CVAE{n,nÎ¸,k,nz}, x::AbstractMatrix) where {n,nÎ¸,k,nz}
    @assert size(x,1) == nÎ¸ + k
    Î¸, Z = if k == 0
        x, similar(x, 0, size(x,2))
    else
        x[1:nÎ¸,:], x[nÎ¸+1:end,:]
    end
    return Î¸, Z
end

####
#### CVAE methods
####

KLDivUnitNormal(Î¼, Ïƒ) = (sum(@. pow2(Ïƒ) + pow2(Î¼) - 2 * log(Ïƒ)) - length(Î¼)) / (2 * size(Î¼,2)) # KL-divergence between approximation posterior and N(0, 1) prior (Note: sum over dim=1, mean over dim=2)
KLDivergence(Î¼q0, Ïƒq, Î¼r0, Ïƒr) = (sum(@. pow2(Ïƒq / Ïƒr) + pow2((Î¼r0 - Î¼q0) / Ïƒr) - 2 * log(Ïƒq / Ïƒr)) - length(Î¼q0)) / (2 * size(Î¼q0,2)) # KL-divergence contribution to cross-entropy (Note: sum over dim=1, mean over dim=2)
EvidenceLowerBound(x, Î¼x0, Ïƒx) = (sum(@. pow2((x - Î¼x0) / Ïƒx) + 2 * log(Ïƒx)) + length(Î¼x0) * log2Ï€(eltype(Î¼x0))) / (2 * size(Î¼x0,2)) # Negative log-likelihood/ELBO contribution to cross-entropy (Note: sum over dim=1, mean over dim=2)

function mv_normal_parameters(cvae::CVAE, Y, Î¸, Z)
    Î¼r0, Ïƒr = split_mean_softplus_std(cvae.E1(Y))
    Î¼q0, Ïƒq = split_mean_softplus_std(cvae.E2(vcat(Y,Î¸,Z)))
    zq = sample_mv_normal(Î¼q0, Ïƒq)
    Î¼x0, Ïƒx = split_mean_softplus_std(cvae.D(vcat(Y,zq)))
    return (; Î¼r0, Ïƒr, Î¼q0, Ïƒq, Î¼x0, Ïƒx)
end

function KL_and_ELBO(cvae::CVAE{n,nÎ¸,k,nz}, Y, Î¸, Z; marginalize_Z::Bool) where {n,nÎ¸,k,nz}
    @unpack Î¼r0, Ïƒr, Î¼q0, Ïƒq, Î¼x0, Ïƒx = mv_normal_parameters(cvae, Y, Î¸, Z)
    KLDiv = KLDivergence(Î¼q0, Ïƒq, Î¼r0, Ïƒr)
    ELBO = marginalize_Z ?
        EvidenceLowerBound(Î¸, Î¼x0[1:nÎ¸,..], Ïƒx[1:nÎ¸,..]) :
        EvidenceLowerBound(vcat(Î¸,Z), Î¼x0, Ïƒx)
    return (; KLDiv, ELBO)
end

function sampleÎ¸Z_setup(cvae::CVAE, Y)
    Î¼r = cvae.E1(Y)
    Î¼r0, Ïƒr = split_mean_softplus_std(Î¼r)
    return Î¼r0, Ïƒr
end

sampleÎ¸Zposterior(cvae::CVAE, Y) = sampleÎ¸Zposterior(cvae, Y, sampleÎ¸Z_setup(cvae, Y)...)

function sampleÎ¸Zposterior(cvae::CVAE, Y, Î¼r0, Ïƒr)
    zr = sample_mv_normal(Î¼r0, Ïƒr)
    Î¼x = cvae.D(vcat(Y,zr))
    Î¼x0, Ïƒx = split_mean_softplus_std(Î¼x)
    x = sample_mv_normal(Î¼x0, Ïƒx)
    Î¸, Z = split_theta_latent(cvae, x)
    return Î¸, Z
end

function Î¸Zposterior_sampler(cvae::CVAE, Y)
    Î¼r0, Ïƒr = sampleÎ¸Z_setup(cvae, Y) # constant over posterior samples
    Î¸Zposterior_sampler_inner() = sampleÎ¸Zposterior(cvae, Y, Î¼r0, Ïƒr)
    return Î¸Zposterior_sampler_inner
end

####
#### Deep prior
####

"""
Deep prior for learning to Î¸ distribution, wrapping (parameterized) functions `Î¸prior` and `Zprior`

    Î¸prior : R^kÎ¸ -> R^nÎ¸
    Zprior : R^kZ -> R^nZ

which generates samples Î¸ ~ Î¸prior(Î¸), Z ~ Zprior(Z) via the transformation of `kÎ¸` and `kZ` implicitly
sampled latent variables, respectively. These Î¸ parameterize physics models, e.g. phys : R^nÎ¸ -> R^n,
and Z parameterize latent variable Rician models.
"""
struct DeepPriorRicianPhysicsModel{T,kÎ¸,kZ,P<:PhysicsModel{T},R<:RicianCorrector,FÎ¸,FZ}
    phys   :: P
    rice   :: R
    Î¸prior :: FÎ¸
    Zprior :: FZ
    DeepPriorRicianPhysicsModel{T,kÎ¸,kZ}(phys::P, rice::R, Î¸prior::FÎ¸, Zprior::FZ) where {T,kÎ¸,kZ,P,R,FÎ¸,FZ} = new{T,kÎ¸,kZ,P,R,FÎ¸,FZ}(phys, rice, Î¸prior, Zprior)
end
Flux.@functor DeepPriorRicianPhysicsModel
Flux.trainable(prior::DeepPriorRicianPhysicsModel) = (prior.Î¸prior, prior.Zprior)
Base.show(io::IO, prior::DeepPriorRicianPhysicsModel) = model_summary(io, Dict("Î¸prior" => prior.Î¸prior, "Î¸prior" => prior.Î¸prior))

sampleÎ¸prior(prior::DeepPriorRicianPhysicsModel{T}, n::Int) where {T} = sampleÎ¸prior(prior, CUDA.CuMatrix{T}, n) # default to sampling Î¸ on the gpu
sampleÎ¸prior(prior::DeepPriorRicianPhysicsModel, Y::AbstractArray, n::Int = size(Y,2)) = sampleÎ¸prior(prior, typeof(Y), n) # Î¸ type is similar to Y type
sampleÎ¸prior(prior::DeepPriorRicianPhysicsModel{T,kÎ¸,kZ}, ::Type{A}, n::Int) where {T, kÎ¸, kZ, A <: AbstractArray{T}} = prior.Î¸prior(randn_similar(A, kÎ¸, n)) # sample from distribution

sampleZprior(prior::DeepPriorRicianPhysicsModel{T}, n::Int) where {T} = sampleZprior(prior, CUDA.CuMatrix{T}, n) # default to sampling Z on the gpu
sampleZprior(prior::DeepPriorRicianPhysicsModel, Y::AbstractArray, n::Int = size(Y,2)) = sampleZprior(prior, typeof(Y), n) # Z type is similar to Y type
sampleZprior(prior::DeepPriorRicianPhysicsModel{T,kÎ¸,kZ}, ::Type{A}, n::Int) where {T, kÎ¸, kZ, A <: AbstractArray{T}} = prior.Zprior(randn_similar(A, kZ, n)) # sample from distribution

#### PhysicsModel + CVAE methods

function sampleÎ¸Z(cvae::CVAE, prior::DeepPriorRicianPhysicsModel, Y::AbstractVecOrMat; posterior_Î¸ = true, posterior_Z = true)
    if posterior_Î¸ || posterior_Z
        return sampleÎ¸Z(cvae, prior, Y, sampleÎ¸Z_setup(cvae, Y)...; posterior_Î¸, posterior_Z)
    else
        Î¸ = sampleÎ¸prior(prior, Y, size(Y,2))
        Z = sampleZprior(prior, Y, size(Y,2))
        return Î¸, Z
    end
end

function sampleÎ¸Z(cvae::CVAE, prior::DeepPriorRicianPhysicsModel, Y::AbstractVecOrMat, Î¼r0, Ïƒr; posterior_Î¸ = true, posterior_Z = true)
    if posterior_Î¸ || posterior_Z
        Î¸hat, Zhat = sampleÎ¸Zposterior(cvae, Y, Î¼r0, Ïƒr)
        Î¸hat = clamp.(Î¸hat, todevice(Î¸lower(prior.phys)), todevice(Î¸upper(prior.phys)))
        Î¸ = posterior_Î¸ ? Î¸hat : sampleÎ¸prior(prior, Y, size(Y,2))
        Z = posterior_Z ? Zhat : sampleZprior(prior, Y, size(Y,2))
        Î¸, Z
    else
        Î¸ = sampleÎ¸prior(prior, Y, size(Y,2))
        Z = sampleZprior(prior, Y, size(Y,2))
        return Î¸, Z
    end
end

function Î¸Z_sampler(cvae::CVAE, prior::DeepPriorRicianPhysicsModel, Y::AbstractVecOrMat; posterior_Î¸ = true, posterior_Z = true)
    Î¼r0, Ïƒr = sampleÎ¸Z_setup(cvae, Y) # constant over posterior samples
    Î¸Z_sampler_inner() = sampleÎ¸Z(cvae, prior, Y, Î¼r0, Ïƒr; posterior_Î¸, posterior_Z)
    return Î¸Z_sampler_inner
end

function sampleXÎ¸Z(cvae::CVAE, prior::DeepPriorRicianPhysicsModel, Y::AbstractVecOrMat; kwargs...)
    #TODO: can't differentiate through @timeit "sampleÎ¸Z"
    #TODO: can't differentiate through @timeit "signal_model"
    Î¸, Z = sampleÎ¸Z(cvae, prior, Y; kwargs...)
    X = signal_model(prior.phys, Î¸)
    return X, Î¸, Z
end

sampleX(cvae::CVAE, prior::DeepPriorRicianPhysicsModel, Y::AbstractVecOrMat; kwargs...) = sampleXÎ¸Z(cvae, prior, Y; kwargs...)[1]

#### RicianCorrector + PhysicsModel + CVAE methods

function sampleXÌ‚Î¸Z(cvae::CVAE, prior::DeepPriorRicianPhysicsModel, Y::AbstractVecOrMat; kwargs...)
    #TODO: can't differentiate through @timeit "sampleXÎ¸Z"
    #TODO: can't differentiate through @timeit "sampleXÌ‚"
    X, Î¸, Z = sampleXÎ¸Z(cvae, prior, Y; kwargs...)
    XÌ‚ = sampleXÌ‚(prior.rice, X, Z)
    return XÌ‚, Î¸, Z
end

sampleXÌ‚(cvae::CVAE, prior::DeepPriorRicianPhysicsModel, Y::AbstractVecOrMat; kwargs...) = sampleXÌ‚Î¸Z(cvae, prior, Y; kwargs...)[1]

####
#### Rician posterior state
####

function sampleXÌ‚(rice::RicianCorrector, X, Z, ninstances = nothing)
    Î½, Ïµ = rician_params(rice, X, Z)
    return add_noise_instance(rice, Î½, Ïµ, ninstances)
end

function NegLogLikelihood(rice::RicianCorrector, Y, Î¼0, Ïƒ)
    if typeof(rice) <: NormalizedRicianCorrector && !isnothing(rice.normalizer)
        Î£Î¼ = rice.normalizer(MMDLearning._rician_mean_cuda.(Î¼0, Ïƒ))
        Î¼0, Ïƒ = (Î¼0 ./ Î£Î¼), (Ïƒ ./ Î£Î¼)
    end
    -sum(MMDLearning._rician_logpdf_cuda.(Y, Î¼0, Ïƒ); dims = 1) # Rician negative log likelihood
end

function make_state(prior::DeepPriorRicianPhysicsModel, Y::AbstractMatrix, Î¸::AbstractMatrix, Z::AbstractMatrix)
    X = signal_model(prior.phys, Î¸)
    Î´, Ïµ = correction_and_noiselevel(prior.rice, X, Z)
    Î½ = add_correction(prior.rice, X, Î´)
    â„“ = reshape(NegLogLikelihood(prior.rice, Y, Î½, Ïµ), 1, :)
    return (; Y, Î¸, Z, X, Î´, Ïµ, Î½, â„“)
end

function posterior_state(
        cvae::CVAE,
        prior::DeepPriorRicianPhysicsModel,
        Y::AbstractMatrix{T};
        miniter = 5,
        maxiter = 100,
        alpha = 0.01,
        mode = :maxlikelihood,
        verbose = false
    ) where {T}

    Î¸Z_sampler_instance = Î¸Z_sampler(cvae, prior, Y; posterior_Î¸ = true, posterior_Z = true)

    function update(last_state, i)
        Î¸new, Znew = Î¸Z_sampler_instance()
        Î¸last = isnothing(last_state) ? nothing : last_state.Î¸
        Zlast = isnothing(last_state) ? nothing : last_state.Z

        if mode === :mean
            Î¸new = isnothing(last_state) ? Î¸new : T(1/i) .* Î¸new .+ T(1-1/i) .* Î¸last
            Znew = isnothing(last_state) ? Znew : T(1/i) .* Znew .+ T(1-1/i) .* Zlast
            new_state = make_state(prior, Y, Î¸new, Znew)
        elseif mode === :maxlikelihood
            new_state = make_state(prior, Y, Î¸new, Znew)
            if !isnothing(last_state)
                mask = new_state.â„“ .< last_state.â„“
                new_state = map(new_state, last_state) do new, last
                    new .= ifelse.(mask, new, last)
                end
            end
        else
            error("Unknown mode: $mode")
        end

        # Check for convergence
        p = isnothing(last_state) ? nothing :
            HypothesisTests.pvalue(
                HypothesisTests.UnequalVarianceTTest(
                    map(x -> x |> Flux.cpu |> vec |> Vector{Float64}, (new_state.â„“, last_state.â„“))...
                )
            )

        return new_state, p
    end

    @timeit "posterior state" CUDA.@sync begin
        state, _ = update(nothing, 1)
        verbose && @info 1, mean_and_std(state.â„“)
        for i in 2:maxiter
            state, p = update(state, i)
            verbose && @info i, mean_and_std(state.â„“), p
            (i >= miniter) && (p > 1 - alpha) && break
        end
    end

    return state
end

#= TODO: update to return both Î¸ and Z means + stddevs?
function (cvae::CVAE)(Y; nsamples::Int = 1, stddev::Bool = false)
    @assert nsamples â‰¥ ifelse(stddev, 2, 1)
    smooth(a, b, Î³) = a + Î³ * (b - a)
    Î¸Zsampler = Î¸Zposterior_sampler(cvae, Y)
    Î¼x = Î¸Zsampler()
    Î¼x_last, Ïƒx2 = zero(Î¼x), zero(Î¼x)
    for i in 2:nsamples
        x = Î¸Zsampler()
        Î¼x_last .= Î¼x
        Î¼x .= smooth.(Î¼x, x, 1//i)
        Ïƒx2 .= smooth.(Ïƒx2, (x .- Î¼x) .* (x .- Î¼x_last), 1//i)
    end
    return stddev ? vcat(Î¼x, sqrt.(Ïƒx2)) : Î¼x
end
=#
