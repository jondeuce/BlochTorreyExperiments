####
#### Rician correctors
####

abstract type RicianCorrector end

# G : ùêë^n -> ùêë^2n mapping X ‚àà ùêë^n ‚ü∂ Œ¥,logœµ ‚àà ùêë^n concatenated as [Œ¥; logœµ]
@with_kw struct VectorRicianCorrector{Gtype} <: RicianCorrector
    G::Gtype
end

# G : ùêë^n -> ùêë^n mapping X ‚àà ùêë^n ‚ü∂ Œ¥ ‚àà ùêë^n with fixed noise œµ0 ‚àà ùêë, or œµ0 ‚àà ùêë^n
@with_kw struct FixedNoiseVectorRicianCorrector{Gtype,T} <: RicianCorrector
    G::Gtype
    œµ0::T
end

# Concrete methods to extract Œ¥ and œµ
function correction_and_noiselevel(G::VectorRicianCorrector, X)
    Œ¥_logœµ = G.G(X)
    Œ¥_logœµ[1:end√∑2, :], exp.(Œ¥_logœµ[end√∑2+1:end, :])
end
correction_and_noiselevel(G::FixedNoiseVectorRicianCorrector, X) = G.G(X), œµ0

# Derived convenience functions
correction(G::RicianCorrector, X) = correction_and_noiselevel(G, X)[1]
noiselevel(G::RicianCorrector, X) = correction_and_noiselevel(G, X)[2]
noise_instance(G::RicianCorrector, X, œµ) = œµ .* randn(eltype(X), size(X)...)
noise_instance(G::RicianCorrector, X) = noise_instance(G, X, noiselevel(G, X))
corrected_signal_instance(G::RicianCorrector, X) = corrected_signal_instance(G, X, correction_and_noiselevel(G, X)...)
corrected_signal_instance(G::RicianCorrector, X, Œ¥, œµ) = corrected_signal_instance(G, abs.(X .+ Œ¥), œµ)
function corrected_signal_instance(G::RicianCorrector, X, œµ)
    œµR = noise_instance(G, X, œµ)
    œµI = noise_instance(G, X, œµ)
    Xœµ = @. sqrt((X + œµR)^2 + œµI^2)
    return Xœµ
end
function rician_params(G::RicianCorrector, X)
    Œ¥, œµ = correction_and_noiselevel(G, X)
    ŒΩ, œÉ = abs.(X .+ Œ¥), œµ
    return ŒΩ, œÉ
end

####
#### Physics model interface
####

abstract type PhysicsModel end

struct ClosedForm{P<:PhysicsModel}
    p::P
end

# Abstract interface
hasclosedform(p::PhysicsModel) = false # fallback
physicsmodel(p::PhysicsModel) = p
physicsmodel(c::ClosedForm) = c.p
Œ∏bounds(p::PhysicsModel) = tuple.(Œ∏lower(p), Œ∏upper(p))
Œ∏bounds(c::ClosedForm) = Œ∏bounds(physicsmodel(c))
ntheta(c::ClosedForm) = ntheta(physicsmodel(c))
nsignal(c::ClosedForm) = nsignal(physicsmodel(c))
function epsilon end

####
#### Toy problem
####

@with_kw struct ToyModel{T} <: PhysicsModel
    œµ0::T = T(0.01)
    Ytrain::Ref{Matrix{T}} = Ref(zeros(T,0,0))
    Ytest::Ref{Matrix{T}} = Ref(zeros(T,0,0))
    Yval::Ref{Matrix{T}} = Ref(zeros(T,0,0))
end
const ClosedFormToyModel{T} = ClosedForm{ToyModel{T}}
const MaybeClosedFormToyModel{T} = Union{ToyModel{T}, ClosedFormToyModel{T}}

ntheta(::ToyModel) = 5
nsignal(::ToyModel) = 128
hasclosedform(::ToyModel) = true
beta(::ToyModel) = 4
beta(::ClosedFormToyModel) = 2
epsilon(c::ClosedFormToyModel) = physicsmodel(c).œµ0

Œ∏labels(::ToyModel) = ["freq", "phase", "offset", "amp", "tconst"]
Œ∏lower(::ToyModel{T}) where {T} = [1/T(64),   T(0), 1/T(4), 1/T(10),  T(16)]
Œ∏upper(::ToyModel{T}) where {T} = [1/T(32), T(œÄ)/2, 1/T(2),  1/T(4), T(128)]
Œ∏error(p::ToyModel, theta, thetahat) = abs.((theta .- thetahat)) ./ (Œ∏upper(p) .- Œ∏lower(p))

function initialize!(p::ToyModel; ntrain::Int, ntest::Int = ntrain, nval::Int = ntrain)
    rng = Random.seed!(0)
    p.Ytrain[] = sampleX(ClosedForm(p), ntrain, p.œµ0; dataset = :train)
    p.Ytest[]  = sampleX(ClosedForm(p), ntest, p.œµ0; dataset = :test)
    p.Yval[]   = sampleX(ClosedForm(p), nval, p.œµ0; dataset = :val)
    Random.seed!(rng)
    return p
end

sampleŒ∏(p::ToyModel, n::Union{Int, Symbol}; dataset::Symbol) = permutedims(reduce(hcat, rand.(Uniform.(Œ∏lower(p), Œ∏upper(p)), n)))

sampleX(p::MaybeClosedFormToyModel, n::Union{Int, Symbol}, epsilon = nothing; dataset::Symbol) = sampleX(p, sampleŒ∏(physicsmodel(p), n; dataset = dataset), epsilon)
sampleX(p::MaybeClosedFormToyModel, theta, epsilon = nothing) = signal_model(p, theta, epsilon)

function sampleY(p::ToyModel, n::Union{Int, Symbol}; dataset::Symbol)
    dataset === :train ? (n === :all ? p.Ytrain[] : sample_columns(p.Ytrain[], n)) :
    dataset === :test  ? (n === :all ? p.Ytest[]  : sample_columns(p.Ytest[], n)) :
    dataset === :val   ? (n === :all ? p.Yval[]   : sample_columns(p.Yval[], n)) :
    error("dataset must be :train, :test, or :val")
end

function _signal_model(
        theta::AbstractVecOrMat,
        epsilon,
        nsamples::Int,
        beta::Int,
    )
    freq, phase, offset, amp, tconst = theta[1:1,:], theta[2:2,:], theta[3:3,:], theta[4:4,:], theta[5:5,:]
    t = 0:nsamples-1
    y = @. (offset + amp * sin(2*(pi*freq)*t - phase)^beta) * exp(-t/tconst)
    if !isnothing(epsilon)
        œµR = epsilon .* randn(eltype(theta), nsamples, size(theta, 2))
        œµI = epsilon .* randn(eltype(theta), nsamples, size(theta, 2))
        y = @. sqrt((y + œµR)^2 + œµI^2)
    end
    return y
end
signal_model(p::MaybeClosedFormToyModel, theta::AbstractVecOrMat, epsilon = nothing) = _signal_model(theta, epsilon, nsignal(p), beta(p))

####
#### Signal model
####

function signal_data(
        image::Array{T,4},
        batchsize = nothing;
        threshold# = T(opts.Threshold)
    ) where {T}
    first_echo = filter!(>(threshold), image[:,:,:,1][:])
    q1 = quantile(first_echo, 0.30)
    q2 = quantile(first_echo, 0.99)
    Is = findall(I -> q1 <= image[I,1] <= q2, CartesianIndices(size(image)[1:3]))
    Y  = image[Is, :]' |> copy
    # map(_ -> display(plot(image[rand(Is),:])), 1:5)
    # histogram(first_echo) |> p -> vline!(p, [q1, q2]) |> display
    Y ./= sum(Y; dims=1)
    return batchsize === nothing ? Y : sample_columns(Y, batchsize)
end

function theta_bounds(T = Float64; ntheta::Int)
    if ntheta == 4
        # theta_labels = ["alpha", "T2short", "dT2", "Ashort"]
        theta_lb = T[ 50.0,    8.0,    8.0, 0.0]
        theta_ub = T[180.0, 1000.0, 1000.0, 1.0]
    elseif ntheta == 5
        # theta_labels = ["alpha", "T2short", "dT2", "Ashort", "Along"]
        theta_lb = T[ 50.0,    8.0,    8.0, 0.0, 0.0]
        theta_ub = T[180.0, 1000.0, 1000.0, 1.0, 1.0]
    else
        error("Number of labels must be 4 or 5")
    end
    theta_bd = collect(zip(theta_lb, theta_ub))
    @assert ntheta == length(theta_bd)
    return theta_bd
end
theta_sampler(args...; kwargs...) = broadcast(bound -> bound[1] + (bound[2]-bound[1]) * rand(typeof(bound[1])), theta_bounds(args...; kwargs...))

function signal_theta_error(theta, thetahat)
    dtheta = (x -> x[2] - x[1]).(theta_bounds(eltype(theta); ntheta = size(theta,1)))
    return abs.((theta .- thetahat)) ./ dtheta
end

noise_model!(buffer::AbstractVector{T}, signal::AbstractVector{T}, œµ::AbstractVector{T}) where {T} = (randn!(buffer); buffer .*= œµ .* signal[1]; buffer)
noise_model(signal::AbstractVector, œµ::AbstractVector) = œµ .* signal[1] .* randn(eltype(signal), length(signal))

function signal_model_work(T = Float64; nTE::Int)
    epg_work = DECAES.EPGdecaycurve_work(T, nTE)
    signal = zeros(T, nTE)
    buf = zeros(T, nTE)
    real_noise = zeros(T, nTE)
    imag_noise = zeros(T, nTE)
    return @ntuple(epg_work, signal, buf, real_noise, imag_noise)
end

function signal_model!(
        work,
        Œ∏::AbstractVector{T},
        œµ::Union{AbstractVector, Nothing} = nothing;
        TE,
        normalize::Bool = true,
    ) where {T}
    @unpack epg_work, signal, real_noise, imag_noise = work
    # alpha, T2short, dT2, Ashort = Œ∏[1], Œ∏[2]/1000, Œ∏[3]/1000, Œ∏[4]
    alpha, T2short, dT2, Ashort, Along = Œ∏[1], Œ∏[2]/1000, Œ∏[3]/1000, Œ∏[4], Œ∏[5]
    refcon  = T(180.0)
    T2long  = T2short + dT2
    #Along  = 1-Ashort
    T1short = T(1000.0)/1000
    T1long  = T(1000.0)/1000

    signal  .= Ashort .* DECAES.EPGdecaycurve!(epg_work, alpha, T(TE), T2short, T1short, refcon) # short component
    signal .+= Along  .* DECAES.EPGdecaycurve!(epg_work, alpha, T(TE), T2long,  T1long,  refcon) # long component
    normalize && (signal ./= sum(signal))

    # Add noise to "real" and "imag" channels in quadrature
    if !isnothing(œµ)
        if eltype(œµ) === eltype(Œ∏)
            noise_model!(real_noise, signal, œµ) # populate real_noise
            noise_model!(imag_noise, signal, œµ) # populate imag_noise
            signal .= sqrt.((signal .+ real_noise).^2 .+ imag_noise.^2)
            normalize && (signal ./= sum(signal))
        else
            # Add forward-differentiable noise to "real" and "imag" channels in quadrature
            signal = sqrt.((signal .+ noise_model(signal, œµ)).^2 .+ noise_model(signal, œµ).^2)
            normalize && (signal ./= sum(signal))
        end
    end

    return signal
end

function signal_model!(
        work,
        Œ∏::AbstractMatrix{T},
        œµ::Union{AbstractVector, Nothing} = nothing;
        kwargs...
    ) where {T}
    @unpack signal = work
    X = zeros(length(signal), size(Œ∏,2))
    @uviews Œ∏ X for j in 1:size(Œ∏,2)
        signal_model!(work, Œ∏[:,j], œµ; kwargs...)
        X[:,j] .= signal
    end
    return X
end

signal_model(Œ∏::AbstractVecOrMat{T}, œµ::Union{AbstractVector, Nothing} = nothing; nTE::Int, TE, kwargs...) where {T} = signal_model!(signal_model_work(T; nTE = nTE), Œ∏, œµ; TE = T(TE), kwargs...)

function mutate_signal(Y::AbstractVecOrMat; meanmutations::Int = 0)
    if meanmutations <= 0
        return Y
    end
    nrow = size(Y, 1)
    p = meanmutations / nrow
    return Y .* (rand(size(Y)...) .> p)
end

####
#### Direct data samplers from prior derived from MLE fitted signals
####

function make_mle_data_samplers(
        imagepath,
        thetaspath;
        ntheta::Int,
        plothist = false,
        padtrain = false,
        normalizesignals = true,
        filteroutliers = false,
    )
    @assert !(padtrain && !normalizesignals) "unnormalized padded training data is not implemented"

    # Set random seed for consistent test/train sets
    rng = Random.seed!(0)

    # Load + preprocess fit results (~25% of voxels dropped)
    fits = deepcopy(BSON.load(thetaspath)["results"])

    if filteroutliers
        println("before filter: $(nrow(fits))")
        filter!(r -> !(999.99 <= r.dT2 && 999.99 <= r.T2short), fits) # drop boundary failures
        filter!(r -> r.dT2 <= 999.99, fits) # drop boundary failures
        filter!(r -> r.T2short <= 100, fits) # drop long T2short (very few points)
        filter!(r -> 8.01 <= r.T2short, fits) # drop boundary failures
        filter!(r -> 8.01 <= r.dT2, fits) # drop boundary failures
        if ntheta == 5
            filter!(r -> 0.005 <= r.Ashort <= 0.15, fits) # drop outlier fits (very few points)
            filter!(r -> 0.005 <= r.Along <= 0.15, fits) # drop outlier fits (very few points)
        end
        filter!(r -> r.loss <= -250, fits) # drop poor fits (very few points)
        println("after filter:  $(nrow(fits))")
    end

    # Shuffle data + collect thetas
    fits = fits[shuffle(MersenneTwister(0), 1:nrow(fits)), :]
    thetas = ntheta == 4 ? # Create ntheta x nSamples matrix
        permutedims(convert(Matrix{Float64}, fits[:, [:alpha, :T2short, :dT2, :Ashort]])) :
        permutedims(convert(Matrix{Float64}, fits[:, [:alpha, :T2short, :dT2, :Ashort, :Along]]))

    # Load image, keeping signals which correspond to thetas
    image = DECAES.load_image(imagepath) # load 4D MatrixSize x nTE image
    Y = convert(Matrix{Float64}, permutedims(image[CartesianIndex.(fits[!, :index]), :])) # convert to nTE x nSamples Matrix

    if normalizesignals
        # Normalize signals individually such that each signal has unit sum
        Y ./= sum(Y; dims = 1)
    else
        # Don't normalize scales individually, but nevertheless scale the signals uniformly down to avoid numerical difficulties
        Y ./= 1e6
        Ysum = sum(Y; dims = 1)

        # Scale thetas and fit results for using unnormalized Y
        thetas[4:4, :] .*= Ysum # scale Ashort
        fits.Ashort .*= vec(Ysum)
        if ntheta == 5
            thetas[5:5, :] .*= Ysum # scale Along
            fits.Along .*= vec(Ysum)
        end
        fits.logsigma .= log.(exp.(fits.logsigma) .* vec(Ysum)) # scale sigma from fit results
        fits.rmse .*= vec(Ysum) # scale rmse from fit results
        fits.loss .+= size(Y,1) .* log.(vec(Ysum)) # scale mle loss from fit results
    end

    # Forward simulation params
    signal_work = signal_model_work(Float64; nTE = 48)
    signal_fun(Œ∏::AbstractMatrix{Float64}, noise::Union{AbstractVector{Float64}, Nothing} = nothing; kwargs...) =
        signal_model!(signal_work, Œ∏, noise; TE = 8e-3, normalize = normalizesignals, kwargs...)

    # Pad training data with thetas sampled uniformly randomly over the prior space
    local Œ∏train_pad
    if padtrain
        @assert normalizesignals "unnormalized padded training data is not implemented"
        Œ∏_pad_lo, Œ∏_pad_hi = minimum(thetas; dims = 2), maximum(thetas; dims = 2)
        Œ∏train_pad = Œ∏_pad_lo .+ (Œ∏_pad_hi .- Œ∏_pad_lo) .* rand(MersenneTwister(0), ntheta, nrow(fits))
        Xtrain_pad = signal_fun(Œ∏train_pad; normalize = false)
        if ntheta == 5
            Œ∏train_pad[4:5, :] ./= sum(Xtrain_pad; dims = 1) # normalize Ashort, Along
            train_pad_filter   = map(Ashort -> 0.005 <= Ashort <= 0.15, Œ∏train_pad[4,:]) # drop outlier samples (very few points)
            train_pad_filter .&= map(Along  -> 0.005 <= Along  <= 0.15, Œ∏train_pad[5,:]) # drop outlier samples (very few points)
            Œ∏train_pad = Œ∏train_pad[:, train_pad_filter]
        end
        println("num padded:    $(size(Œ∏train_pad,2))")
    end

    # Plot prior distribution histograms
    if plothist
        theta_cols = ntheta == 4 ? [:alpha, :T2short, :dT2, :Ashort] : [:alpha, :T2short, :dT2, :Ashort, :Along]
        display(plot([histogram(fits[!,c]; lab = c, nbins = 75) for c in [theta_cols; :logsigma; :loss]]...))
    end

    # Generate data samplers
    itrain =                   1 : 2*(size(Y,2)√∑4)
    itest  = 2*(size(Y,2)√∑4) + 1 : 3*(size(Y,2)√∑4)
    ival   = 3*(size(Y,2)√∑4) + 1 : size(Y,2)

    # True data (Y) samplers
    Ytrain, Ytest, Yval = Y[:,itrain], Y[:,itest], Y[:,ival]
    function sampleY(batchsize; dataset = :train)
        dataset === :train ? (batchsize === nothing ? Ytrain : sample_columns(Ytrain, batchsize)) :
        dataset === :test  ? (batchsize === nothing ? Ytest  : sample_columns(Ytest,  batchsize)) :
        dataset === :val   ? (batchsize === nothing ? Yval   : sample_columns(Yval,   batchsize)) :
        error("dataset must be :train, :test, or :val")
    end

    # Fit parameters (Œ∏) samplers
    Œ∏train, Œ∏test, Œ∏val = thetas[:,itrain], thetas[:,itest], thetas[:,ival]
    if padtrain
        Œ∏train = hcat(Œ∏train, Œ∏train_pad)
        Œ∏train = Œ∏train[:,shuffle(MersenneTwister(0), 1:size(Œ∏train,2))] # mix training + padded thetas
    end
    function sampleŒ∏(batchsize; dataset = :train)
        dataset === :train ? (batchsize === nothing ? Œ∏train : sample_columns(Œ∏train, batchsize)) :
        dataset === :test  ? (batchsize === nothing ? Œ∏test  : sample_columns(Œ∏test,  batchsize)) :
        dataset === :val   ? (batchsize === nothing ? Œ∏val   : sample_columns(Œ∏val,   batchsize)) :
        error("dataset must be :train, :test, or :val")
    end

    # Model data (X) samplers
    function _sampleX_model(batchsize; dataset = :train, kwargs...)
        signal_fun(sampleŒ∏(batchsize; dataset = dataset); kwargs...)
    end

    # Direct model data (X) samplers
    Xtrain = _sampleX_model(nothing; dataset = :train)
    Xtest  = _sampleX_model(nothing; dataset = :test)
    Xval   = _sampleX_model(nothing; dataset = :val)
    function _sampleX_direct(batchsize; dataset = :train)
        dataset === :train ? (batchsize === nothing ? Xtrain : sample_columns(Xtrain, batchsize)) :
        dataset === :test  ? (batchsize === nothing ? Xtest  : sample_columns(Xtest,  batchsize)) :
        dataset === :val   ? (batchsize === nothing ? Xval   : sample_columns(Xval,   batchsize)) :
        error("dataset must be :train, :test, or :val")
    end

    # Model data (X) samplers
    function sampleX(batchsize; kwargs...)
        if batchsize === nothing
            _sampleX_direct(batchsize; kwargs...)
        else
            _sampleX_model(batchsize; kwargs...)
        end
    end

    # Output train/test/val dataframe partitions
    fits_train, fits_test, fits_val = fits[itrain,:], fits[itest,:], fits[ival,:]

    # Reset random seed
    Random.seed!(rng)

    return @ntuple(sampleX, sampleY, sampleŒ∏, fits_train, fits_test, fits_val)
end

####
#### Maximum likelihood estimation inference
####

function signal_loglikelihood_inference(
        y::AbstractVector,
        initial_guess = nothing,
        model = x -> (x, zero(x)),
        signal_fun = Œ∏ -> toy_signal_model(Œ∏, nothing, 4);
        bounds = toy_theta_bounds(),
        objective = :mle,
        bbopt_kwargs = Dict(:MaxTime => 1.0),
    )

    # Deterministic loss function, suitable for Optim
    function mle_loss(Œ∏)
        yÃÑhat, œµhat = model(signal_fun(Œ∏))
        return -sum(logpdf.(Rician.(yÃÑhat, œµhat), y))
    end

    # Stochastic loss function, only suitable for BlackBoxOptim
    function rmse_loss(Œ∏)
        yÃÑhat, œµhat = model(signal_fun(Œ∏))
        yhat = rand.(Rician.(yÃÑhat, œµhat))
        return sqrt(mean(abs2, y .- yhat))
    end

    loss = objective === :mle ? mle_loss : rmse_loss

    bbres = nothing
    if objective !== :mle || (objective === :mle && isnothing(initial_guess))
        bbres = BlackBoxOptim.bboptimize(loss;
            SearchRange = bounds,
            TraceMode = :silent,
            bbopt_kwargs...
        )
    end

    optres = nothing
    if objective === :mle
        Œ∏0 = isnothing(initial_guess) ? BlackBoxOptim.best_candidate(bbres) : initial_guess
        lo = (x->x[1]).(bounds)
        hi = (x->x[2]).(bounds)
        # dfc = Optim.TwiceDifferentiableConstraints(lo, hi)
        # df = Optim.TwiceDifferentiable(loss, Œ∏0; autodiff = :forward)
        # optres = Optim.optimize(df, dfc, Œ∏0, Optim.IPNewton())
        df = Optim.OnceDifferentiable(loss, Œ∏0; autodiff = :forward)
        optres = Optim.optimize(df, lo, hi, Œ∏0, Optim.Fminbox(Optim.LBFGS()))
        # optres = Optim.optimize(df, lo, hi, Œ∏0, Optim.Fminbox(Optim.BFGS()))
    end

    return @ntuple(bbres, optres)
end
function signal_loglikelihood_inference(Y::AbstractMatrix, Œ∏0::Union{<:AbstractMatrix, Nothing} = nothing, args...; kwargs...)
    _args = [deepcopy(args) for _ in 1:Threads.nthreads()]
    _kwargs = [deepcopy(kwargs) for _ in 1:Threads.nthreads()]
    tasks = map(1:size(Y,2)) do j
        Threads.@spawn begin
            tid = Threads.threadid()
            initial_guess = !isnothing(Œ∏0) ? Œ∏0[:,j] : nothing
            signal_loglikelihood_inference(Y[:,j], initial_guess, _args[tid]...; _kwargs[tid]...)
        end
    end
    return map(Threads.fetch, tasks)
end

#=
for _ in 1:1
    noise_level = 1e-2
    Œ∏ = toy_theta_sampler(1);
    x = toy_signal_model(Œ∏, nothing, 4);
    y = toy_signal_model(Œ∏, nothing, 2);
    xœµ = toy_signal_model(Œ∏, noise_level, 4);
    yœµ = toy_signal_model(Œ∏, noise_level, 2);

    m = x -> ((dx, œµ) = correction_and_noiselevel(x); return (abs.(x.+dx), œµ));

    @time bbres1, _ = signal_loglikelihood_inference(yœµ, nothing, m; objective = :rmse)[1];
    Œ∏hat1 = BlackBoxOptim.best_candidate(bbres1);
    xhat1 = toy_signal_model(Œ∏hat1, nothing, 4);
    dxhat1, œµhat1 = correction_and_noiselevel(xhat1);
    yhat1 = corrected_signal_instance(xhat1, dxhat1, œµhat1);

    @time bbres2, optres2 = signal_loglikelihood_inference(yœµ, nothing, m; objective = :mle)[1];
    Œ∏hat2 = Optim.minimizer(optres2); #BlackBoxOptim.best_candidate(bbres2);
    xhat2 = toy_signal_model(Œ∏hat2, nothing, 4);
    dxhat2, œµhat2 = correction_and_noiselevel(xhat2);
    yhat2 = corrected_signal_instance(xhat2, dxhat2, œµhat2);

    p1 = plot([y[:,1] x[:,1]]; label = ["YŒ∏" "XŒ∏"], line = (2,));
    p2 = plot([yœµ[:,1] xœµ[:,1]]; label = ["YŒ∏œµ" "XŒ∏œµ"], line = (2,));
    p3 = plot([yœµ[:,1] yhat1]; label = ["YŒ∏œµ" "YÃÑŒ∏œµ‚ÇÅ"], line = (2,));
    p4 = plot([yœµ[:,1] yhat2]; label = ["YŒ∏œµ" "YÃÑŒ∏œµ‚ÇÇ"], line = (2,));
    plot(p1,p2,p3,p4) |> display;

    @show toy_theta_error(Œ∏[:,1], Œ∏hat1)';
    @show toy_theta_error(Œ∏[:,1], Œ∏hat2)';
    @show ‚àömean(abs2, y[:,1] .- (xhat1 .+ dxhat1));
    @show ‚àömean(abs2, y[:,1] .- (xhat2 .+ dxhat2));
    @show ‚àömean([mean(abs2, yœµ[:,1] .- corrected_signal_instance(xhat1, dxhat1, œµhat1)) for _ in 1:1000]);
    @show ‚àömean([mean(abs2, yœµ[:,1] .- corrected_signal_instance(xhat2, dxhat2, œµhat2)) for _ in 1:1000]);
end;
=#

####
#### Toy problem MCMC inference
####

#=
Turing.@model toy_model_rician_noise(
        y,
        correction_and_noiselevel,
    ) = begin
    freq   ~ Uniform(1/64,  1/32)
    phase  ~ Uniform( 0.0,  pi/2)
    offset ~ Uniform( 0.25,  0.5)
    amp    ~ Uniform( 0.1,  0.25)
    tconst ~ Uniform(16.0, 128.0)
    # logeps ~ Uniform(-4.0,  -2.0)
    # epsilon = 10^logeps

    # Compute toy signal model without noise
    x = toy_signal_model([freq, phase, offset, amp, tconst], nothing, 4)
    yhat, œµhat = correction_and_noiselevel(x)

    # Model noise as Rician
    for i in 1:length(y)
        # ŒΩ, œÉ = x[i], epsilon
        ŒΩ, œÉ = yhat[i], œµhat[i]
        y[i] ~ Rician(ŒΩ, œÉ)
    end
end
=#

function toy_theta_mcmc_inference(
        y::AbstractVector,
        correction_and_noiselevel,
        callback = (y, chain) -> true,
    )
    model = function (x)
        xhat, œµhat = correction_and_noiselevel(x)
        yhat = rand.(Rician.(xhat, œµhat))
        return yhat
    end
    res = signal_loglikelihood_inference(y, nothing, model)
    theta0 = best_candidate(res)
    while true
        chain = sample(toy_model_rician_noise(y, correction_and_noiselevel), NUTS(), 1000; verbose = true, init_theta = theta0)
        # chain = psample(toy_model_rician_noise(y, correction_and_noiselevel), NUTS(), 1000, 3; verbose = true, init_theta = theta0)
        callback(y, chain) && return chain
    end
end
function toy_theta_mcmc_inference(Y::AbstractMatrix, args...; kwargs...)
    tasks = map(1:size(Y,2)) do j
        Threads.@spawn signal_loglikelihood_inference(Y[:,j], initial_guess, args...; kwargs...)
    end
    return map(Threads.fetch, tasks)
end

function find_cutoff(x; initfrac = 0.25, pthresh = 1e-4)
    cutoff = clamp(round(Int, initfrac * length(x)), 2, length(x))
    for i = cutoff+1:length(x)
        mu = mean(x[1:i-1])
        sig = std(x[1:i-1])
        p = ccdf(Normal(mu, sig), x[i])
        (p < pthresh) && break
        cutoff += 1
    end
    return cutoff
end

#=
for _ in 1:1
    correction_and_noiselevel = let _model = deepcopy(BSON.load("/home/jon/Documents/UBCMRI/BlochTorreyExperiments-master/MMDLearning/output/2020-02-20T15:43:48.506/best-model.bson")["model"]) #deepcopy(model)
        function(x)
            out = _model(x)
            dx, logœµ = out[1:end√∑2], out[end√∑2+1:end]
            return abs.(x .+ dx), exp.(logœµ)
        end
    end
    signal_model = function(Œ∏hat)
        x = toy_signal_model(Œ∏hat, nothing, 4)
        xhat, œµhat = correction_and_noiselevel(x)
        # zR = œµhat .* randn(size(x)...)
        # zI = œµhat .* randn(size(x)...)
        # yhat = @. sqrt((xhat + zR)^2 + zI^2)
        yhat = rand.(Rician.(xhat, œµhat))
    end
    fitresults = function(y, c)
        Œ∏hat = map(k -> mean(c[k])[1,:mean], [:freq, :phase, :offset, :amp, :tconst])
        # œµhat = 10^map(k -> mean(c[k])[1,:mean], [:logeps])[1]
        # yhat, œµhat = correction_and_noiselevel(toy_signal_model(Œ∏hat, nothing, 4))
        yhat = signal_model(Œ∏hat)
        yerr = sqrt(mean(abs2, y - yhat))
        @ntuple(Œ∏hat, yhat, yerr)
    end
    plotresults = function(y, c)
        @unpack Œ∏hat, yhat, yerr = fitresults(y, c)
        display(plot(c))
        display(plot([y yhat]))
        return nothing
        # return plot(c) #|> display
        # return plot([y yhat]) #|> display
    end

    # Œ∏ = [freq, phase, offset, amp, tconst]
    # Random.seed!(0);
    noise_level = 1e-2;
    Œ∏ = toy_theta_sampler(16);
    Y = toy_signal_model(Œ∏, noise_level, 2);

    # @time cs = toy_theta_mcmc_inference(Y, correction_and_noiselevel);
    # res = map(j -> fitresults(Y[:,j], cs[j]), 1:size(Y,2))
    # ps = map(j -> plotresults(Y[:,j], cs[j]), 1:size(Y,2))
    # Œ∏hat = reduce(hcat, map(k -> mean(c[k])[1,:mean], [:freq, :phase, :offset, :amp, :tconst]) for c in cs)
    # Yerr = sort(getfield.(res, :yerr))

    @time bbres = signal_loglikelihood_inference(Y, nothing, signal_model);
    Yerr = sort(best_fitness.(bbres))
    Œ∏hat = best_candidate.(bbres)
    Yhat = signal_model.(Œ∏hat)
    Œ∏hat = reduce(hcat, Œ∏hat)
    Yhat = reduce(hcat, Yhat)
    map(j -> display(plot([Y[:,j] Yhat[:,j]])), 1:size(Y,2))

    let
        p = plot()
        sticks!(p, Yerr; m = (:circle,4), lab = "Yerr")
        # sticks!(p, [0; diff(Yerr)]; m = (:circle,4), lab = "dYerr")
        hline!(p, [2noise_level]; lab = "2œµ")
        vline!(p, [find_cutoff(Yerr; pthresh = 1e-4)]; lab = "cutoff", line = (:black, :dash))
        display(p)
    end

    display(Œ∏hat)
    display(Œ∏)
    display((Œ∏.-Œ∏hat)./Œ∏)
end
=#

#=
for _ in 1:100
    let seed = rand(0:1000_000)
        rng = Random.seed!(seed)
        p = plot();
        Random.seed!(seed); plot!(toy_signal_model(3, nothing, 2); ylim = (0, 1.5));
        Random.seed!(seed); plot!(toy_signal_model(3, nothing, 2.5); ylim = (0, 1.5));
        display(p);
        Random.seed!(rng);
    end;
end;
=#

nothing
