"""
Conditional variational autoencoder.

Architecture inspired by:
    "Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy"
    https://arxiv.org/abs/1802.08797
"""
struct CVAE{n,nÎ¸,nÎ¸M,k,nz,Dist,E1,E2,D,F,Fâ»Â¹}
    E1  :: E1
    E2  :: E2
    D   :: D
    f   :: F
    fâ»Â¹ :: Fâ»Â¹
end
CVAE{n,nÎ¸,nÎ¸M,k,nz}(enc1::E1, enc2::E2, dec::D, f::F, fâ»Â¹::Fâ»Â¹; posterior_dist::Type = Gaussian) where {n,nÎ¸,nÎ¸M,k,nz,E1,E2,D,F,Fâ»Â¹} = CVAE{n,nÎ¸,nÎ¸M,k,nz,posterior_dist,E1,E2,D,F,Fâ»Â¹}(enc1, enc2, dec, f, fâ»Â¹)

const CVAEPosteriorDist{Dist} = CVAE{n,nÎ¸,nÎ¸M,k,nz,Dist} where {n,nÎ¸,nÎ¸M,k,nz}

Flux.functor(::Type{<:CVAE{n,nÎ¸,nÎ¸M,k,nz,Dist}}, c) where {n,nÎ¸,nÎ¸M,k,nz,Dist} = (E1 = c.E1, E2 = c.E2, D = c.D, f = c.f, fâ»Â¹ = c.fâ»Â¹,), fs -> CVAE{n,nÎ¸,nÎ¸M,k,nz}(fs...; posterior_dist = Dist)
Base.show(io::IO, ::CVAE{n,nÎ¸,nÎ¸M,k,nz,Dist}) where {n,nÎ¸,nÎ¸M,k,nz,Dist} = print(io, "CVAE$((;n,nÎ¸,nÎ¸M,k,nz,Dist))")

nsignal(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = n
ntheta(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = nÎ¸
nmarginalized(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = nÎ¸M
nlatent(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = k
nembedding(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = nz

struct CVAETrainingState{C <: CVAE, A, S}
    cvae::C
    YÌ„::A
    Î¸Ì„::A
    ZÌ„::A
    Î¼r0::A
    logÏƒr::A
    Î¼q0::A
    logÏƒq::A
    nrm_state::S
end

function CVAETrainingState(cvae::CVAE, Y, Î¸, Z = zeros_similar(Î¸, 0, size(Î¸,2)))
    YÌ„, Î¸Ì„, ZÌ„, nrm_state = normalize_inputs(cvae, Y, Î¸, Z)
    YÌ„pad = pad_signal(cvae, YÌ„)
    Î¼r = cvae.E1(YÌ„pad)
    Î¼q = cvae.E2(YÌ„pad, Î¸Ì„, ZÌ„)
    Î¼r0, logÏƒr = split_dim1(Î¼r)
    Î¼q0, logÏƒq = split_dim1(Î¼q)
    return CVAETrainingState(cvae, YÌ„pad, Î¸Ì„, ZÌ„, Î¼r0, logÏƒr, Î¼q0, logÏƒq, nrm_state)
end
signal(state::CVAETrainingState) = state.YÌ„

struct CVAEInferenceState{C <: CVAE, A, S}
    cvae::C
    YÌ„::A
    Î¼r0::A
    logÏƒr::A
    nrm_state::S
end

function CVAEInferenceState(cvae::CVAE, Y)
    YÌ„, nrm_state = normalize_inputs(cvae, Y)
    YÌ„pad = pad_signal(cvae, YÌ„)
    Î¼r = cvae.E1(YÌ„pad)
    Î¼r0, logÏƒr = split_dim1(Î¼r)
    return CVAEInferenceState(cvae, YÌ„pad, Î¼r0, logÏƒr, nrm_state)
end
signal(state::CVAEInferenceState) = state.YÌ„

normalize_inputs(cvae::CVAE, Y) = cvae.f((Y,)) # returns (YÌ„, nrm_state)
normalize_inputs(cvae::CVAE, Y, Î¸, Z) = cvae.f((Y, Î¸, Z)) # returns (YÌ„, Î¸Ì„, ZÌ„, nrm_state)
unnormalize_outputs(state::CVAETrainingState, Î¸Ì„M, ZÌ„) = state.cvae.fâ»Â¹((Î¸Ì„M, ZÌ„, state.nrm_state)) # returns (Î¸M, Z)
unnormalize_outputs(state::CVAEInferenceState, Î¸Ì„M, ZÌ„) = state.cvae.fâ»Â¹((Î¸Ì„M, ZÌ„, state.nrm_state)) # returns (Î¸M, Z)

# Layer which transforms matrix of [Î¼â€²; logÏƒâ€²] âˆˆ [â„^nz; â„^nz] to bounded intervals [Î¼; logÏƒ] âˆˆ [ğ’ŸÎ¼^nz; ğ’ŸlogÏƒ^nz]:
#      Î¼ bounded: prevent CVAE from "memorizing" inputs via mean latent embedding vectors which are far from zero
#   logÏƒ bounded: similarly, prevent CVAE from "memorizing" inputs via latent embedding vectors which are nearly constant, i.e. have zero variance
CVAELatentTransform(nz, ğ’ŸÎ¼ = (-3,3), ğ’ŸlogÏƒ = (-6,0)) = Flux.Chain(
    Base.BroadcastFunction(tanh),
    CatScale([(-1,1) => ğ’ŸÎ¼, (-1,1) => ğ’ŸlogÏƒ], [nz, nz]),
)

####
#### CVAE helpers
####

@inline split_at(x::AbstractVecOrMat, n::Int) = n == size(x,1) ? (x, zeros_similar(x, 0, size(x)[2:end]...)) : (x[1:n, ..], x[n+1:end, ..])
split_theta_latent(cvae::CVAE, x::AbstractVecOrMat) = split_at(x, ntheta(cvae))
split_marginal_latent(cvae::CVAE, x::AbstractVecOrMat) = split_at(x, nmarginalized(cvae))

function split_marginal_latent_pairs(cvae::CVAE, x::AbstractVecOrMat)
    Î¼1Î¸_Î¼1Z, Î¼2Î¸_Î¼2Z = split_dim1(x) # x = [Î¼1Î¸; Î¼1Z; Î¼2Î¸; Î¼1Z]
    Î¼1Î¸, Î¼1Z = split_marginal_latent(cvae, Î¼1Î¸_Î¼1Z) # size(Î¼1Î¸,1) = nÎ¸M, size(Î¼1Z,1) = nlatent
    Î¼2Î¸, Î¼2Z = split_marginal_latent(cvae, Î¼2Î¸_Î¼2Z) # size(Î¼2Î¸,1) = nÎ¸M, size(Î¼2Z,1) = nlatent
    return (Î¼1Î¸, Î¼2Î¸, Î¼1Z, Î¼2Z)
end

function pad_signal(Y::AbstractVecOrMat, n)
    if size(Y,1) < n
        pad = zeros_similar(Y, n - size(Y,1), size(Y)[2:end]...)
        vcat(Y, pad)
    elseif size(Y,1) > n
        Y[1:n, ..]
    else
        Y
    end
end
pad_signal(::CVAE{n}, Y) where {n} = pad_signal(Y, n)

function signal_mask(Y::AbstractVecOrMat; minkept::Int = firstindex(Y,1), maxkept::Int = lastindex(Y,1))
    # Create mask which randomly zeroes the tails of the columns of Y. That is, for each column `j` the mask satisfies
    # `m[i <= nâ±¼, j] = 1` and `m[i > nâ±¼, j] = 0`, with `minkept <= nâ±¼ <= maxkept` chosen randomly per column `j`
    Irows   = arr_similar(Y, collect(1:size(Y,1)))
    Icutoff = arr_similar(Y, collect(rand(minkept:maxkept, 1, size(Y)[2:end]...)))
    mask    = arr_similar(Y, Irows .<= Icutoff)
    return mask
end

function pad_and_mask_signal(Y::AbstractVecOrMat, n; minkept, maxkept)
    Ypad  = pad_signal(Y, n)
    M     = Zygote.@ignore signal_mask(Ypad; minkept, maxkept)
    Ymask = M .* Ypad
    return Ymask, M
end

####
#### CVAE methods
####

function KLDivergence(state::CVAETrainingState)
    @unpack Î¼q0, logÏƒq, Î¼r0, logÏƒr = state
    KLDivGaussian(Î¼q0, logÏƒq, Î¼r0, logÏƒr)
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{Gaussian}}
    @unpack cvae, YÌ„, Î¸Ì„, ZÌ„, Î¼q0, logÏƒq = state
    nÎ¸M = nmarginalized(cvae)
    zq = sample_mv_normal(Î¼q0, exp.(logÏƒq))
    Î¼x0, logÏƒx = split_dim1(cvae.D(YÌ„, zq))
    ELBO = marginalize_Z ?
        NegLogLGaussian(Î¸Ì„[1:nÎ¸M, ..], Î¼x0[1:nÎ¸M, ..], logÏƒx[1:nÎ¸M, ..]) :
        NegLogLGaussian(vcat(Î¸Ì„[1:nÎ¸M, ..], ZÌ„), Î¼x0, logÏƒx)
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
    @unpack cvae, YÌ„, Î¸Ì„, ZÌ„, Î¼q0, logÏƒq = state
    nÎ¸M = nmarginalized(cvae)
    zq = sample_mv_normal(Î¼q0, exp.(logÏƒq))
    Î¼x = cvae.D(YÌ„, zq) # Î¼x = D(YÌ„, zq) = [Î¼Î¸Ì„M; Î¼ZÌ„; logÏƒÎ¸Ì„M; logÏƒZÌ„]
    Ïƒâ»Â¹Î¼Î¸Ì„M, logÏƒÎ¸Ì„M, Î¼ZÌ„, logÏƒZÌ„ = split_marginal_latent_pairs(cvae, Î¼x)
    Î¼Î¸Ì„M = tanh.(Ïƒâ»Â¹Î¼Î¸Ì„M) # transform from unbounded Ïƒâ»Â¹Î¼Î¸Ì„M âˆˆ â„^nÎ¸ to bounded interval [-1, 1]^nÎ¸
    ELBO_Î¸ = NegLogLTruncatedGaussian(Î¸Ì„[1:nÎ¸M, ..], Î¼Î¸Ì„M, logÏƒÎ¸Ì„M, -1, 1)
    if marginalize_Z
        ELBO = ELBO_Î¸
    else
        ELBO = ELBO_Î¸ + NegLogLGaussian(ZÌ„, Î¼ZÌ„, logÏƒZÌ„)
    end
end

function KL_and_ELBO(state::CVAETrainingState; marginalize_Z::Bool = nlatent(state.cvae) == 0)
    KLDiv = KLDivergence(state)
    ELBO = EvidenceLowerBound(state; marginalize_Z)
    return (; KLDiv, ELBO)
end

KL_and_ELBO(cvae::CVAE, Y, Î¸, Z = zeros_similar(Î¸, 0, size(Î¸,2)); marginalize_Z::Bool = nlatent(cvae) == 0) = KL_and_ELBO(CVAETrainingState(cvae, Y, Î¸, Z); marginalize_Z)

sampleÎ¸Zposterior(cvae::CVAE, Y; kwargs...) = sampleÎ¸Zposterior(CVAEInferenceState(cvae, Y); kwargs...)

function sampleÎ¸Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{Gaussian}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Î¼r0` is the most likely value for `zr` and `Î¼x0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, YÌ„, Î¼r0, logÏƒr = state
    zr = mode ? Î¼r0 : sample_mv_normal(Î¼r0, exp.(logÏƒr))
    Î¼x = cvae.D(YÌ„, zr)
    Î¼x0, logÏƒx = split_dim1(Î¼x)
    x = mode ? Î¼x0 : sample_mv_normal(Î¼x0, exp.(logÏƒx))
    Î¸Ì„M, ZÌ„ = split_marginal_latent(cvae, x)
    Î¸M, Z = unnormalize_outputs(state, Î¸Ì„M, ZÌ„)
    return Î¸M, ZÌ„
end

function sampleÎ¸Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Î¼r0` is the most likely value for `zr` and `Î¼x0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, YÌ„, Î¼r0, logÏƒr = state
    zr = mode ? Î¼r0 : sample_mv_normal(Î¼r0, exp.(logÏƒr))
    Î¼x = cvae.D(YÌ„, zr)
    Ïƒâ»Â¹Î¼Î¸Ì„M, logÏƒÎ¸Ì„M, Î¼ZÌ„, logÏƒZÌ„ = split_marginal_latent_pairs(cvae, Î¼x)
    Î¼Î¸Ì„M = tanh.(Ïƒâ»Â¹Î¼Î¸Ì„M) # transform from unbounded Ïƒâ»Â¹Î¼Î¸Ì„M âˆˆ â„^nÎ¸ to bounded interval [-1, 1]^nÎ¸
    Î¸Ì„M = mode ? Î¼Î¸Ì„M : sample_trunc_mv_normal(Î¼Î¸Ì„M, exp.(logÏƒÎ¸Ì„M), -1, 1)
    ZÌ„ = mode || nlatent(state.cvae) == 0 ? Î¼ZÌ„ : sample_mv_normal(Î¼ZÌ„, exp.(logÏƒZÌ„))
    Î¸M, Z = unnormalize_outputs(state, Î¸Ì„M, ZÌ„)
    return Î¸M, Z
end

function Î¸Zposterior_sampler(cvae::CVAE, Y)
    state = CVAEInferenceState(cvae, Y) # constant over posterior samples
    Î¸Zposterior_sampler_inner(; kwargs...) = sampleÎ¸Zposterior(state; kwargs...)
    return Î¸Zposterior_sampler_inner
end

#### CVAE + PhysicsModel + AbstractMetaDataSignal methods

function sampleÎ¸Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; posterior_mode = false)
    return sampleÎ¸Z(phys, cvae, Ymeta, CVAEInferenceState(cvae, signal(Ymeta)); posterior_mode)
end

function sampleÎ¸Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal, state::CVAEInferenceState; posterior_mode = false)
    Î¸M, Z = sampleÎ¸Zposterior(state; mode = posterior_mode)
    Î¸Mlo = arr_similar(Î¸Ì‚M, Î¸marginalized(phys, Î¸lower(phys)))
    Î¸Mhi = arr_similar(Î¸Ì‚M, Î¸marginalized(phys, Î¸upper(phys)))
    Î¸ = vcat(clamp.(Î¸M, Î¸Mlo, Î¸Mhi), Zygote.@ignore(Î¸nuissance(phys, Ymeta)))
    return Î¸, Z
end

function Î¸Z_sampler(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal)
    state = CVAEInferenceState(cvae, signal(Ymeta)) # constant over posterior samples
    Î¸Z_sampler_inner(; kwargs...) = sampleÎ¸Z(phys, cvae, Ymeta, state; kwargs...)
    return Î¸Z_sampler_inner
end

function sampleXÎ¸Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...)
    Î¸, Z = sampleÎ¸Z(phys, cvae, Ymeta; kwargs...)
    X = signal_model(phys, Ymeta, Î¸)
    (size(X,1) > nsignal(Ymeta)) && (X = X[1:nsignal(Ymeta), ..])
    return X, Î¸, Z
end

sampleX(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÎ¸Z(phys, cvae, Ymeta; kwargs...)[1]

function posterior_state(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...)
    Î¸, Z = sampleÎ¸Z(phys, cvae, Ymeta; posterior_mode = false, kwargs...)
    posterior_state(phys, Ymeta, Î¸, Z)
end

@with_kw_noshow struct OnlineMetropolisSampler{T}
    Î¸::Array{T,3} # parameter values
    ntheta::Int           = size(Î¸, 1) # number of parameters per data point
    ndata::Int            = size(Î¸, 2) # number of data points, i.e. each column of Î¸ represents estimates for a separate datum Y
    nsamples::Int         = size(Î¸, 3) # length of the Metropolis-Hastings MCMC chain which is recorded
    i::Vector{Int}        = ones(Int, ndata) # current sample index in cyclical chain buffer Î¸
    accept::Array{Bool,3} = zeros(Bool, 1, ndata, nsamples) # records whether proposal was accepted or not
    neglogPXÎ¸::Array{T,3} = fill(T(Inf), 1, ndata, nsamples) # negative log likelihoods; initialize with Inf to guarantee acceptance of first sample
    neglogPÎ¸::Array{T,3}  = zeros(T, 1, ndata, nsamples) # negative log priors; initialization is moot due to neglogPXÎ¸ initialzed to Inf
    neglogQÎ¸::Array{T,3}  = zeros(T, 1, ndata, nsamples) # proposal distribution negative log likelihood; as opposed to standard MH, assumed to be independent of previous sample, i.e. Q(Î¸|Î¸â€²) â‰¡ Q(Î¸); initialization is moot due to neglogPXÎ¸ initialzed to Inf
end
Base.show(io::IO, s::OnlineMetropolisSampler{T}) where {T} = print(io, "OnlineMetropolisSampler{$(T)}(ntheta = $(s.ntheta), ndata = $(s.ndata), nsamples = $(s.nsamples))")

buffer_index(s::OnlineMetropolisSampler, j::Int) = CartesianIndex(j, mod1(s.i[j], s.nsamples))
random_index(s::OnlineMetropolisSampler, j::Int) = CartesianIndex(j, rand(1:s.nsamples))
buffer_indices(s::OnlineMetropolisSampler, J = 1:s.ndata) = buffer_index.((s,), J)
random_indices(s::OnlineMetropolisSampler, J = 1:s.ndata) = random_index.((s,), J)
function Random.rand(s::OnlineMetropolisSampler, J = 1:s.ndata)
    idx = random_indices(s, J)
    return s.Î¸[:, idx], s.neglogPXÎ¸[:, idx], s.neglogPÎ¸[:, idx], s.neglogQÎ¸[:, idx]
end

# c.f. https://stats.stackexchange.com/a/163790
function update!(s::OnlineMetropolisSampler, Î¸â€²::AbstractMatrix, neglogPXÎ¸â€²::AbstractMatrix, neglogPÎ¸â€²::AbstractMatrix, neglogQÎ¸â€²::AbstractMatrix, J = 1:s.ndata)
    @assert size(Î¸â€², 1) == s.ntheta
    @assert size(Î¸â€², 2) == size(neglogPXÎ¸â€², 2) == size(neglogPÎ¸â€², 2) == size(neglogQÎ¸â€², 2)
    @assert size(neglogPXÎ¸â€², 1) == size(neglogPÎ¸â€², 1) == size(neglogQÎ¸â€², 1) == 1

    # DECAES.tforeach(eachindex(J); blocksize = 16) do j
    Threads.@threads for j in eachindex(J)
        @inbounds begin
            col        = J[j]
            curr       = buffer_index(s, col)
            s.i[col]  += 1
            next       = buffer_index(s, col)

            # Metropolis-Hastings acceptance ratio:
            #        Î± = min(1, (PXÎ¸â€² * PÎ¸â€² * QÎ¸) / (PXÎ¸ * PÎ¸ * QÎ¸â€²))
            # ==> logÎ± = min(0, logPXÎ¸â€² + logPÎ¸â€² + logQÎ¸ - logPXÎ¸ - logPÎ¸ - logQÎ¸â€²)
            logÎ±       = min(0, s.neglogPXÎ¸[1,curr] + s.neglogPÎ¸[1,curr] + neglogQÎ¸â€²[1,j] - neglogPXÎ¸â€²[1,j] - neglogPÎ¸â€²[1,j] - s.neglogQÎ¸[1,curr])
            accept     = logÎ± > log(rand())

            # Update theta, negative log likelihoods, and negative log priors with accepted points or current points
            s.accept[1,next]    = accept
            @inbounds for i in 1:size(Î¸â€², 1)
                s.Î¸[i,next]     = accept ? Î¸â€²[i,j]         : s.Î¸[i,curr]
            end
            s.neglogPXÎ¸[1,next] = accept ? neglogPXÎ¸â€²[1,j] : s.neglogPXÎ¸[1,curr]
            s.neglogPÎ¸[1,next]  = accept ? neglogPÎ¸â€²[1,j]  : s.neglogPÎ¸[1,curr]
            s.neglogQÎ¸[1,next]  = accept ? neglogQÎ¸â€²[1,j]  : s.neglogQÎ¸[1,curr]
        end
    end
end

function update!(s::OnlineMetropolisSampler{T}, phys::BiexpEPGModel{T}, cvae::CVAEPosteriorDist{TruncatedGaussian}, img::CPMGImage, Y_gpu, Y_cpu = cpu(T, Y_gpu); img_cols) where {T}
    Î¸â€², _      = sampleÎ¸Zposterior(cvae, Y_gpu)
    Î¸â€²         = cpu(T, Î¸â€²)
    Xâ€²         = signal_model(phys, img, Î¸â€²)
    neglogPXÎ¸â€² = negloglikelihood(phys, Y_cpu, Xâ€², Î¸â€²)
    neglogPÎ¸â€²  = neglogprior(phys, Î¸â€²)
    neglogQÎ¸â€²  = zeros_similar(neglogPÎ¸â€²)
    update!(s, Î¸â€², neglogPXÎ¸â€², neglogPÎ¸â€², neglogQÎ¸â€², img_cols)
end

function update!(s::OnlineMetropolisSampler{T}, phys::BiexpEPGModel{T}, cvae::CVAE, img::CPMGImage; dataset::Symbol, gpu_batch_size::Int) where {T}
    Y         = img.partitions[dataset]
    J_ranges  = collect(Iterators.partition(1:size(Y, 2), gpu_batch_size))
    for (i, (Y_gpu,)) in enumerate(CUDA.CuIterator((Y[:, J],) for J in J_ranges))
        Y_cpu = Y[:, J_ranges[i]]
        update!(s, phys, cvae, img, Y_gpu, Y_cpu; img_cols = J_ranges[i])
    end
end
