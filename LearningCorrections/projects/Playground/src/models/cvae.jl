"""
Conditional variational autoencoder.

Architecture inspired by:
    "Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy"
    https://arxiv.org/abs/1802.08797
"""
struct CVAE{n,nŒ∏,nŒ∏M,k,nz,Dist,E1,E2,D,B1,B2}
    E1 :: E1
    E2 :: E2
    D  :: D
    Œ∏bd :: B1
    Œ∏ÃÑbd :: B2
end
CVAE{n,nŒ∏,nŒ∏M,k,nz}(enc1::E1, enc2::E2, dec::D, Œ∏bd::B1, Œ∏ÃÑbd::B2; posterior_dist::Type = Gaussian) where {n,nŒ∏,nŒ∏M,k,nz,E1,E2,D,B1,B2} = CVAE{n,nŒ∏,nŒ∏M,k,nz,posterior_dist,E1,E2,D,B1,B2}(enc1, enc2, dec, Œ∏bd, Œ∏ÃÑbd)

const CVAEPosteriorDist{Dist} = CVAE{n,nŒ∏,nŒ∏M,k,nz,Dist} where {n,nŒ∏,nŒ∏M,k,nz}

Flux.functor(::Type{<:CVAE{n,nŒ∏,nŒ∏M,k,nz,Dist}}, c) where {n,nŒ∏,nŒ∏M,k,nz,Dist} = (E1 = c.E1, E2 = c.E2, D = c.D, Œ∏bd = c.Œ∏bd, Œ∏ÃÑbd = c.Œ∏ÃÑbd,), fs -> CVAE{n,nŒ∏,nŒ∏M,k,nz}(fs...; posterior_dist = Dist)
Base.show(io::IO, ::CVAE{n,nŒ∏,nŒ∏M,k,nz,Dist}) where {n,nŒ∏,nŒ∏M,k,nz,Dist} = print(io, "CVAE$((;n,nŒ∏,nŒ∏M,k,nz,Dist))")

nsignal(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = n
ntheta(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = nŒ∏
nmarginalized(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = nŒ∏M
nlatent(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = k
nembedding(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = nz

function Œ∏_linear_xform(::CVAE, Œ∏, bd1, bd2)
    slope, bias = Zygote.@ignore begin
        bounds = bd1[1:size(Œ∏,1)] .=> bd2[1:size(Œ∏,1)]
        slope, bias = unzip(linear_xform_slope_and_bias.(bounds))
        arr_similar(Œ∏, slope), arr_similar(Œ∏, bias)
    end
    return slope .* Œ∏ .+ bias
end
Œ∏_linear_normalize(cvae::CVAE, Œ∏) = Œ∏_linear_xform(cvae, Œ∏, cvae.Œ∏bd, cvae.Œ∏ÃÑbd)
Œ∏ÃÑ_linear_unnormalize(cvae::CVAE, Œ∏) = Œ∏_linear_xform(cvae, Œ∏, cvae.Œ∏ÃÑbd, cvae.Œ∏bd)

# Layer which transforms matrix of [Œº‚Ä≤; logœÉ‚Ä≤] ‚àà [‚Ñù^nz; ‚Ñù^nz] to bounded intervals [Œº; logœÉ] ‚àà [ùíüŒº^nz; ùíülogœÉ^nz]:
#      Œº bounded: prevent CVAE from "memorizing" inputs via mean latent embedding vectors which are far from zero
#   logœÉ bounded: similarly, prevent CVAE from "memorizing" inputs via latent embedding vectors which are nearly constant, i.e. have zero variance
CVAELatentTransform(nz, ùíüŒº = (-2,2), ùíülogœÉ = (-4,0)) = Flux.Chain(
    Base.BroadcastFunction(tanh),
    CatScale([(-1,1) => ùíüŒº, (-1,1) => ùíülogœÉ], [nz, nz]),
)

####
#### CVAE helpers
####

@inline split_at(x::AbstractVecOrMat, n::Int) = n == size(x,1) ? (x, zeros_similar(x, 0, size(x)[2:end]...)) : (x[1:n, ..], x[n+1:end, ..])
split_theta_latent(cvae::CVAE, x::AbstractVecOrMat) = split_at(x, ntheta(cvae))
split_marginal_latent(cvae::CVAE, x::AbstractVecOrMat) = split_at(x, nmarginalized(cvae))

function split_marginal_latent_pairs(cvae::CVAE, x::AbstractVecOrMat)
    Œº1Œ∏_Œº1Z, Œº2Œ∏_Œº2Z = split_dim1(x) # x = [Œº1Œ∏; Œº1Z; Œº2Œ∏; Œº1Z]
    Œº1Œ∏, Œº1Z = split_marginal_latent(cvae, Œº1Œ∏_Œº1Z) # size(Œº1Œ∏,1) = nŒ∏M, size(Œº1Z,1) = nlatent
    Œº2Œ∏, Œº2Z = split_marginal_latent(cvae, Œº2Œ∏_Œº2Z) # size(Œº2Œ∏,1) = nŒ∏M, size(Œº2Z,1) = nlatent
    return (Œº1Œ∏, Œº2Œ∏, Œº1Z, Œº2Z)
end

####
#### CVAE methods
####

function pad_signal(Y::AbstractVecOrMat, n)
    if size(Y,1) < n
        pad = zeros_similar(Y, n - size(Y,1), size(Y)[2:end]...)
        vcat(Y, pad)
    elseif size(Y,1) > n
        Y[1:n, ..]
    else
        Y
    end
end
pad_signal(::CVAE{n}, Y) where {n} = pad_signal(Y, n)

function signal_mask(Y::AbstractVecOrMat; minkept::Int = firstindex(Y,1), maxkept::Int = lastindex(Y,1))
    # Create mask which randomly zeroes the tails of the columns of Y. That is, for each column `j` the mask satisfies
    # `m[i <= n‚±º, j] = 1` and `m[i > n‚±º, j] = 0`, with `minkept <= n‚±º <= maxkept` chosen randomly per column `j`
    Irows   = arr_similar(Y, collect(1:size(Y,1)))
    Icutoff = arr_similar(Y, collect(rand(minkept:maxkept, 1, size(Y)[2:end]...)))
    mask    = arr_similar(Y, Irows .<= Icutoff)
    return mask
end

struct CVAETrainingState{C <: CVAE, A}
    cvae::C
    Y::A
    Œ∏ÃÑ::A
    Z::A
    Œºr0::A
    logœÉr::A
    Œºq0::A
    logœÉq::A
end

function CVAETrainingState(cvae::CVAE, Y, Œ∏, Z = zeros_similar(Œ∏, 0, size(Œ∏,2)))
    Ypad = pad_signal(cvae, Y)
    Œ∏ÃÑ = Œ∏_linear_normalize(cvae, Œ∏)
    Œºr = cvae.E1(Ypad)
    Œºq = cvae.E2(Ypad, Œ∏ÃÑ, Z)
    Œºr0, logœÉr = split_dim1(Œºr)
    Œºq0, logœÉq = split_dim1(Œºq)
    return CVAETrainingState(cvae, Ypad, Œ∏ÃÑ, Z, Œºr0, logœÉr, Œºq0, logœÉq)
end
signal(state::CVAETrainingState) = state.Y

struct CVAEInferenceState{C <: CVAE, A}
    cvae::C
    Y::A
    Œºr0::A
    logœÉr::A
end

function CVAEInferenceState(cvae::CVAE, Y)
    Ypad = pad_signal(cvae, Y)
    Œºr = cvae.E1(Ypad)
    Œºr0, logœÉr = split_dim1(Œºr)
    return CVAEInferenceState(cvae, Ypad, Œºr0, logœÉr)
end
signal(state::CVAEInferenceState) = state.Y

function KLDivergence(state::CVAETrainingState)
    @unpack Œºq0, logœÉq, Œºr0, logœÉr = state
    KLDivGaussian(Œºq0, logœÉq, Œºr0, logœÉr)
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{Gaussian}}
    @unpack cvae, Y, Œ∏ÃÑ, Z, Œºq0, logœÉq = state
    nŒ∏M = nmarginalized(cvae)
    zq = sample_mv_normal(Œºq0, exp.(logœÉq))
    Œºx0, logœÉx = split_dim1(cvae.D(Y, zq))
    ELBO = marginalize_Z ?
        NegLogLGaussian(Œ∏ÃÑ[1:nŒ∏M, ..], Œºx0[1:nŒ∏M, ..], logœÉx[1:nŒ∏M, ..]) :
        NegLogLGaussian(vcat(Œ∏ÃÑ[1:nŒ∏M, ..], Z), Œºx0, logœÉx)
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
    @unpack cvae, Y, Œ∏ÃÑ, Z, Œºq0, logœÉq = state
    nŒ∏M = nmarginalized(cvae)
    zq = sample_mv_normal(Œºq0, exp.(logœÉq))
    Œºx = cvae.D(Y, zq) # Œºx = D(Y, zq) = [ŒºŒ∏ÃÑM; ŒºZ; logœÉŒ∏ÃÑM; logœÉZ]
    Œ∏ÃÑMlo = Zygote.@ignore arr_similar(Œºx, (x->x[1]).(cvae.Œ∏ÃÑbd[1:nŒ∏M, ..]))
    Œ∏ÃÑMhi = Zygote.@ignore arr_similar(Œºx, (x->x[2]).(cvae.Œ∏ÃÑbd[1:nŒ∏M, ..]))
    œÉ‚Åª¬πŒºŒ∏ÃÑM, logœÉŒ∏ÃÑM, ŒºZ, logœÉZ = split_marginal_latent_pairs(cvae, Œºx)
    ŒºŒ∏ÃÑM  = clamp.(Œ∏ÃÑMlo .+ (Œ∏ÃÑMhi .- Œ∏ÃÑMlo) .* Flux.œÉ.(œÉ‚Åª¬πŒºŒ∏ÃÑM), Œ∏ÃÑMlo, Œ∏ÃÑMhi) # transform from unbounded œÉ‚Åª¬πŒºŒ∏ÃÑM ‚àà ‚Ñù^nŒ∏ to bounded interval [Œ∏ÃÑMlo, Œ∏ÃÑMhi]^nŒ∏
    ELBO_Œ∏ = NegLogLTruncatedGaussian(Œ∏ÃÑ[1:nŒ∏M, ..], ŒºŒ∏ÃÑM, logœÉŒ∏ÃÑM, Œ∏ÃÑMlo, Œ∏ÃÑMhi)
    if marginalize_Z
        ELBO = ELBO_Œ∏
    else
        ELBO = ELBO_Œ∏ + NegLogLGaussian(Z, ŒºZ, logœÉZ)
    end
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{Kumaraswamy}}
    @unpack cvae, Y, Œ∏ÃÑ, Z, Œºq0, logœÉq = state
    nŒ∏M = nmarginalized(cvae)
    zq = sample_mv_normal(Œºq0, exp.(logœÉq))
    Œºx = cvae.D(Y, zq) # Œºx = D(Y, zq) = [Œ±Œ∏; ŒºZ; Œ≤Œ∏; logœÉZ]
    Œ±Œ∏, Œ≤Œ∏, ŒºZ, logœÉZ = split_marginal_latent_pairs(cvae, Œºx)
    ELBO_Œ∏ = NegLogLKumaraswamy(Œ∏ÃÑ[1:nŒ∏M, ..], Œ±Œ∏, Œ≤Œ∏)
    if marginalize_Z
        ELBO = ELBO_Œ∏
    else
        ELBO = ELBO_Œ∏ + NegLogLGaussian(Z, ŒºZ, logœÉZ)
    end
end

function KL_and_ELBO(state::CVAETrainingState; marginalize_Z::Bool = nlatent(state.cvae) == 0)
    KLDiv = KLDivergence(state)
    ELBO = EvidenceLowerBound(state; marginalize_Z)
    return (; KLDiv, ELBO)
end

KL_and_ELBO(cvae::CVAE, Y, Œ∏, Z = zeros_similar(Œ∏, 0, size(Œ∏,2)); marginalize_Z::Bool = nlatent(cvae) == 0) = KL_and_ELBO(CVAETrainingState(cvae, Y, Œ∏, Z); marginalize_Z)

sampleŒ∏Zposterior(cvae::CVAE, Y; kwargs...) = sampleŒ∏Zposterior(CVAEInferenceState(cvae, Y); kwargs...)

function sampleŒ∏Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{Gaussian}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Œºr0` is the most likely value for `zr` and `Œºx0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, Y, Œºr0, logœÉr = state
    zr = mode ? Œºr0 : sample_mv_normal(Œºr0, exp.(logœÉr))
    Œºx = cvae.D(Y, zr)
    Œºx0, logœÉx = split_dim1(Œºx)
    x = mode ? Œºx0 : sample_mv_normal(Œºx0, exp.(logœÉx))
    Œ∏ÃÑM, Z = split_marginal_latent(cvae, x)
    Œ∏M = Œ∏ÃÑ_linear_unnormalize(cvae, Œ∏ÃÑM)
    return Œ∏M, Z
end

function sampleŒ∏Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Œºr0` is the most likely value for `zr` and `Œºx0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, Y, Œºr0, logœÉr = state
    nŒ∏M = nmarginalized(cvae)
    zr = mode ? Œºr0 : sample_mv_normal(Œºr0, exp.(logœÉr))
    Œºx = cvae.D(Y, zr)
    Œ∏ÃÑMlo = Zygote.@ignore arr_similar(Œºx, (x->x[1]).(cvae.Œ∏ÃÑbd[1:nŒ∏M, ..]))
    Œ∏ÃÑMhi = Zygote.@ignore arr_similar(Œºx, (x->x[2]).(cvae.Œ∏ÃÑbd[1:nŒ∏M, ..]))
    œÉ‚Åª¬πŒºŒ∏ÃÑM, logœÉŒ∏ÃÑM, ŒºZ, logœÉZ = split_marginal_latent_pairs(cvae, Œºx)
    ŒºŒ∏ÃÑM  = clamp.(Œ∏ÃÑMlo .+ (Œ∏ÃÑMhi .- Œ∏ÃÑMlo) .* Flux.œÉ.(œÉ‚Åª¬πŒºŒ∏ÃÑM), Œ∏ÃÑMlo, Œ∏ÃÑMhi) # transform from unbounded œÉ‚Åª¬πŒºŒ∏ÃÑM ‚àà ‚Ñù^nŒ∏ to bounded interval [Œ∏ÃÑMlo, Œ∏ÃÑMhi]^nŒ∏
    Œ∏ÃÑM = mode ? ŒºŒ∏ÃÑM : sample_trunc_mv_normal(ŒºŒ∏ÃÑM, exp.(logœÉŒ∏ÃÑM), Œ∏ÃÑMlo, Œ∏ÃÑMhi)
    Z = mode || nlatent(state.cvae) == 0 ? ŒºZ : sample_mv_normal(ŒºZ, exp.(logœÉZ))
    Œ∏M = Œ∏ÃÑ_linear_unnormalize(cvae, Œ∏ÃÑM)
    return Œ∏M, Z
end

function sampleŒ∏Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{Kumaraswamy}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Œºr0` is the most likely value for `zr` and `Œºx0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, Y, Œºr0, logœÉr = state
    zr = mode ? Œºr0 : sample_mv_normal(Œºr0, exp.(logœÉr))
    Œºx = cvae.D(Y, zr)
    Œ±Œ∏, Œ≤Œ∏, ŒºZ, logœÉZ = split_marginal_latent_pairs(cvae, Œºx)
    Œ∏ÃÑM = mode ? mode_kumaraswamy(Œ±Œ∏, Œ≤Œ∏) : sample_kumaraswamy(Œ±Œ∏, Œ≤Œ∏)
    Z = mode || nlatent(state.cvae) == 0 ? ŒºZ : sample_mv_normal(ŒºZ, exp.(logœÉZ))
    Œ∏M = Œ∏ÃÑ_linear_unnormalize(cvae, Œ∏ÃÑM)
    return Œ∏M, Z
end

function Œ∏Zposterior_sampler(cvae::CVAE, Y; kwargs...)
    state = CVAEInferenceState(cvae, Y) # constant over posterior samples
    Œ∏Zposterior_sampler_inner() = sampleŒ∏Zposterior(state; kwargs...)
    return Œ∏Zposterior_sampler_inner
end

####
#### Deep prior
####

"""
Deep prior for learning distribution of some œï, wrapping (possibly parameterized) functions `prior` and `noisesource`

    noisesource: ‚àÖ -> R^kœï
    prior : R^kœï -> R^nœï

which generates samples œï ~ prior(œï) from kœï-dimensional samples from noisesource.
"""
struct DeepPrior{T,kœï,FP,FN}
    prior :: FP
    noisesource :: FN
end
DeepPrior{T,kœï}(prior::FP, noisesource::FN) where {T,kœï,FP,FN} = DeepPrior{T,kœï,FP,FN}(prior, noisesource)

Flux.functor(::Type{<:DeepPrior{T,kœï}}, p) where {T,kœï} = (prior = p.prior, noisesource = p.noisesource,), fs -> DeepPrior{T,kœï}(fs...)
Base.show(io::IO, ::DeepPrior{T,kœï}) where {T,kœï} = print(io, "DeepPrior$((;T,kœï))")

(p::DeepPrior{T,kœï})(x::A) where {T, kœï, A <: AbstractVecOrMat{T}} = p.prior(p.noisesource(A, kœï, size(x,2))) # sample from distribution

StatsBase.sample(p::DeepPrior{T}, n::Int) where {T} = StatsBase.sample(p, CuMatrix{T}, n) # default to sampling Œ∏ on the gpu
StatsBase.sample(p::DeepPrior, Y::AbstractVecOrMat, n::Int = size(Y,2)) = StatsBase.sample(p, typeof(Y), n) # Œ∏ type is similar to Y type
StatsBase.sample(p::DeepPrior{T,kœï}, ::Type{A}, n::Int) where {T, kœï, A <: AbstractVecOrMat{T}} = p.prior(p.noisesource(A, kœï, n)) # sample from distribution

const MaybeDeepPrior = Union{Nothing, <:DeepPrior}

#### CVAE + PhysicsModel + DeepPrior + AbstractMetaDataSignal methods

function sampleŒ∏Z(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; posterior_Œ∏ = true, posterior_Z = true, posterior_mode = false)
    if posterior_Œ∏ || posterior_Z
        return sampleŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta, CVAEInferenceState(cvae, signal(Ymeta)); posterior_Œ∏, posterior_Z, posterior_mode)
    else
        Œ∏ = sample(Œ∏prior, signal(Ymeta))
        Z = sample(Zprior, signal(Ymeta))
        return Œ∏, Z
    end
end
sampleŒ∏Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleŒ∏Z(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true) # no prior passed -> posterior_Œ∏ = posterior_Z = true

function sampleŒ∏Z(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal, state::CVAEInferenceState; posterior_Œ∏ = true, posterior_Z = true, posterior_mode = false)
    if posterior_Œ∏ || posterior_Z
        Œ∏ÃÇM, ZÃÇ = sampleŒ∏Zposterior(state; mode = posterior_mode)
        Z = if posterior_Z
            ZÃÇ
        else
            sample(Zprior, signal(Ymeta))
        end
        Œ∏ = if posterior_Œ∏
            Œ∏Mlo = arr_similar(Œ∏ÃÇM, Œ∏marginalized(phys, Œ∏lower(phys)))
            Œ∏Mhi = arr_similar(Œ∏ÃÇM, Œ∏marginalized(phys, Œ∏upper(phys)))
            vcat(clamp.(Œ∏ÃÇM, Œ∏Mlo, Œ∏Mhi), Zygote.@ignore(Œ∏nuissance(phys, Ymeta))) #TODO Zygote.@ignore necessary?
        else
            sample(Œ∏prior, signal(Ymeta))
        end
    else
        Œ∏ = sample(Œ∏prior, signal(Ymeta))
        Z = sample(Zprior, signal(Ymeta))
    end
    return Œ∏, Z
end
sampleŒ∏Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal, state::CVAEInferenceState; kwargs...) = sampleŒ∏Z(phys, cvae, nothing, nothing, Ymeta, state; kwargs..., posterior_Œ∏ = true, posterior_Z = true) # no prior passed -> posterior_Œ∏ = posterior_Z = true

function Œ∏Z_sampler(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...)
    state = CVAEInferenceState(cvae, signal(Ymeta)) # constant over posterior samples
    Œ∏Z_sampler_inner() = sampleŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta, state; kwargs...)
    return Œ∏Z_sampler_inner
end
Œ∏Z_sampler(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = Œ∏Z_sampler(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true) # no prior passed -> posterior_Œ∏ = posterior_Z = true

function sampleXŒ∏Z(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...)
    #TODO: can't differentiate through @timeit "sampleŒ∏Z"
    #TODO: can't differentiate through @timeit "signal_model"
    Œ∏, Z = sampleŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta; kwargs...)
    X = signal_model(phys, Ymeta, Œ∏)
    (size(X,1) > nsignal(Ymeta)) && (X = X[1:nsignal(Ymeta), ..])
    return X, Œ∏, Z
end
sampleXŒ∏Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXŒ∏Z(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true) # no prior passed -> posterior_Œ∏ = posterior_Z = true

sampleX(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta; kwargs...)[1]
sampleX(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleX(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true) # no prior passed -> posterior_Œ∏ = posterior_Z = true

function posterior_state(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; accum_loss = ‚Ñì -> sum(‚Ñì; dims = 1), kwargs...)
    Œ∏, Z = sampleŒ∏Z(phys, cvae, Ymeta; posterior_Œ∏ = true, posterior_Z = true, posterior_mode = false, kwargs...)
    X = signal_model(phys, Ymeta, Œ∏)
    X = clamp_dim1(signal(Ymeta), X)
    XÃÇ = add_noise_instance(phys, X, Œ∏)
    ‚Ñì = loglikelihood(phys, signal(Ymeta), X, Œ∏; accum_loss) #TODO make a loglikelihood
    œµ = noiselevel(phys, Œ∏)
    return (; Y = signal(Ymeta), XÃÇ, Œ∏, Z, X, œµ, ŒΩ = X, Œ¥ = zeros_similar(X, 1, size(X,2)), ‚Ñì)
end

####
#### Rician posterior state
####

sampleXÃÇ(rice::RicianCorrector, X, Z = nothing, ninstances = nothing) = sample_rician_state(rice, X, Z, ninstances).XÃÇ

function sampleXÃÇŒ∏Z(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...)
    #TODO: can't differentiate through @timeit "sampleXŒ∏Z"
    #TODO: can't differentiate through @timeit "sampleXÃÇ"
    X, Œ∏, Z = sampleXŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta; kwargs...)
    XÃÇ = sampleXÃÇ(rice, X, Z)
    return XÃÇ, Œ∏, Z
end
sampleXÃÇŒ∏Z(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÃÇŒ∏Z(phys, rice, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true) # no prior passed -> posterior_Œ∏ = posterior_Z = true

sampleXÃÇ(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÃÇŒ∏Z(phys, rice, cvae, Œ∏prior, Zprior, Ymeta; kwargs...)[1]
sampleXÃÇ(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÃÇ(phys, rice, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true) # no prior passed -> posterior_Œ∏ = posterior_Z = true

function NegLogLikelihood(::PhysicsModel, rice::RicianCorrector, Y::AbstractVecOrMat, Œº0, œÉ)
    if typeof(rice) <: NormalizedRicianCorrector && (rice.normalizer !== nothing)
        # Approximate the normalization factor as the normalization factor of the mean signal.
        # For gaussian noise mean signal = Œº0, but for rician noise mean signal ~ sqrt(Œº0^2 + œÉ^2), at least when Œº0 >> œÉ
        s = inv.(rice.normalizer(mean_rician.(Œº0, œÉ)))
        neglogL_rician.(Y, s .* Œº0, log.(s .* œÉ)) # Rician negative log likelihood
    else
        neglogL_rician.(Y, Œº0, log.(œÉ)) # Rician negative log likelihood
    end
end

function NegLogLikelihood(::EPGModel, rice::RicianCorrector, Y::AbstractVecOrMat, Œº0, œÉ)
    # Likelihood is "maximimally generous" w.r.t. normalization factor, i.e. we perform MLE to find optimal scaling factor
    logs = Zygote.@ignore begin
        _, results = mle_biexp_epg_noise_only(Œº0, Y, log.(œÉ); freeze_logœµ = true, freeze_logs = false, verbose = false)
        arr_similar(Y, permutedims(results.logscale))
    end
    neglogL_rician.(Y, exp.(logs) .* Œº0, logs .+ log.(œÉ)) # Rician negative log likelihood
end

function posterior_state(phys::PhysicsModel, rice::RicianCorrector, Y::AbstractVecOrMat, Œ∏::AbstractVecOrMat, Z::AbstractVecOrMat; accum_loss = ‚Ñì -> sum(‚Ñì; dims = 1))
    X = signal_model(phys, Œ∏)
    @unpack Œ¥, œµ, ŒΩ = rician_state(rice, X, Z)
    X, Œ¥, œµ, ŒΩ = clamp_dim1(Y, (X, Œ¥, œµ, ŒΩ))
    ‚Ñì = NegLogLikelihood(phys, rice, Y, ŒΩ, œµ)
    (accum_loss !== nothing) && (‚Ñì = accum_loss(‚Ñì))
    return (; Y, Œ∏, Z, X, Œ¥, œµ, ŒΩ, ‚Ñì)
end

function posterior_state(
        phys::PhysicsModel,
        rice::RicianCorrector,
        cvae::CVAE,
        Ymeta::AbstractMetaDataSignal{T};
        miniter = 5,
        maxiter = 100,
        alpha = 0.01,
        mode = :maxlikelihood,
        verbose = false
    ) where {T}

    (mode === :mode) && (miniter = maxiter = 1)
    Œ∏Z_sampler_instance = Œ∏Z_sampler(phys, cvae, Ymeta; posterior_mode = mode === :mode)
    new_posterior_state(Œ∏new, Znew) = posterior_state(phys, rice, signal(Ymeta), Œ∏new, Znew; accum_loss = ‚Ñì -> sum(‚Ñì; dims = 1))

    function update(last_state, i)
        Œ∏new, Znew = Œ∏Z_sampler_instance()
        Œ∏last = (last_state === nothing) ? nothing : last_state.Œ∏
        Zlast = (last_state === nothing) ? nothing : last_state.Z

        if mode === :mean
            Œ∏new = (last_state === nothing) ? Œ∏new : T(1/i) .* Œ∏new .+ T(1-1/i) .* Œ∏last
            Znew = (last_state === nothing) ? Znew : T(1/i) .* Znew .+ T(1-1/i) .* Zlast
            new_state = new_posterior_state(Œ∏new, Znew)
        elseif mode === :mode
            new_state = new_posterior_state(Œ∏new, Znew)
        elseif mode === :maxlikelihood
            new_state = new_posterior_state(Œ∏new, Znew)
            if (last_state !== nothing)
                mask = new_state.‚Ñì .< last_state.‚Ñì
                new_state = map(new_state, last_state) do new, last
                    new .= ifelse.(mask, new, last)
                end
            end
        else
            error("Unknown mode: $mode")
        end

        # Check for convergence
        p = (last_state === nothing) ? nothing :
            HypothesisTests.pvalue(
                HypothesisTests.UnequalVarianceTTest(
                    map(x -> x |> cpu |> vec |> Vector{Float64}, (new_state.‚Ñì, last_state.‚Ñì))...
                )
            )

        return new_state, p
    end

    state, _ = update(nothing, 1)
    verbose && @info 1, mean_and_std(state.‚Ñì)
    for i in 2:maxiter
        state, p = update(state, i)
        verbose && @info i, mean_and_std(state.‚Ñì), p
        (i >= miniter) && (p > 1 - alpha) && break
    end

    return state
end

#= TODO: update to return both Œ∏ and Z means + stddevs?
function (cvae::CVAE)(Y; nsamples::Int = 1, stddev::Bool = false)
    @assert nsamples ‚â• ifelse(stddev, 2, 1)
    smooth(a, b, Œ≥) = a + Œ≥ * (b - a)
    Œ∏Zsampler = Œ∏Zposterior_sampler(cvae, Y)
    Œºx = Œ∏Zsampler()
    Œºx_last, œÉx2 = zero(Œºx), zero(Œºx)
    for i in 2:nsamples
        x = Œ∏Zsampler()
        Œºx_last .= Œºx
        Œºx .= smooth.(Œºx, x, 1//i)
        œÉx2 .= smooth.(œÉx2, (x .- Œºx) .* (x .- Œºx_last), 1//i)
    end
    return stddev ? vcat(Œºx, sqrt.(œÉx2)) : Œºx
end
=#
