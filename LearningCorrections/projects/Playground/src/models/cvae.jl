"""
Conditional variational autoencoder.

Architecture inspired by:
    "Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy"
    https://arxiv.org/abs/1802.08797
"""
struct CVAE{n,nŒ∏,nŒ∏M,k,nz,Dist,E1,E2,D,F,F‚Åª¬π}
    E1  :: E1
    E2  :: E2
    D   :: D
    f   :: F
    f‚Åª¬π :: F‚Åª¬π
end
CVAE{n,nŒ∏,nŒ∏M,k,nz}(enc1::E1, enc2::E2, dec::D, f::F, f‚Åª¬π::F‚Åª¬π; posterior_dist::Type = Gaussian) where {n,nŒ∏,nŒ∏M,k,nz,E1,E2,D,F,F‚Åª¬π} = CVAE{n,nŒ∏,nŒ∏M,k,nz,posterior_dist,E1,E2,D,F,F‚Åª¬π}(enc1, enc2, dec, f, f‚Åª¬π)

const CVAEPosteriorDist{Dist} = CVAE{n,nŒ∏,nŒ∏M,k,nz,Dist} where {n,nŒ∏,nŒ∏M,k,nz}

Flux.functor(::Type{<:CVAE{n,nŒ∏,nŒ∏M,k,nz,Dist}}, c) where {n,nŒ∏,nŒ∏M,k,nz,Dist} = (E1 = c.E1, E2 = c.E2, D = c.D, f = c.f, f‚Åª¬π = c.f‚Åª¬π,), fs -> CVAE{n,nŒ∏,nŒ∏M,k,nz}(fs...; posterior_dist = Dist)
Base.show(io::IO, ::CVAE{n,nŒ∏,nŒ∏M,k,nz,Dist}) where {n,nŒ∏,nŒ∏M,k,nz,Dist} = print(io, "CVAE$((;n,nŒ∏,nŒ∏M,k,nz,Dist))")

nsignal(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = n
ntheta(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = nŒ∏
nmarginalized(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = nŒ∏M
nlatent(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = k
nembedding(::CVAE{n,nŒ∏,nŒ∏M,k,nz}) where {n,nŒ∏,nŒ∏M,k,nz} = nz

struct CVAETrainingState{C <: CVAE, A, S}
    cvae::C
    YÃÑ::A
    Œ∏ÃÑ::A
    ZÃÑ::A
    Œºr0::A
    logœÉr::A
    Œºq0::A
    logœÉq::A
    nrm_state::S
end

function CVAETrainingState(cvae::CVAE, Y, Œ∏, Z = zeros_similar(Œ∏, 0, size(Œ∏,2)))
    YÃÑ, Œ∏ÃÑ, ZÃÑ, nrm_state = normalize_inputs(cvae, Y, Œ∏, Z)
    YÃÑpad = pad_signal(cvae, YÃÑ)
    Œºr = cvae.E1(YÃÑpad)
    Œºq = cvae.E2(YÃÑpad, Œ∏ÃÑ, ZÃÑ)
    Œºr0, logœÉr = split_dim1(Œºr)
    Œºq0, logœÉq = split_dim1(Œºq)
    return CVAETrainingState(cvae, YÃÑpad, Œ∏ÃÑ, ZÃÑ, Œºr0, logœÉr, Œºq0, logœÉq, nrm_state)
end
signal(state::CVAETrainingState) = state.YÃÑ

struct CVAEInferenceState{C <: CVAE, A, S}
    cvae::C
    YÃÑ::A
    Œºr0::A
    logœÉr::A
    nrm_state::S
end

function CVAEInferenceState(cvae::CVAE, Y)
    YÃÑ, nrm_state = normalize_inputs(cvae, Y)
    YÃÑpad = pad_signal(cvae, YÃÑ)
    Œºr = cvae.E1(YÃÑpad)
    Œºr0, logœÉr = split_dim1(Œºr)
    return CVAEInferenceState(cvae, YÃÑpad, Œºr0, logœÉr, nrm_state)
end
signal(state::CVAEInferenceState) = state.YÃÑ

normalize_inputs(cvae::CVAE, Y) = cvae.f((Y,)) # returns (YÃÑ, nrm_state)
normalize_inputs(cvae::CVAE, Y, Œ∏, Z) = cvae.f((Y, Œ∏, Z)) # returns (YÃÑ, Œ∏ÃÑ, ZÃÑ, nrm_state)
unnormalize_outputs(state::CVAETrainingState, Œ∏ÃÑM, ZÃÑ) = state.cvae.f‚Åª¬π((Œ∏ÃÑM, ZÃÑ, state.nrm_state)) # returns (Œ∏M, Z)
unnormalize_outputs(state::CVAEInferenceState, Œ∏ÃÑM, ZÃÑ) = state.cvae.f‚Åª¬π((Œ∏ÃÑM, ZÃÑ, state.nrm_state)) # returns (Œ∏M, Z)

# Layer which transforms matrix of [Œº‚Ä≤; logœÉ‚Ä≤] ‚àà [‚Ñù^nz; ‚Ñù^nz] to bounded intervals [Œº; logœÉ] ‚àà [ùíüŒº^nz; ùíülogœÉ^nz]:
#      Œº bounded: prevent CVAE from "memorizing" inputs via mean latent embedding vectors which are far from zero
#   logœÉ bounded: similarly, prevent CVAE from "memorizing" inputs via latent embedding vectors which are nearly constant, i.e. have zero variance
CVAELatentTransform(nz, ùíüŒº = (-3,3), ùíülogœÉ = (-6,0)) = Flux.Chain(
    Base.BroadcastFunction(tanh),
    CatScale([(-1,1) => ùíüŒº, (-1,1) => ùíülogœÉ], [nz, nz]),
)

####
#### CVAE helpers
####

@inline split_at(x::AbstractVecOrMat, n::Int) = n == size(x,1) ? (x, zeros_similar(x, 0, size(x)[2:end]...)) : (x[1:n, ..], x[n+1:end, ..])
split_theta_latent(cvae::CVAE, x::AbstractVecOrMat) = split_at(x, ntheta(cvae))
split_marginal_latent(cvae::CVAE, x::AbstractVecOrMat) = split_at(x, nmarginalized(cvae))

function split_marginal_latent_pairs(cvae::CVAE, x::AbstractVecOrMat)
    Œº1Œ∏_Œº1Z, Œº2Œ∏_Œº2Z = split_dim1(x) # x = [Œº1Œ∏; Œº1Z; Œº2Œ∏; Œº1Z]
    Œº1Œ∏, Œº1Z = split_marginal_latent(cvae, Œº1Œ∏_Œº1Z) # size(Œº1Œ∏,1) = nŒ∏M, size(Œº1Z,1) = nlatent
    Œº2Œ∏, Œº2Z = split_marginal_latent(cvae, Œº2Œ∏_Œº2Z) # size(Œº2Œ∏,1) = nŒ∏M, size(Œº2Z,1) = nlatent
    return (Œº1Œ∏, Œº2Œ∏, Œº1Z, Œº2Z)
end

function pad_signal(Y::AbstractVecOrMat, n)
    if size(Y,1) < n
        pad = zeros_similar(Y, n - size(Y,1), size(Y)[2:end]...)
        vcat(Y, pad)
    elseif size(Y,1) > n
        Y[1:n, ..]
    else
        Y
    end
end
pad_signal(::CVAE{n}, Y) where {n} = pad_signal(Y, n)

function signal_mask(Y::AbstractVecOrMat; minkept::Int = firstindex(Y,1), maxkept::Int = lastindex(Y,1))
    # Create mask which randomly zeroes the tails of the columns of Y. That is, for each column `j` the mask satisfies
    # `m[i <= n‚±º, j] = 1` and `m[i > n‚±º, j] = 0`, with `minkept <= n‚±º <= maxkept` chosen randomly per column `j`
    Irows   = arr_similar(Y, collect(1:size(Y,1)))
    Icutoff = arr_similar(Y, collect(rand(minkept:maxkept, 1, size(Y)[2:end]...)))
    mask    = arr_similar(Y, Irows .<= Icutoff)
    return mask
end

####
#### CVAE methods
####

function KLDivergence(state::CVAETrainingState)
    @unpack Œºq0, logœÉq, Œºr0, logœÉr = state
    KLDivGaussian(Œºq0, logœÉq, Œºr0, logœÉr)
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{Gaussian}}
    @unpack cvae, YÃÑ, Œ∏ÃÑ, ZÃÑ, Œºq0, logœÉq = state
    nŒ∏M = nmarginalized(cvae)
    zq = sample_mv_normal(Œºq0, exp.(logœÉq))
    Œºx0, logœÉx = split_dim1(cvae.D(YÃÑ, zq))
    ELBO = marginalize_Z ?
        NegLogLGaussian(Œ∏ÃÑ[1:nŒ∏M, ..], Œºx0[1:nŒ∏M, ..], logœÉx[1:nŒ∏M, ..]) :
        NegLogLGaussian(vcat(Œ∏ÃÑ[1:nŒ∏M, ..], ZÃÑ), Œºx0, logœÉx)
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
    @unpack cvae, YÃÑ, Œ∏ÃÑ, ZÃÑ, Œºq0, logœÉq = state
    nŒ∏M = nmarginalized(cvae)
    zq = sample_mv_normal(Œºq0, exp.(logœÉq))
    Œºx = cvae.D(YÃÑ, zq) # Œºx = D(YÃÑ, zq) = [ŒºŒ∏ÃÑM; ŒºZÃÑ; logœÉŒ∏ÃÑM; logœÉZÃÑ]
    œÉ‚Åª¬πŒºŒ∏ÃÑM, logœÉŒ∏ÃÑM, ŒºZÃÑ, logœÉZÃÑ = split_marginal_latent_pairs(cvae, Œºx)
    ŒºŒ∏ÃÑM = tanh.(œÉ‚Åª¬πŒºŒ∏ÃÑM) # transform from unbounded œÉ‚Åª¬πŒºŒ∏ÃÑM ‚àà ‚Ñù^nŒ∏ to bounded interval [-1, 1]^nŒ∏
    ELBO_Œ∏ = NegLogLTruncatedGaussian(Œ∏ÃÑ[1:nŒ∏M, ..], ŒºŒ∏ÃÑM, logœÉŒ∏ÃÑM, -1, 1)
    if marginalize_Z
        ELBO = ELBO_Œ∏
    else
        ELBO = ELBO_Œ∏ + NegLogLGaussian(ZÃÑ, ŒºZÃÑ, logœÉZÃÑ)
    end
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{Kumaraswamy}}
    @unpack cvae, YÃÑ, Œ∏ÃÑ, ZÃÑ, Œºq0, logœÉq = state
    nŒ∏M = nmarginalized(cvae)
    zq = sample_mv_normal(Œºq0, exp.(logœÉq))
    Œºx = cvae.D(YÃÑ, zq) # Œºx = D(YÃÑ, zq) = [Œ±Œ∏; ŒºZÃÑ; Œ≤Œ∏; logœÉZÃÑ]
    Œ±Œ∏, Œ≤Œ∏, ŒºZÃÑ, logœÉZÃÑ = split_marginal_latent_pairs(cvae, Œºx)
    ELBO_Œ∏ = NegLogLKumaraswamy(Œ∏ÃÑ[1:nŒ∏M, ..], Œ±Œ∏, Œ≤Œ∏)
    if marginalize_Z
        ELBO = ELBO_Œ∏
    else
        ELBO = ELBO_Œ∏ + NegLogLGaussian(ZÃÑ, ŒºZÃÑ, logœÉZÃÑ)
    end
end

function KL_and_ELBO(state::CVAETrainingState; marginalize_Z::Bool = nlatent(state.cvae) == 0)
    KLDiv = KLDivergence(state)
    ELBO = EvidenceLowerBound(state; marginalize_Z)
    return (; KLDiv, ELBO)
end

KL_and_ELBO(cvae::CVAE, Y, Œ∏, Z = zeros_similar(Œ∏, 0, size(Œ∏,2)); marginalize_Z::Bool = nlatent(cvae) == 0) = KL_and_ELBO(CVAETrainingState(cvae, Y, Œ∏, Z); marginalize_Z)

sampleŒ∏Zposterior(cvae::CVAE, Y; kwargs...) = sampleŒ∏Zposterior(CVAEInferenceState(cvae, Y); kwargs...)

function sampleŒ∏Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{Gaussian}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Œºr0` is the most likely value for `zr` and `Œºx0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, YÃÑ, Œºr0, logœÉr = state
    zr = mode ? Œºr0 : sample_mv_normal(Œºr0, exp.(logœÉr))
    Œºx = cvae.D(YÃÑ, zr)
    Œºx0, logœÉx = split_dim1(Œºx)
    x = mode ? Œºx0 : sample_mv_normal(Œºx0, exp.(logœÉx))
    Œ∏ÃÑM, ZÃÑ = split_marginal_latent(cvae, x)
    Œ∏M, Z = unnormalize_outputs(state, Œ∏ÃÑM, ZÃÑ)
    return Œ∏M, ZÃÑ
end

# function sampleŒ∏Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
#     #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Œºr0` is the most likely value for `zr` and `Œºx0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
#     @unpack cvae, YÃÑ, Œºr0, logœÉr = state
#     zr = mode ? Œºr0 : sample_mv_normal(Œºr0, exp.(logœÉr))
#     Œºx = cvae.D(YÃÑ, zr)
#     œÉ‚Åª¬πŒºŒ∏ÃÑM, logœÉŒ∏ÃÑM, ŒºZÃÑ, logœÉZÃÑ = split_marginal_latent_pairs(cvae, Œºx)
#     ŒºŒ∏ÃÑM = tanh.(œÉ‚Åª¬πŒºŒ∏ÃÑM) # transform from unbounded œÉ‚Åª¬πŒºŒ∏ÃÑM ‚àà ‚Ñù^nŒ∏ to bounded interval [-1, 1]^nŒ∏
#     Œ∏ÃÑM = mode ? ŒºŒ∏ÃÑM : sample_trunc_mv_normal(ŒºŒ∏ÃÑM, exp.(logœÉŒ∏ÃÑM), -1, 1)
#     ZÃÑ = mode || nlatent(state.cvae) == 0 ? ŒºZÃÑ : sample_mv_normal(ŒºZÃÑ, exp.(logœÉZÃÑ))
#     Œ∏M, Z = unnormalize_outputs(state, Œ∏ÃÑM, ZÃÑ)
#     return Œ∏M, Z
# end

function sampleŒ∏Zposterior_state(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Œºr0` is the most likely value for `zr` and `Œºx0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, YÃÑ, Œºr0, logœÉr = state
    zr = mode ? Œºr0 : sample_mv_normal(Œºr0, exp.(logœÉr))
    Œºx = cvae.D(YÃÑ, zr)
    œÉ‚Åª¬πŒºŒ∏ÃÑM, logœÉŒ∏ÃÑM, ŒºZÃÑ, logœÉZÃÑ = split_marginal_latent_pairs(cvae, Œºx)
    ŒºŒ∏ÃÑM = tanh.(œÉ‚Åª¬πŒºŒ∏ÃÑM) # transform from unbounded œÉ‚Åª¬πŒºŒ∏ÃÑM ‚àà ‚Ñù^nŒ∏ to bounded interval [-1, 1]^nŒ∏
    Œ∏ÃÑM = mode ? ŒºŒ∏ÃÑM : sample_trunc_mv_normal(ŒºŒ∏ÃÑM, exp.(logœÉŒ∏ÃÑM), -1, 1)
    ZÃÑ = mode || nlatent(state.cvae) == 0 ? ŒºZÃÑ : sample_mv_normal(ŒºZÃÑ, exp.(logœÉZÃÑ))
    Œ∏M, Z = unnormalize_outputs(state, Œ∏ÃÑM, ZÃÑ)
    return @ntuple(state, zr, ŒºŒ∏ÃÑM, logœÉŒ∏ÃÑM, ŒºZÃÑ, logœÉZÃÑ, Œ∏ÃÑM, ZÃÑ, Œ∏M, Z)
end

function sampleŒ∏Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
    #TODO make proper CVAEPosteriorState struct
    @unpack Œ∏M, Z = sampleŒ∏Zposterior_state(state; mode)
    return Œ∏M, Z
end

function sampleŒ∏Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{Kumaraswamy}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Œºr0` is the most likely value for `zr` and `Œºx0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, YÃÑ, Œºr0, logœÉr = state
    zr = mode ? Œºr0 : sample_mv_normal(Œºr0, exp.(logœÉr))
    Œºx = cvae.D(YÃÑ, zr)
    Œ±Œ∏, Œ≤Œ∏, ŒºZÃÑ, logœÉZÃÑ = split_marginal_latent_pairs(cvae, Œºx)
    Œ∏ÃÑM = mode ? mode_kumaraswamy(Œ±Œ∏, Œ≤Œ∏) : sample_kumaraswamy(Œ±Œ∏, Œ≤Œ∏)
    ZÃÑ = mode || nlatent(state.cvae) == 0 ? ŒºZÃÑ : sample_mv_normal(ŒºZÃÑ, exp.(logœÉZÃÑ))
    Œ∏M, Z = unnormalize_outputs(state, Œ∏ÃÑM, ZÃÑ)
    return Œ∏M, Z
end

function Œ∏Zposterior_sampler(cvae::CVAE, Y)
    state = CVAEInferenceState(cvae, Y) # constant over posterior samples
    Œ∏Zposterior_sampler_inner(; kwargs...) = sampleŒ∏Zposterior(state; kwargs...)
    return Œ∏Zposterior_sampler_inner
end

####
#### Deep prior
####

"""
Deep prior for learning distribution of some œï, wrapping (possibly parameterized) functions `prior` and `noisesource`

    noisesource: ‚àÖ -> R^kœï
    prior : R^kœï -> R^nœï

which generates samples œï ~ prior(œï) from kœï-dimensional samples from noisesource.
"""
struct DeepPrior{T,kœï,FP,FN}
    prior :: FP
    noisesource :: FN
end
DeepPrior{T,kœï}(prior::FP, noisesource::FN) where {T,kœï,FP,FN} = DeepPrior{T,kœï,FP,FN}(prior, noisesource)

Flux.functor(::Type{<:DeepPrior{T,kœï}}, p) where {T,kœï} = (prior = p.prior, noisesource = p.noisesource,), fs -> DeepPrior{T,kœï}(fs...)
Base.show(io::IO, ::DeepPrior{T,kœï}) where {T,kœï} = print(io, "DeepPrior$((;T,kœï))")

(p::DeepPrior{T,kœï})(x::A) where {T, kœï, A <: AbstractVecOrMat{T}} = p.prior(p.noisesource(A, kœï, size(x,2))) # sample from distribution

StatsBase.sample(p::DeepPrior{T}, n::Int) where {T} = StatsBase.sample(p, CuMatrix{T}, n) # default to sampling Œ∏ on the gpu
StatsBase.sample(p::DeepPrior, Y::AbstractVecOrMat, n::Int = size(Y,2)) = StatsBase.sample(p, typeof(Y), n) # Œ∏ type is similar to Y type
StatsBase.sample(p::DeepPrior{T,kœï}, ::Type{A}, n::Int) where {T, kœï, A <: AbstractVecOrMat{T}} = p.prior(p.noisesource(A, kœï, n)) # sample from distribution

const MaybeDeepPrior = Union{Nothing, <:DeepPrior}

#### CVAE + PhysicsModel + DeepPrior + AbstractMetaDataSignal methods

function sampleŒ∏Z(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; posterior_Œ∏ = true, posterior_Z = true, posterior_mode = false)
    if posterior_Œ∏ || posterior_Z
        return sampleŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta, CVAEInferenceState(cvae, signal(Ymeta)); posterior_Œ∏, posterior_Z, posterior_mode)
    else
        Œ∏ = sample(Œ∏prior, signal(Ymeta))
        Z = sample(Zprior, signal(Ymeta))
        return Œ∏, Z
    end
end
sampleŒ∏Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleŒ∏Z(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true)

function sampleŒ∏Z(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal, state::CVAEInferenceState; posterior_Œ∏ = true, posterior_Z = true, posterior_mode = false)
    if posterior_Œ∏ || posterior_Z
        Œ∏ÃÇM, ZÃÇ = sampleŒ∏Zposterior(state; mode = posterior_mode)
        Z = if posterior_Z
            ZÃÇ
        else
            sample(Zprior, signal(Ymeta))
        end
        Œ∏ = if posterior_Œ∏
            Œ∏Mlo = arr_similar(Œ∏ÃÇM, Œ∏marginalized(phys, Œ∏lower(phys)))
            Œ∏Mhi = arr_similar(Œ∏ÃÇM, Œ∏marginalized(phys, Œ∏upper(phys)))
            vcat(clamp.(Œ∏ÃÇM, Œ∏Mlo, Œ∏Mhi), Zygote.@ignore(Œ∏nuissance(phys, Ymeta))) #TODO Zygote.@ignore necessary?
        else
            sample(Œ∏prior, signal(Ymeta))
        end
    else
        Œ∏ = sample(Œ∏prior, signal(Ymeta))
        Z = sample(Zprior, signal(Ymeta))
    end
    return Œ∏, Z
end
sampleŒ∏Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal, state::CVAEInferenceState; kwargs...) = sampleŒ∏Z(phys, cvae, nothing, nothing, Ymeta, state; kwargs..., posterior_Œ∏ = true, posterior_Z = true)

function Œ∏Z_sampler(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal)
    state = CVAEInferenceState(cvae, signal(Ymeta)) # constant over posterior samples
    Œ∏Z_sampler_inner(; kwargs...) = sampleŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta, state; kwargs...)
    return Œ∏Z_sampler_inner
end
Œ∏Z_sampler(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal) = Œ∏Z_sampler(phys, cvae, nothing, nothing, Ymeta)

function sampleXŒ∏Z(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...)
    #TODO: can't differentiate through @timeit "sampleŒ∏Z"
    #TODO: can't differentiate through @timeit "signal_model"
    Œ∏, Z = sampleŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta; kwargs...)
    X = signal_model(phys, Ymeta, Œ∏)
    (size(X,1) > nsignal(Ymeta)) && (X = X[1:nsignal(Ymeta), ..])
    return X, Œ∏, Z
end
sampleXŒ∏Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXŒ∏Z(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true)

sampleX(phys::PhysicsModel, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta; kwargs...)[1]
sampleX(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleX(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true)

function posterior_state(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...)
    Œ∏, Z = sampleŒ∏Z(phys, cvae, Ymeta; posterior_Œ∏ = true, posterior_Z = true, posterior_mode = false, kwargs...)
    posterior_state(phys, Ymeta, Œ∏, Z)
end

@with_kw_noshow struct OnlineMetropolisSampler{T}
    Œ∏::Array{T,3} # parameter values
    ntheta::Int           = size(Œ∏, 1) # number of parameters per data point
    ndata::Int            = size(Œ∏, 2) # number of data points, i.e. each column of Œ∏ represents estimates for a separate datum Y
    nsamples::Int         = size(Œ∏, 3) # length of the Metropolis-Hastings MCMC chain which is recorded
    i::Vector{Int}        = ones(Int, ndata) # current sample index in cyclical chain buffer Œ∏
    accept::Array{Bool,3} = zeros(Bool, 1, ndata, nsamples) # records whether proposal was accepted or not
    neglogPXŒ∏::Array{T,3} = fill(T(Inf), 1, ndata, nsamples) # negative log likelihoods; initialize with Inf to guarantee acceptance of first sample
    neglogPŒ∏::Array{T,3}  = zeros(T, 1, ndata, nsamples) # negative log priors; initialization is moot due to neglogPXŒ∏ initialzed to Inf
    neglogQŒ∏::Array{T,3}  = zeros(T, 1, ndata, nsamples) # proposal distribution negative log likelihood; as opposed to standard MH, assumed to be independent of previous sample, i.e. Q(Œ∏|Œ∏‚Ä≤) ‚â° Q(Œ∏); initialization is moot due to neglogPXŒ∏ initialzed to Inf
end
Base.show(io::IO, s::OnlineMetropolisSampler{T}) where {T} = print(io, "OnlineMetropolisSampler{$(T)}(ntheta = $(s.ntheta), ndata = $(s.ndata), nsamples = $(s.nsamples))")

buffer_index(s::OnlineMetropolisSampler, j::Int) = CartesianIndex(j, mod1(s.i[j], s.nsamples))
random_index(s::OnlineMetropolisSampler, j::Int) = CartesianIndex(j, rand(1:s.nsamples))
buffer_indices(s::OnlineMetropolisSampler, J = 1:s.ndata) = buffer_index.((s,), J)
random_indices(s::OnlineMetropolisSampler, J = 1:s.ndata) = random_index.((s,), J)
function Random.rand(s::OnlineMetropolisSampler, J = 1:s.ndata)
    idx = random_indices(s, J)
    return s.Œ∏[:, idx], s.neglogPXŒ∏[:, idx], s.neglogPŒ∏[:, idx], s.neglogQŒ∏[:, idx]
end

# c.f. https://stats.stackexchange.com/a/163790
function update!(s::OnlineMetropolisSampler, Œ∏‚Ä≤::AbstractMatrix, neglogPXŒ∏‚Ä≤::AbstractMatrix, neglogPŒ∏‚Ä≤::AbstractMatrix, neglogQŒ∏‚Ä≤::AbstractMatrix, J = 1:s.ndata)
    @assert size(Œ∏‚Ä≤, 1) == s.ntheta
    @assert size(Œ∏‚Ä≤, 2) == size(neglogPXŒ∏‚Ä≤, 2) == size(neglogPŒ∏‚Ä≤, 2) == size(neglogQŒ∏‚Ä≤, 2)
    @assert size(neglogPXŒ∏‚Ä≤, 1) == size(neglogPŒ∏‚Ä≤, 1) == size(neglogQŒ∏‚Ä≤, 1) == 1

    # DECAES.tforeach(eachindex(J); blocksize = 16) do j
    Threads.@threads for j in eachindex(J)
        @inbounds begin
            col        = J[j]
            curr       = buffer_index(s, col)
            s.i[col]  += 1
            next       = buffer_index(s, col)

            # Metropolis-Hastings acceptance ratio:
            #        Œ± = min(1, (PXŒ∏‚Ä≤ * PŒ∏‚Ä≤ * QŒ∏) / (PXŒ∏ * PŒ∏ * QŒ∏‚Ä≤))
            # ==> logŒ± = min(0, logPXŒ∏‚Ä≤ + logPŒ∏‚Ä≤ + logQŒ∏ - logPXŒ∏ - logPŒ∏ - logQŒ∏‚Ä≤)
            logŒ±       = min(0, s.neglogPXŒ∏[1,curr] + s.neglogPŒ∏[1,curr] + neglogQŒ∏‚Ä≤[1,j] - neglogPXŒ∏‚Ä≤[1,j] - neglogPŒ∏‚Ä≤[1,j] - s.neglogQŒ∏[1,curr])
            accept     = logŒ± > log(rand())

            # Update theta, negative log likelihoods, and negative log priors with accepted points or current points
            s.accept[1,next]    = accept
            @inbounds for i in 1:size(Œ∏‚Ä≤, 1)
                s.Œ∏[i,next]     = accept ? Œ∏‚Ä≤[i,j]         : s.Œ∏[i,curr]
            end
            s.neglogPXŒ∏[1,next] = accept ? neglogPXŒ∏‚Ä≤[1,j] : s.neglogPXŒ∏[1,curr]
            s.neglogPŒ∏[1,next]  = accept ? neglogPŒ∏‚Ä≤[1,j]  : s.neglogPŒ∏[1,curr]
            s.neglogQŒ∏[1,next]  = accept ? neglogQŒ∏‚Ä≤[1,j]  : s.neglogQŒ∏[1,curr]
        end
    end
end

function update!(s::OnlineMetropolisSampler{T}, phys::EPGModel{T}, cvae::CVAEPosteriorDist{TruncatedGaussian}, img::CPMGImage, Y_gpu, Y_cpu = cpu(T, Y_gpu); img_cols) where {T}
    # Œ∏‚Ä≤, _ = sampleŒ∏Zposterior(cvae, Y_gpu)
    # Œ∏‚Ä≤    = cpu(T, Œ∏‚Ä≤)
    post_state = sampleŒ∏Zposterior_state(CVAEInferenceState(cvae, Y_gpu)) # return @ntuple(state, zr, ŒºŒ∏ÃÑM, logœÉŒ∏ÃÑM, ŒºZÃÑ, logœÉZÃÑ, Œ∏ÃÑM, ZÃÑ, Œ∏M, Z)
    Œ∏‚Ä≤         = cpu(T, post_state.Œ∏M)
    X‚Ä≤         = signal_model(phys, img, Œ∏‚Ä≤)
    neglogPXŒ∏‚Ä≤ = negloglikelihood(phys, Y_cpu, X‚Ä≤, Œ∏‚Ä≤)
    neglogPŒ∏‚Ä≤  = neglogprior(phys, Œ∏‚Ä≤)
    neglogQŒ∏‚Ä≤  = zeros_similar(neglogPŒ∏‚Ä≤)
    # neglogQŒ∏‚Ä≤= cpu(T, sum(neglogL_trunc_gaussian(post_state.Œ∏ÃÑM, post_state.ŒºŒ∏ÃÑM, post_state.logœÉŒ∏ÃÑM, -one(T), one(T)); dims = 1))

    # NOTE: nevermind, don't do this; would need previous zr which corresponded with previous Œ∏
    # # Need to update proposal distribution (CVAE) likelihood for previous sample as well, since the proposal distribution has changed
    # idx                = buffer_indices(s, img_cols)
    # Œ∏                  = gpu(T, s.Œ∏[:, idx])
    # Z                  = zeros_similar(Œ∏, 0, size(Œ∏, 2))
    # _, Œ∏ÃÑ, _, _         = normalize_inputs(cvae, Y_gpu, Œ∏, Z)
    # s.neglogQŒ∏[:, idx] = cpu(T, sum(neglogL_trunc_gaussian(Œ∏ÃÑ, post_state.ŒºŒ∏ÃÑM, post_state.logœÉŒ∏ÃÑM, -one(T), one(T)); dims = 1))
    # @show mean(neglogPXŒ∏‚Ä≤)
    # @show mean(neglogPŒ∏‚Ä≤)
    # @show mean(neglogQŒ∏‚Ä≤)
    # @show mean(s.neglogQŒ∏[:, idx])

    update!(s, Œ∏‚Ä≤, neglogPXŒ∏‚Ä≤, neglogPŒ∏‚Ä≤, neglogQŒ∏‚Ä≤, img_cols)
end

function update!(s::OnlineMetropolisSampler{T}, phys::EPGModel{T}, cvae::CVAE, img::CPMGImage; dataset::Symbol, gpu_batch_size::Int) where {T}
    Y         = img.partitions[dataset]
    J_ranges  = collect(Iterators.partition(1:size(Y, 2), gpu_batch_size))
    for (i, (Y_gpu,)) in enumerate(CUDA.CuIterator((Y[:, J],) for J in J_ranges))
        Y_cpu = Y[:, J_ranges[i]]
        update!(s, phys, cvae, img, Y_gpu, Y_cpu; img_cols = J_ranges[i])
    end
end

function _test_online_mh_sampler(phys::EPGModel)
    ndata = 10
    nsamples = 10000

    # initialize sampler with uniformly random samples
    Œ∏lo, Œ∏hi = Œ∏lower(phys), Œ∏upper(phys)
    Œ∏ = reshape(sample_uniform(Œ∏lo, Œ∏hi, ndata * nsamples), :, ndata, nsamples)
    s = OnlineMetropolisSampler{Float64}(Œ∏ = Œ∏)

    # true answers
    Œ∏_plot = Œ∏lo .+ (Œ∏hi .- Œ∏lo) .* range(0,1,length=200)'
    PŒ∏_plot = neglogpriors(phys, Œ∏_plot) .|> neglogp -> exp(-neglogp)

    while true
        # draw uniform random guess and update sampler
        J = 1:10 # rand(1:ndata√∑4) : rand(3*(ndata√∑4):ndata)
        Œ∏‚Ä≤ = sample_uniform(Œ∏lo, Œ∏hi, length(J))
        neglogPXŒ∏‚Ä≤ = zeros_similar(Œ∏‚Ä≤, 1, length(J)) # constant zero; we are just trying to reproduce the prior
        neglogPŒ∏‚Ä≤ = neglogprior(phys, Œ∏‚Ä≤)
        neglogQŒ∏‚Ä≤ = zeros_similar(neglogPŒ∏‚Ä≤)
        update!(s, Œ∏‚Ä≤, neglogPXŒ∏‚Ä≤, neglogPŒ∏‚Ä≤, neglogQŒ∏‚Ä≤, J)

        # plot compared to expected prior pdf
        if mod(s.i[1], nsamples√∑2) == 0
            plot(
                map(1:7) do i
                    p = plot()
                    stephist!(p, s.Œ∏[i, rand(1:ndata), :]; label = Œ∏labels(phys)[i], normalized = :pdf)
                    plot!(p, Œ∏_plot[i, :], PŒ∏_plot[i, :]; label = :none)
                    p
                end...,
                stephist(s.neglogPXŒ∏[1, rand(1:ndata), :]; label = L"\log{P(X|\theta)}", normalized = :pdf),
                stephist(s.neglogPŒ∏[1, rand(1:ndata), :]; label = L"\log{P(\theta)}", normalized = :pdf);
            ) |> display
            sleep(0.1)
        end
    end
end

####
#### Rician posterior state
####

sampleXÃÇ(rice::RicianCorrector, X, Z = nothing, ninstances = nothing) = sample_rician_state(rice, X, Z, ninstances).XÃÇ

function sampleXÃÇŒ∏Z(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...)
    #TODO: can't differentiate through @timeit "sampleXŒ∏Z"
    #TODO: can't differentiate through @timeit "sampleXÃÇ"
    X, Œ∏, Z = sampleXŒ∏Z(phys, cvae, Œ∏prior, Zprior, Ymeta; kwargs...)
    XÃÇ = sampleXÃÇ(rice, X, Z)
    return XÃÇ, Œ∏, Z
end
sampleXÃÇŒ∏Z(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÃÇŒ∏Z(phys, rice, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true)

sampleXÃÇ(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Œ∏prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÃÇŒ∏Z(phys, rice, cvae, Œ∏prior, Zprior, Ymeta; kwargs...)[1]
sampleXÃÇ(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÃÇ(phys, rice, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Œ∏ = true, posterior_Z = true)

function NegLogLikelihood(::PhysicsModel, rice::RicianCorrector, Y::AbstractVecOrMat, Œº0, œÉ)
    if typeof(rice) <: NormalizedRicianCorrector && (rice.normalizer !== nothing)
        # Approximate the normalization factor as the normalization factor of the mean signal.
        # For gaussian noise mean signal = Œº0, but for rician noise mean signal ~ sqrt(Œº0^2 + œÉ^2), at least when Œº0 >> œÉ
        s = inv.(rice.normalizer(mean_rician.(Œº0, œÉ)))
        neglogL_rician.(Y, s .* Œº0, log.(s .* œÉ)) # Rician negative log likelihood
    else
        neglogL_rician.(Y, Œº0, log.(œÉ)) # Rician negative log likelihood
    end
end

function NegLogLikelihood(::EPGModel, rice::RicianCorrector, Y::AbstractVecOrMat, Œº0, œÉ)
    # Likelihood is "maximimally generous" w.r.t. normalization factor, i.e. we perform MLE to find optimal scaling factor
    logs = Zygote.@ignore begin
        _, results = mle_biexp_epg_noise_only(Œº0, Y, log.(œÉ); freeze_logœµ = true, freeze_logs = false, verbose = false)
        arr_similar(Y, permutedims(results.logscale))
    end
    neglogL_rician.(Y, exp.(logs) .* Œº0, logs .+ log.(œÉ)) # Rician negative log likelihood
end

function posterior_state(phys::PhysicsModel, rice::RicianCorrector, Y::AbstractVecOrMat, Œ∏::AbstractVecOrMat, Z::AbstractVecOrMat; accum_loss = ‚Ñì -> sum(‚Ñì; dims = 1))
    X = signal_model(phys, Œ∏)
    @unpack Œ¥, œµ, ŒΩ = rician_state(rice, X, Z)
    X, Œ¥, œµ, ŒΩ = clamp_dim1(Y, (X, Œ¥, œµ, ŒΩ))
    ‚Ñì = NegLogLikelihood(phys, rice, Y, ŒΩ, œµ)
    (accum_loss !== nothing) && (‚Ñì = accum_loss(‚Ñì))
    return (; Y, Œ∏, Z, X, Œ¥, œµ, ŒΩ, ‚Ñì)
end

function posterior_state(
        phys::PhysicsModel,
        rice::RicianCorrector,
        cvae::CVAE,
        Ymeta::AbstractMetaDataSignal{T};
        miniter = 5,
        maxiter = 100,
        alpha = 0.01,
        mode = :maxlikelihood,
        verbose = false
    ) where {T}

    (mode === :mode) && (miniter = maxiter = 1)
    Œ∏Z_sampler_instance = Œ∏Z_sampler(phys, cvae, Ymeta)
    new_posterior_state(Œ∏new, Znew) = posterior_state(phys, rice, signal(Ymeta), Œ∏new, Znew; accum_loss = ‚Ñì -> sum(‚Ñì; dims = 1))

    function update(last_state, i)
        Œ∏new, Znew = Œ∏Z_sampler_instance(posterior_mode = mode === :mode)
        Œ∏last = (last_state === nothing) ? nothing : last_state.Œ∏
        Zlast = (last_state === nothing) ? nothing : last_state.Z

        if mode === :mean
            Œ∏new = (last_state === nothing) ? Œ∏new : T(1/i) .* Œ∏new .+ T(1-1/i) .* Œ∏last
            Znew = (last_state === nothing) ? Znew : T(1/i) .* Znew .+ T(1-1/i) .* Zlast
            new_state = new_posterior_state(Œ∏new, Znew)
        elseif mode === :mode
            new_state = new_posterior_state(Œ∏new, Znew)
        elseif mode === :maxlikelihood
            new_state = new_posterior_state(Œ∏new, Znew)
            if (last_state !== nothing)
                mask = new_state.‚Ñì .< last_state.‚Ñì
                new_state = map(new_state, last_state) do new, last
                    new .= ifelse.(mask, new, last)
                end
            end
        else
            error("Unknown mode: $mode")
        end

        # Check for convergence
        p = (last_state === nothing) ? nothing :
            HypothesisTests.pvalue(
                HypothesisTests.UnequalVarianceTTest(
                    map(x -> x |> cpu |> vec |> Vector{Float64}, (new_state.‚Ñì, last_state.‚Ñì))...
                )
            )

        return new_state, p
    end

    state, _ = update(nothing, 1)
    verbose && @info 1, mean_and_std(state.‚Ñì)
    for i in 2:maxiter
        state, p = update(state, i)
        verbose && @info i, mean_and_std(state.‚Ñì), p
        (i >= miniter) && (p > 1 - alpha) && break
    end

    return state
end

#= TODO: update to return both Œ∏ and Z means + stddevs?
function (cvae::CVAE)(Y; nsamples::Int = 1, stddev::Bool = false)
    @assert nsamples ‚â• ifelse(stddev, 2, 1)
    smooth(a, b, Œ≥) = a + Œ≥ * (b - a)
    Œ∏Zsampler = Œ∏Zposterior_sampler(cvae, Y)
    Œºx = Œ∏Zsampler()
    Œºx_last, œÉx2 = zero(Œºx), zero(Œºx)
    for i in 2:nsamples
        x = Œ∏Zsampler()
        Œºx_last .= Œºx
        Œºx .= smooth.(Œºx, x, 1//i)
        œÉx2 .= smooth.(œÉx2, (x .- Œºx) .* (x .- Œºx_last), 1//i)
    end
    return stddev ? vcat(Œºx, sqrt.(œÉx2)) : Œºx
end
=#
