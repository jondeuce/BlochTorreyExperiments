"""
Conditional variational autoencoder.

Architecture inspired by:
    "Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy"
    https://arxiv.org/abs/1802.08797
"""
struct CVAE{n,nÎ¸,nÎ¸M,k,nz,Dist,E1,E2,D,F,Fâ»Â¹}
    E1  :: E1
    E2  :: E2
    D   :: D
    f   :: F
    fâ»Â¹ :: Fâ»Â¹
end
CVAE{n,nÎ¸,nÎ¸M,k,nz}(enc1::E1, enc2::E2, dec::D, f::F, fâ»Â¹::Fâ»Â¹; posterior_dist::Type = Gaussian) where {n,nÎ¸,nÎ¸M,k,nz,E1,E2,D,F,Fâ»Â¹} = CVAE{n,nÎ¸,nÎ¸M,k,nz,posterior_dist,E1,E2,D,F,Fâ»Â¹}(enc1, enc2, dec, f, fâ»Â¹)

const CVAEPosteriorDist{Dist} = CVAE{n,nÎ¸,nÎ¸M,k,nz,Dist} where {n,nÎ¸,nÎ¸M,k,nz}

Flux.functor(::Type{<:CVAE{n,nÎ¸,nÎ¸M,k,nz,Dist}}, c) where {n,nÎ¸,nÎ¸M,k,nz,Dist} = (E1 = c.E1, E2 = c.E2, D = c.D, f = c.f, fâ»Â¹ = c.fâ»Â¹,), fs -> CVAE{n,nÎ¸,nÎ¸M,k,nz}(fs...; posterior_dist = Dist)
Base.show(io::IO, ::CVAE{n,nÎ¸,nÎ¸M,k,nz,Dist}) where {n,nÎ¸,nÎ¸M,k,nz,Dist} = print(io, "CVAE$((;n,nÎ¸,nÎ¸M,k,nz,Dist))")

nsignal(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = n
ntheta(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = nÎ¸
nmarginalized(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = nÎ¸M
nlatent(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = k
nembedding(::CVAE{n,nÎ¸,nÎ¸M,k,nz}) where {n,nÎ¸,nÎ¸M,k,nz} = nz

struct CVAETrainingState{C <: CVAE, A, S}
    cvae::C
    YÌ„::A
    Î¸Ì„::A
    ZÌ„::A
    Î¼r0::A
    logÏƒr::A
    Î¼q0::A
    logÏƒq::A
    nrm_state::S
end

function CVAETrainingState(cvae::CVAE, Y, Î¸, Z = zeros_similar(Î¸, 0, size(Î¸,2)))
    YÌ„, Î¸Ì„, ZÌ„, nrm_state = normalize(cvae, Y, Î¸, Z)
    YÌ„pad = pad_signal(cvae, YÌ„)
    Î¼r = cvae.E1(YÌ„pad)
    Î¼q = cvae.E2(YÌ„pad, Î¸Ì„, ZÌ„)
    Î¼r0, logÏƒr = split_dim1(Î¼r)
    Î¼q0, logÏƒq = split_dim1(Î¼q)
    return CVAETrainingState(cvae, YÌ„pad, Î¸Ì„, ZÌ„, Î¼r0, logÏƒr, Î¼q0, logÏƒq, nrm_state)
end
signal(state::CVAETrainingState) = state.Y

struct CVAEInferenceState{C <: CVAE, A, S}
    cvae::C
    YÌ„::A
    Î¼r0::A
    logÏƒr::A
    nrm_state::S
end

function CVAEInferenceState(cvae::CVAE, Y)
    YÌ„, nrm_state = normalize(cvae, Y)
    YÌ„pad = pad_signal(cvae, YÌ„)
    Î¼r = cvae.E1(YÌ„pad)
    Î¼r0, logÏƒr = split_dim1(Î¼r)
    return CVAEInferenceState(cvae, YÌ„pad, Î¼r0, logÏƒr, nrm_state)
end
signal(state::CVAEInferenceState) = state.Y

LinearAlgebra.normalize(cvae::CVAE, Y) = cvae.f((Y,)) # returns (YÌ„, nrm_state)
LinearAlgebra.normalize(cvae::CVAE, Y, Î¸, Z) = cvae.f((Y, Î¸, Z)) # returns (YÌ„, Î¸Ì„, ZÌ„, nrm_state)
unnormalize(state::CVAETrainingState, Î¸Ì„M, ZÌ„) = state.cvae.fâ»Â¹((Î¸Ì„M, ZÌ„, state.nrm_state)) # returns (Î¸M, Z)
unnormalize(state::CVAEInferenceState, Î¸Ì„M, ZÌ„) = state.cvae.fâ»Â¹((Î¸Ì„M, ZÌ„, state.nrm_state)) # returns (Î¸M, Z)

# Layer which transforms matrix of [Î¼â€²; logÏƒâ€²] âˆˆ [â„^nz; â„^nz] to bounded intervals [Î¼; logÏƒ] âˆˆ [ğ’ŸÎ¼^nz; ğ’ŸlogÏƒ^nz]:
#      Î¼ bounded: prevent CVAE from "memorizing" inputs via mean latent embedding vectors which are far from zero
#   logÏƒ bounded: similarly, prevent CVAE from "memorizing" inputs via latent embedding vectors which are nearly constant, i.e. have zero variance
CVAELatentTransform(nz, ğ’ŸÎ¼ = (-3,3), ğ’ŸlogÏƒ = (-6,0)) = Flux.Chain(
    Base.BroadcastFunction(tanh),
    CatScale([(-1,1) => ğ’ŸÎ¼, (-1,1) => ğ’ŸlogÏƒ], [nz, nz]),
)

####
#### CVAE helpers
####

@inline split_at(x::AbstractVecOrMat, n::Int) = n == size(x,1) ? (x, zeros_similar(x, 0, size(x)[2:end]...)) : (x[1:n, ..], x[n+1:end, ..])
split_theta_latent(cvae::CVAE, x::AbstractVecOrMat) = split_at(x, ntheta(cvae))
split_marginal_latent(cvae::CVAE, x::AbstractVecOrMat) = split_at(x, nmarginalized(cvae))

function split_marginal_latent_pairs(cvae::CVAE, x::AbstractVecOrMat)
    Î¼1Î¸_Î¼1Z, Î¼2Î¸_Î¼2Z = split_dim1(x) # x = [Î¼1Î¸; Î¼1Z; Î¼2Î¸; Î¼1Z]
    Î¼1Î¸, Î¼1Z = split_marginal_latent(cvae, Î¼1Î¸_Î¼1Z) # size(Î¼1Î¸,1) = nÎ¸M, size(Î¼1Z,1) = nlatent
    Î¼2Î¸, Î¼2Z = split_marginal_latent(cvae, Î¼2Î¸_Î¼2Z) # size(Î¼2Î¸,1) = nÎ¸M, size(Î¼2Z,1) = nlatent
    return (Î¼1Î¸, Î¼2Î¸, Î¼1Z, Î¼2Z)
end

function pad_signal(Y::AbstractVecOrMat, n)
    if size(Y,1) < n
        pad = zeros_similar(Y, n - size(Y,1), size(Y)[2:end]...)
        vcat(Y, pad)
    elseif size(Y,1) > n
        Y[1:n, ..]
    else
        Y
    end
end
pad_signal(::CVAE{n}, Y) where {n} = pad_signal(Y, n)

function signal_mask(Y::AbstractVecOrMat; minkept::Int = firstindex(Y,1), maxkept::Int = lastindex(Y,1))
    # Create mask which randomly zeroes the tails of the columns of Y. That is, for each column `j` the mask satisfies
    # `m[i <= nâ±¼, j] = 1` and `m[i > nâ±¼, j] = 0`, with `minkept <= nâ±¼ <= maxkept` chosen randomly per column `j`
    Irows   = arr_similar(Y, collect(1:size(Y,1)))
    Icutoff = arr_similar(Y, collect(rand(minkept:maxkept, 1, size(Y)[2:end]...)))
    mask    = arr_similar(Y, Irows .<= Icutoff)
    return mask
end

####
#### CVAE methods
####

function KLDivergence(state::CVAETrainingState)
    @unpack Î¼q0, logÏƒq, Î¼r0, logÏƒr = state
    KLDivGaussian(Î¼q0, logÏƒq, Î¼r0, logÏƒr)
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{Gaussian}}
    @unpack cvae, YÌ„, Î¸Ì„, ZÌ„, Î¼q0, logÏƒq = state
    nÎ¸M = nmarginalized(cvae)
    zq = sample_mv_normal(Î¼q0, exp.(logÏƒq))
    Î¼x0, logÏƒx = split_dim1(cvae.D(YÌ„, zq))
    ELBO = marginalize_Z ?
        NegLogLGaussian(Î¸Ì„[1:nÎ¸M, ..], Î¼x0[1:nÎ¸M, ..], logÏƒx[1:nÎ¸M, ..]) :
        NegLogLGaussian(vcat(Î¸Ì„[1:nÎ¸M, ..], ZÌ„), Î¼x0, logÏƒx)
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
    @unpack cvae, YÌ„, Î¸Ì„, ZÌ„, Î¼q0, logÏƒq = state
    nÎ¸M = nmarginalized(cvae)
    zq = sample_mv_normal(Î¼q0, exp.(logÏƒq))
    Î¼x = cvae.D(YÌ„, zq) # Î¼x = D(YÌ„, zq) = [Î¼Î¸Ì„M; Î¼ZÌ„; logÏƒÎ¸Ì„M; logÏƒZÌ„]
    Ïƒâ»Â¹Î¼Î¸Ì„M, logÏƒÎ¸Ì„M, Î¼ZÌ„, logÏƒZÌ„ = split_marginal_latent_pairs(cvae, Î¼x)
    Î¼Î¸Ì„M = tanh.(Ïƒâ»Â¹Î¼Î¸Ì„M) # transform from unbounded Ïƒâ»Â¹Î¼Î¸Ì„M âˆˆ â„^nÎ¸ to bounded interval [-1, 1]^nÎ¸
    ELBO_Î¸ = NegLogLTruncatedGaussian(Î¸Ì„[1:nÎ¸M, ..], Î¼Î¸Ì„M, logÏƒÎ¸Ì„M, -1, 1)
    if marginalize_Z
        ELBO = ELBO_Î¸
    else
        ELBO = ELBO_Î¸ + NegLogLGaussian(ZÌ„, Î¼ZÌ„, logÏƒZÌ„)
    end
end

function EvidenceLowerBound(state::CVAETrainingState{C}; marginalize_Z::Bool = nlatent(state.cvae) == 0) where {C <: CVAEPosteriorDist{Kumaraswamy}}
    @unpack cvae, YÌ„, Î¸Ì„, ZÌ„, Î¼q0, logÏƒq = state
    nÎ¸M = nmarginalized(cvae)
    zq = sample_mv_normal(Î¼q0, exp.(logÏƒq))
    Î¼x = cvae.D(YÌ„, zq) # Î¼x = D(YÌ„, zq) = [Î±Î¸; Î¼ZÌ„; Î²Î¸; logÏƒZÌ„]
    Î±Î¸, Î²Î¸, Î¼ZÌ„, logÏƒZÌ„ = split_marginal_latent_pairs(cvae, Î¼x)
    ELBO_Î¸ = NegLogLKumaraswamy(Î¸Ì„[1:nÎ¸M, ..], Î±Î¸, Î²Î¸)
    if marginalize_Z
        ELBO = ELBO_Î¸
    else
        ELBO = ELBO_Î¸ + NegLogLGaussian(ZÌ„, Î¼ZÌ„, logÏƒZÌ„)
    end
end

function KL_and_ELBO(state::CVAETrainingState; marginalize_Z::Bool = nlatent(state.cvae) == 0)
    KLDiv = KLDivergence(state)
    ELBO = EvidenceLowerBound(state; marginalize_Z)
    return (; KLDiv, ELBO)
end

KL_and_ELBO(cvae::CVAE, Y, Î¸, Z = zeros_similar(Î¸, 0, size(Î¸,2)); marginalize_Z::Bool = nlatent(cvae) == 0) = KL_and_ELBO(CVAETrainingState(cvae, Y, Î¸, Z); marginalize_Z)

sampleÎ¸Zposterior(cvae::CVAE, Y; kwargs...) = sampleÎ¸Zposterior(CVAEInferenceState(cvae, Y); kwargs...)

function sampleÎ¸Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{Gaussian}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Î¼r0` is the most likely value for `zr` and `Î¼x0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, YÌ„, Î¼r0, logÏƒr = state
    zr = mode ? Î¼r0 : sample_mv_normal(Î¼r0, exp.(logÏƒr))
    Î¼x = cvae.D(YÌ„, zr)
    Î¼x0, logÏƒx = split_dim1(Î¼x)
    x = mode ? Î¼x0 : sample_mv_normal(Î¼x0, exp.(logÏƒx))
    Î¸Ì„M, ZÌ„ = split_marginal_latent(cvae, x)
    Î¸M, Z = unnormalize(state, Î¸Ì„M, ZÌ„)
    return Î¸M, ZÌ„
end

function sampleÎ¸Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{TruncatedGaussian}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Î¼r0` is the most likely value for `zr` and `Î¼x0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, YÌ„, Î¼r0, logÏƒr = state
    nÎ¸M = nmarginalized(cvae)
    zr = mode ? Î¼r0 : sample_mv_normal(Î¼r0, exp.(logÏƒr))
    Î¼x = cvae.D(YÌ„, zr)
    Ïƒâ»Â¹Î¼Î¸Ì„M, logÏƒÎ¸Ì„M, Î¼ZÌ„, logÏƒZÌ„ = split_marginal_latent_pairs(cvae, Î¼x)
    Î¼Î¸Ì„M = tanh.(Ïƒâ»Â¹Î¼Î¸Ì„M) # transform from unbounded Ïƒâ»Â¹Î¼Î¸Ì„M âˆˆ â„^nÎ¸ to bounded interval [-1, 1]^nÎ¸
    Î¸Ì„M = mode ? Î¼Î¸Ì„M : sample_trunc_mv_normal(Î¼Î¸Ì„M, exp.(logÏƒÎ¸Ì„M), -1, 1)
    ZÌ„ = mode || nlatent(state.cvae) == 0 ? Î¼ZÌ„ : sample_mv_normal(Î¼ZÌ„, exp.(logÏƒZÌ„))
    Î¸M, Z = unnormalize(state, Î¸Ì„M, ZÌ„)
    return Î¸M, Z
end

function sampleÎ¸Zposterior(state::CVAEInferenceState{C}; mode = false) where {C <: CVAEPosteriorDist{Kumaraswamy}}
    #TODO: `mode` is probably not strictly the correct term, but in practice it should be something akin to the distribution mode since `Î¼r0` is the most likely value for `zr` and `Î¼x0` is the most likely value for `x` **conditional on `zr`**; likely there are counterexamples to this simple reasoning, though...
    @unpack cvae, YÌ„, Î¼r0, logÏƒr = state
    zr = mode ? Î¼r0 : sample_mv_normal(Î¼r0, exp.(logÏƒr))
    Î¼x = cvae.D(YÌ„, zr)
    Î±Î¸, Î²Î¸, Î¼ZÌ„, logÏƒZÌ„ = split_marginal_latent_pairs(cvae, Î¼x)
    Î¸Ì„M = mode ? mode_kumaraswamy(Î±Î¸, Î²Î¸) : sample_kumaraswamy(Î±Î¸, Î²Î¸)
    ZÌ„ = mode || nlatent(state.cvae) == 0 ? Î¼ZÌ„ : sample_mv_normal(Î¼ZÌ„, exp.(logÏƒZÌ„))
    Î¸M, Z = unnormalize(state, Î¸Ì„M, ZÌ„)
    return Î¸M, Z
end

function Î¸Zposterior_sampler(cvae::CVAE, Y; kwargs...)
    state = CVAEInferenceState(cvae, Y) # constant over posterior samples
    Î¸Zposterior_sampler_inner() = sampleÎ¸Zposterior(state; kwargs...)
    return Î¸Zposterior_sampler_inner
end

####
#### Deep prior
####

"""
Deep prior for learning distribution of some Ï•, wrapping (possibly parameterized) functions `prior` and `noisesource`

    noisesource: âˆ… -> R^kÏ•
    prior : R^kÏ• -> R^nÏ•

which generates samples Ï• ~ prior(Ï•) from kÏ•-dimensional samples from noisesource.
"""
struct DeepPrior{T,kÏ•,FP,FN}
    prior :: FP
    noisesource :: FN
end
DeepPrior{T,kÏ•}(prior::FP, noisesource::FN) where {T,kÏ•,FP,FN} = DeepPrior{T,kÏ•,FP,FN}(prior, noisesource)

Flux.functor(::Type{<:DeepPrior{T,kÏ•}}, p) where {T,kÏ•} = (prior = p.prior, noisesource = p.noisesource,), fs -> DeepPrior{T,kÏ•}(fs...)
Base.show(io::IO, ::DeepPrior{T,kÏ•}) where {T,kÏ•} = print(io, "DeepPrior$((;T,kÏ•))")

(p::DeepPrior{T,kÏ•})(x::A) where {T, kÏ•, A <: AbstractVecOrMat{T}} = p.prior(p.noisesource(A, kÏ•, size(x,2))) # sample from distribution

StatsBase.sample(p::DeepPrior{T}, n::Int) where {T} = StatsBase.sample(p, CuMatrix{T}, n) # default to sampling Î¸ on the gpu
StatsBase.sample(p::DeepPrior, Y::AbstractVecOrMat, n::Int = size(Y,2)) = StatsBase.sample(p, typeof(Y), n) # Î¸ type is similar to Y type
StatsBase.sample(p::DeepPrior{T,kÏ•}, ::Type{A}, n::Int) where {T, kÏ•, A <: AbstractVecOrMat{T}} = p.prior(p.noisesource(A, kÏ•, n)) # sample from distribution

const MaybeDeepPrior = Union{Nothing, <:DeepPrior}

#### CVAE + PhysicsModel + DeepPrior + AbstractMetaDataSignal methods

function sampleÎ¸Z(phys::PhysicsModel, cvae::CVAE, Î¸prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; posterior_Î¸ = true, posterior_Z = true, posterior_mode = false)
    if posterior_Î¸ || posterior_Z
        return sampleÎ¸Z(phys, cvae, Î¸prior, Zprior, Ymeta, CVAEInferenceState(cvae, signal(Ymeta)); posterior_Î¸, posterior_Z, posterior_mode)
    else
        Î¸ = sample(Î¸prior, signal(Ymeta))
        Z = sample(Zprior, signal(Ymeta))
        return Î¸, Z
    end
end
sampleÎ¸Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleÎ¸Z(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Î¸ = true, posterior_Z = true) # no prior passed -> posterior_Î¸ = posterior_Z = true

function sampleÎ¸Z(phys::PhysicsModel, cvae::CVAE, Î¸prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal, state::CVAEInferenceState; posterior_Î¸ = true, posterior_Z = true, posterior_mode = false)
    if posterior_Î¸ || posterior_Z
        Î¸Ì‚M, ZÌ‚ = sampleÎ¸Zposterior(state; mode = posterior_mode)
        Z = if posterior_Z
            ZÌ‚
        else
            sample(Zprior, signal(Ymeta))
        end
        Î¸ = if posterior_Î¸
            Î¸Mlo = arr_similar(Î¸Ì‚M, Î¸marginalized(phys, Î¸lower(phys)))
            Î¸Mhi = arr_similar(Î¸Ì‚M, Î¸marginalized(phys, Î¸upper(phys)))
            vcat(clamp.(Î¸Ì‚M, Î¸Mlo, Î¸Mhi), Zygote.@ignore(Î¸nuissance(phys, Ymeta))) #TODO Zygote.@ignore necessary?
        else
            sample(Î¸prior, signal(Ymeta))
        end
    else
        Î¸ = sample(Î¸prior, signal(Ymeta))
        Z = sample(Zprior, signal(Ymeta))
    end
    return Î¸, Z
end
sampleÎ¸Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal, state::CVAEInferenceState; kwargs...) = sampleÎ¸Z(phys, cvae, nothing, nothing, Ymeta, state; kwargs..., posterior_Î¸ = true, posterior_Z = true) # no prior passed -> posterior_Î¸ = posterior_Z = true

function Î¸Z_sampler(phys::PhysicsModel, cvae::CVAE, Î¸prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...)
    state = CVAEInferenceState(cvae, signal(Ymeta)) # constant over posterior samples
    Î¸Z_sampler_inner() = sampleÎ¸Z(phys, cvae, Î¸prior, Zprior, Ymeta, state; kwargs...)
    return Î¸Z_sampler_inner
end
Î¸Z_sampler(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = Î¸Z_sampler(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Î¸ = true, posterior_Z = true) # no prior passed -> posterior_Î¸ = posterior_Z = true

function sampleXÎ¸Z(phys::PhysicsModel, cvae::CVAE, Î¸prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...)
    #TODO: can't differentiate through @timeit "sampleÎ¸Z"
    #TODO: can't differentiate through @timeit "signal_model"
    Î¸, Z = sampleÎ¸Z(phys, cvae, Î¸prior, Zprior, Ymeta; kwargs...)
    X = signal_model(phys, Ymeta, Î¸)
    (size(X,1) > nsignal(Ymeta)) && (X = X[1:nsignal(Ymeta), ..])
    return X, Î¸, Z
end
sampleXÎ¸Z(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÎ¸Z(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Î¸ = true, posterior_Z = true) # no prior passed -> posterior_Î¸ = posterior_Z = true

sampleX(phys::PhysicsModel, cvae::CVAE, Î¸prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÎ¸Z(phys, cvae, Î¸prior, Zprior, Ymeta; kwargs...)[1]
sampleX(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleX(phys, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Î¸ = true, posterior_Z = true) # no prior passed -> posterior_Î¸ = posterior_Z = true

function posterior_state(phys::PhysicsModel, cvae::CVAE, Ymeta::AbstractMetaDataSignal; accum_loss = â„“ -> sum(â„“; dims = 1), kwargs...)
    Î¸, Z = sampleÎ¸Z(phys, cvae, Ymeta; posterior_Î¸ = true, posterior_Z = true, posterior_mode = false, kwargs...)
    posterior_state(phys, Ymeta, Î¸, Z; accum_loss)
end

@with_kw_noshow struct OnlineMetropolisSampler{T}
    n::Int
    Î¸::Array{T,3} # parameter values
    neglogPXÎ¸::Array{T,3} = fill(T(Inf), 1, size(Î¸, 2), n) # negative log likelihoods
    neglogPÎ¸::Array{T,3} = fill(T(Inf), 1, size(Î¸, 2), n) # negative log priors
    i::Vector{Int} = ones(Int, size(Î¸, 2)) # current sample index
end
Base.show(io::IO, s::OnlineMetropolisSampler{T}) where {T} = print(io, "OnlineMetropolisSampler{T}(ntheta = $(size(s.Î¸,1)), ndata = $(size(s.Î¸,2)), nsamples = $(s.n))")

buffer_indices(s::OnlineMetropolisSampler, J = 1:size(s.Î¸, 2)) = CartesianIndex.(J, mod1.(s.i[J], s.n))

# c.f. https://stats.stackexchange.com/a/163790
function update!(s::OnlineMetropolisSampler, Î¸â€²::A, neglogPXÎ¸â€²::A, neglogPÎ¸â€²::A, J = 1:size(s.Î¸, 2)) where {A <: AbstractMatrix}
    # Extract copies of current theta, negative log likelihood, and negative log prior state
    Î¸â€²         = arr_similar(s.Î¸, Î¸â€²)
    neglogPXÎ¸â€² = arr_similar(s.Î¸, neglogPXÎ¸â€²)
    neglogPÎ¸â€²  = arr_similar(s.Î¸, neglogPÎ¸â€²)
    idx        = buffer_indices(s, J)
    Î¸          = s.Î¸[:, idx]
    neglogPXÎ¸  = s.neglogPXÎ¸[:, idx]
    neglogPÎ¸   = s.neglogPÎ¸[:, idx]

    # Metropolis-Hastings acceptance ratio:
    #        Î± = min(1, (PXÎ¸â€² * PÎ¸â€²) / (PXÎ¸ * PÎ¸))
    # ==> logÎ± = min(0, logPXÎ¸â€² + logPÎ¸â€² - logPXÎ¸ - logPÎ¸)
    logÎ±       = @. min(0, neglogPXÎ¸ + neglogPÎ¸ - neglogPXÎ¸â€² - neglogPÎ¸â€²)
    accept     = vec(logÎ± .> log.(rand_similar(logÎ±)))

    # Update theta, negative log likelihoods, and negative log priors with accepted points,
    # increment sample counters, and copy updated values into sample caches
    # accepted_slice = CartesianIndex.(J[accept], mod1.(s.i[J[accept]], s.n))
    Î¸[:, accept]         .= Î¸â€²[:, accept]
    neglogPXÎ¸[:, accept] .= neglogPXÎ¸â€²[:, accept]
    neglogPÎ¸[:, accept]  .= neglogPÎ¸â€²[:, accept]
    s.i[J]              .+= 1
    idx                  .= buffer_indices(s, J)
    s.Î¸[:, idx]          .= Î¸
    s.neglogPXÎ¸[:, idx]  .= neglogPXÎ¸
    s.neglogPÎ¸[:, idx]   .= neglogPÎ¸

    return arr_similar(A, Î¸), arr_similar(A, neglogPXÎ¸), arr_similar(A, neglogPÎ¸)
end

function update!(s::OnlineMetropolisSampler, phys::EPGModel, cvae::CVAE, Ymeta::MetaCPMGSignal, args...; kwargs...)
    Î¸â€², _      = sampleÎ¸Z(phys, cvae, Ymeta; posterior_Î¸ = true, posterior_Z = true, posterior_mode = false, kwargs...)
    Xâ€²         = signal_model(phys, Ymeta, Î¸â€²)
    neglogPXÎ¸â€² = negloglikelihood(phys, signal(Ymeta), Xâ€², Î¸â€²)
    neglogPÎ¸â€²  = neglogprior(phys, Î¸â€²)
    Î¸, â„“XÎ¸, â„“Î¸ = update!(s, Î¸â€², neglogPXÎ¸â€², neglogPÎ¸â€², args...)
    X          = signal_model(phys, Ymeta, Î¸)
    return X, Î¸, â„“XÎ¸, â„“Î¸
end

function _test_online_mh_sampler(phys::EPGModel)
    ndata = 10
    nsamples = 10000

    # initialize sampler with uniformly random samples
    Î¸lo, Î¸hi = Î¸lower(phys), Î¸upper(phys)
    Î¸ = reshape(sample_uniform(Î¸lo, Î¸hi, ndata * nsamples), :, ndata, nsamples)
    s = OnlineMetropolisSampler{Float64}(n = nsamples, Î¸ = Î¸)

    # true answers
    Î¸_plot = Î¸lo .+ (Î¸hi .- Î¸lo) .* range(0,1,length=200)'
    PÎ¸_plot = neglogprior(phys, Î¸_plot; accum = nothing) .|> neglogp -> exp(-neglogp)

    while true
        # draw uniform random guess and update sampler
        J = rand(1:ndataÃ·2) : rand(ndataÃ·2+1:ndata)
        Î¸â€² = sample_uniform(Î¸lo, Î¸hi, length(J))
        neglogPXÎ¸â€² = zeros_similar(Î¸â€², 1, length(J)) # constant zero; we are just trying to reproduce the prior
        neglogPÎ¸â€² = neglogprior(phys, Î¸â€²)
        update!(s, Î¸â€², neglogPXÎ¸â€², neglogPÎ¸â€², J)

        # plot compared to expected prior pdf
        if mod(s.i[1], nsamplesÃ·2) == 0
            plot(
                map(1:7) do i
                    p = plot()
                    stephist!(p, s.Î¸[i, rand(1:ndata), :]; label = Î¸labels(phys)[i], normalized = :pdf)
                    plot!(p, Î¸_plot[i, :], PÎ¸_plot[i, :]; label = :none)
                    p
                end...,
                stephist(s.neglogPXÎ¸[1, rand(1:ndata), :]; label = L"\log{P(X|\theta)}", normalized = :pdf),
                stephist(s.neglogPÎ¸[1, rand(1:ndata), :]; label = L"\log{P(\theta)}", normalized = :pdf);
            ) |> display
            sleep(0.1)
        end
    end
end

####
#### Rician posterior state
####

sampleXÌ‚(rice::RicianCorrector, X, Z = nothing, ninstances = nothing) = sample_rician_state(rice, X, Z, ninstances).XÌ‚

function sampleXÌ‚Î¸Z(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Î¸prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...)
    #TODO: can't differentiate through @timeit "sampleXÎ¸Z"
    #TODO: can't differentiate through @timeit "sampleXÌ‚"
    X, Î¸, Z = sampleXÎ¸Z(phys, cvae, Î¸prior, Zprior, Ymeta; kwargs...)
    XÌ‚ = sampleXÌ‚(rice, X, Z)
    return XÌ‚, Î¸, Z
end
sampleXÌ‚Î¸Z(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÌ‚Î¸Z(phys, rice, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Î¸ = true, posterior_Z = true) # no prior passed -> posterior_Î¸ = posterior_Z = true

sampleXÌ‚(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Î¸prior::MaybeDeepPrior, Zprior::MaybeDeepPrior, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÌ‚Î¸Z(phys, rice, cvae, Î¸prior, Zprior, Ymeta; kwargs...)[1]
sampleXÌ‚(phys::PhysicsModel, rice::RicianCorrector, cvae::CVAE, Ymeta::AbstractMetaDataSignal; kwargs...) = sampleXÌ‚(phys, rice, cvae, nothing, nothing, Ymeta; kwargs..., posterior_Î¸ = true, posterior_Z = true) # no prior passed -> posterior_Î¸ = posterior_Z = true

function NegLogLikelihood(::PhysicsModel, rice::RicianCorrector, Y::AbstractVecOrMat, Î¼0, Ïƒ)
    if typeof(rice) <: NormalizedRicianCorrector && (rice.normalizer !== nothing)
        # Approximate the normalization factor as the normalization factor of the mean signal.
        # For gaussian noise mean signal = Î¼0, but for rician noise mean signal ~ sqrt(Î¼0^2 + Ïƒ^2), at least when Î¼0 >> Ïƒ
        s = inv.(rice.normalizer(mean_rician.(Î¼0, Ïƒ)))
        neglogL_rician.(Y, s .* Î¼0, log.(s .* Ïƒ)) # Rician negative log likelihood
    else
        neglogL_rician.(Y, Î¼0, log.(Ïƒ)) # Rician negative log likelihood
    end
end

function NegLogLikelihood(::EPGModel, rice::RicianCorrector, Y::AbstractVecOrMat, Î¼0, Ïƒ)
    # Likelihood is "maximimally generous" w.r.t. normalization factor, i.e. we perform MLE to find optimal scaling factor
    logs = Zygote.@ignore begin
        _, results = mle_biexp_epg_noise_only(Î¼0, Y, log.(Ïƒ); freeze_logÏµ = true, freeze_logs = false, verbose = false)
        arr_similar(Y, permutedims(results.logscale))
    end
    neglogL_rician.(Y, exp.(logs) .* Î¼0, logs .+ log.(Ïƒ)) # Rician negative log likelihood
end

function posterior_state(phys::PhysicsModel, rice::RicianCorrector, Y::AbstractVecOrMat, Î¸::AbstractVecOrMat, Z::AbstractVecOrMat; accum_loss = â„“ -> sum(â„“; dims = 1))
    X = signal_model(phys, Î¸)
    @unpack Î´, Ïµ, Î½ = rician_state(rice, X, Z)
    X, Î´, Ïµ, Î½ = clamp_dim1(Y, (X, Î´, Ïµ, Î½))
    â„“ = NegLogLikelihood(phys, rice, Y, Î½, Ïµ)
    (accum_loss !== nothing) && (â„“ = accum_loss(â„“))
    return (; Y, Î¸, Z, X, Î´, Ïµ, Î½, â„“)
end

function posterior_state(
        phys::PhysicsModel,
        rice::RicianCorrector,
        cvae::CVAE,
        Ymeta::AbstractMetaDataSignal{T};
        miniter = 5,
        maxiter = 100,
        alpha = 0.01,
        mode = :maxlikelihood,
        verbose = false
    ) where {T}

    (mode === :mode) && (miniter = maxiter = 1)
    Î¸Z_sampler_instance = Î¸Z_sampler(phys, cvae, Ymeta; posterior_mode = mode === :mode)
    new_posterior_state(Î¸new, Znew) = posterior_state(phys, rice, signal(Ymeta), Î¸new, Znew; accum_loss = â„“ -> sum(â„“; dims = 1))

    function update(last_state, i)
        Î¸new, Znew = Î¸Z_sampler_instance()
        Î¸last = (last_state === nothing) ? nothing : last_state.Î¸
        Zlast = (last_state === nothing) ? nothing : last_state.Z

        if mode === :mean
            Î¸new = (last_state === nothing) ? Î¸new : T(1/i) .* Î¸new .+ T(1-1/i) .* Î¸last
            Znew = (last_state === nothing) ? Znew : T(1/i) .* Znew .+ T(1-1/i) .* Zlast
            new_state = new_posterior_state(Î¸new, Znew)
        elseif mode === :mode
            new_state = new_posterior_state(Î¸new, Znew)
        elseif mode === :maxlikelihood
            new_state = new_posterior_state(Î¸new, Znew)
            if (last_state !== nothing)
                mask = new_state.â„“ .< last_state.â„“
                new_state = map(new_state, last_state) do new, last
                    new .= ifelse.(mask, new, last)
                end
            end
        else
            error("Unknown mode: $mode")
        end

        # Check for convergence
        p = (last_state === nothing) ? nothing :
            HypothesisTests.pvalue(
                HypothesisTests.UnequalVarianceTTest(
                    map(x -> x |> cpu |> vec |> Vector{Float64}, (new_state.â„“, last_state.â„“))...
                )
            )

        return new_state, p
    end

    state, _ = update(nothing, 1)
    verbose && @info 1, mean_and_std(state.â„“)
    for i in 2:maxiter
        state, p = update(state, i)
        verbose && @info i, mean_and_std(state.â„“), p
        (i >= miniter) && (p > 1 - alpha) && break
    end

    return state
end

#= TODO: update to return both Î¸ and Z means + stddevs?
function (cvae::CVAE)(Y; nsamples::Int = 1, stddev::Bool = false)
    @assert nsamples â‰¥ ifelse(stddev, 2, 1)
    smooth(a, b, Î³) = a + Î³ * (b - a)
    Î¸Zsampler = Î¸Zposterior_sampler(cvae, Y)
    Î¼x = Î¸Zsampler()
    Î¼x_last, Ïƒx2 = zero(Î¼x), zero(Î¼x)
    for i in 2:nsamples
        x = Î¸Zsampler()
        Î¼x_last .= Î¼x
        Î¼x .= smooth.(Î¼x, x, 1//i)
        Ïƒx2 .= smooth.(Ïƒx2, (x .- Î¼x) .* (x .- Î¼x_last), 1//i)
    end
    return stddev ? vcat(Î¼x, sqrt.(Ïƒx2)) : Î¼x
end
=#
