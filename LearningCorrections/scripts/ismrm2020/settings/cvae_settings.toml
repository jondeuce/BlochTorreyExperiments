dir     = "./output/cvae-tmp"
prec    = 64
gpu     = false
save    = true
timeout = 23.0 # Training timeout (hours)

[data]
val_data    = ""
test_data   = ""
train_data  = ""
val_batch   = "auto" # Auto treats entire validation set as one batch
test_batch  = "auto" # Auto treats entire test set as one batch
train_batch = 256    # Batch size for training
nval        = 1024   # Number of validation samples
ntest       = 1024   # Number of test samples
ntrain      = 4096   # Number of train samples
padtrain    = false  # Pad training data with uniformly random prior samples
[data.info]
nfeatures   = 128 # Height of input data
nchannels   = 1 # Number of input data channels
nlabels     = 5 # Number of output labels
labmean     = [0.0234375, 0.7853981633974483, 0.375, 0.175, 72.0] # Label distribution means
labwidth    = [0.015625,  1.5707963267948966, 0.25,  0.15, 112.0] # Label distribution widths
labnames    = ["freq", "phase", "offset", "amp", "tconst"]
labinfer    = ["freq", "phase", "offset", "amp", "tconst"]
labweights  = [1.0, 1.0, 1.0, 1.0, 1.0] # Weights for loss function
labscale    = [1.0, 1.0, 1.0, 1.0, 1.0] # scale labels before plotting
labunits    = ["Hz", "rad", "a.u.", "a.u.", "s"] # label units, after scaling
[data.preprocess]
normalize   = "" # normalize input signals ("unitsum" -> signal sums to 1)
[data.postprocess]
noise       = 1e-2 # Rician noise level
# [data.info]
# nfeatures   = 48 # Height of input data
# nchannels   = 1 # Number of input data channels
# nlabels     = 5 # Number of output labels
# labmean     = [0.0, 0.0, 0.0, 0.0, 0.0] # Label distribution means (set internally)
# labwidth    = [0.0, 0.0, 0.0, 0.0, 0.0] # Label distribution widths (set internally)
# labnames    = ["cosd(alpha)", "T2short", "T2long", "Ashort", "Along"]
# labinfer    = ["cosd(alpha)", "T2short", "T2long", "Ashort", "Along"]
# labunits    = ["cos(deg)", "ms", "ms", "a.u.", "a.u."] # label units, after scaling
# labweights  = [1.0, 1.0, 1.0, 1.0, 1.0] # Weights for loss function
# labscale    = [1.0, 1.0, 1.0, 1.0, 1.0] # scale labels before plotting
# [data.preprocess]
# normalize   = "" # normalize input signals ("unitsum" -> signal sums to 1)
# [data.postprocess]
# noise       = 0.0005 # Rician noise level

[model]
loss      = "l2"
acc       = "rmse"
gamma     = 1.0 # Reweighting factor: loss = gamma * ELBO + KLdiv
# [model.load]
# path    = "/home/jdoucette/Documents/code/BlochTorreyResults/Experiments/MyelinWaterLearning/ismrm2020/pretrained-dense-model-with-perm/2019-11-01-T-16-12-14-111.acc=rmse_loss=l2_DenseLIGOCVAE_Ndense1=128_Ndense2=128_Ndense3=128_Ndense4=128_Ndense5=128_Ndense6=128_Xout=6_Zdim=20_act=leakyrelu.model-checkpoint.bson"
[model.DenseLIGOCVAE]
Xout      = 5 # Number of learned labels
Nh        = 2 # Number of inner hidden dense layers
Dh        = 64 # Dimension of inner hidden dense layers
Zdim      = 6 # Latent variable dimension
act       = "relu"
boundmean = true # Bound mean to be within labmean +/- labwidth/2

[optimizer]
epochs   = 1_000_000
lrrate   = 1_000_000
lrdrop   = 3.1623
lrbounds = [1e-5, 0.1]
[optimizer.ADAM]
lr       = 1.0e-4
beta     = [0.9, 0.999]
decay    = 0.0
