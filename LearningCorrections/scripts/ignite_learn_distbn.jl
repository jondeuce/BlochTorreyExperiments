####
#### Setup
####

using Pkg; Pkg.activate(joinpath(@__DIR__, ".."))
using MMDLearning
using PyCall
pyplot(size=(800,600))

const settings = TOML.parse("""
    [data]
        out    = "./output/ignite-tmp"
        ntrain = 102_400
        ntest  = 10_240
        nval   = 10_240

    [train]
        timeout     = 1e9 #TODO
        epochs      = 999_999
        batchsize   = 1024 #2048 #3072
        kernelrate  = 100 # Train kernel every `kernelrate` iterations
        kernelsteps = 10 # Gradient updates per kernel train
        GANrate     = 10 # Train GAN losses every `GANrate` iterations, on average
        GANsucc     = 10 # Train GAN losses for `GANsucc` successive iterations, then break for `(GANrate-1)*GANsucc` iterations
        Dsteps      = 10 # Train GAN losses with `Dsteps` discrim updates per genatr update

    [eval]
        ninfer      = 128 # Number of MLE parameter inferences
        nperms      = 128 # Number of permutations for c_alpha calculation + perm plot
        inferperiod = 300.0 # TODO
        saveperiod  = 300.0 # TODO
        showrate    = 1 # TODO

    [opt]
        lrdrop   = 1.0 #10.0 3.1623 #1.7783
        lrthresh = 1e-5
        lrrate   = 1000
        [opt.k]
            loss = "tstatistic" #"MMD"
            lr = 1e-2
        [opt.mmd]
            lr = 1e-4 #3.1623e-4 #1.0e-5 #TODO
        [opt.G]
            lr = 1e-4 #3.1623e-4 #1.0e-5 #TODO
        [opt.D]
            lr = 1e-4 #3.1623e-4 #1.0e-5 #TODO

    [arch]
        physics = "toy" # "toy" or "mri"
        type    = "hyb" # "gan", "mmd", or "hyb"
        [arch.kernel]
            nbandwidth = 4 #TODO
            bwbounds   = [0.0, 0.0] # unset by default; bounds for kernel bandwidths (logsigma)
        [arch.genatr]
            hdim        = 128
            nhidden     = 2
            maxcorr     = 0.0 # unset by default; correction amplitude
            noisebounds = [0.0, 0.0] # unset by default; noise amplitude
        [arch.discrim]
            hdim    = 128
            nhidden = 2
""")

Ignite.parse_command_line!(settings)
Ignite.compare_and_set!(settings["arch"]["kernel"], "bwbounds",    [0.0, 0.0], settings["arch"]["physics"] == "toy" ?  [-8.0, 4.0] : [-10.0, 4.0])
Ignite.compare_and_set!(settings["arch"]["genatr"], "maxcorr",            0.0, settings["arch"]["physics"] == "toy" ?          0.1 :       0.025 )
Ignite.compare_and_set!(settings["arch"]["genatr"], "noisebounds", [0.0, 0.0], settings["arch"]["physics"] == "toy" ? [-8.0, -2.0] : [-6.0, -3.0])
Ignite.save_and_print(settings; outpath = settings["data"]["out"], filename = "settings.toml")

# Initialize generator + discriminator + kernel
function make_models(phys)
    models = Dict{String, Any}()
    n = nsignal(phys)
    toT(m) = Flux.paramtype(eltype(phys), m)

    # Rician generator. First `n` elements for `Î´X` scaled to (-Î´, Î´), second `n` elements for `logÏµ` scaled to (logÏµ_bw[1], logÏµ_bw[2])
    models["G"] = let
        hdim = settings["arch"]["genatr"]["hdim"]::Int
        nhidden = settings["arch"]["genatr"]["nhidden"]::Int
        maxcorr = settings["arch"]["genatr"]["maxcorr"]::Float64
        noisebounds = settings["arch"]["genatr"]["noisebounds"]::Vector{Float64}
        Flux.Chain(
            MMDLearning.MLP(n => 2n, nhidden, hdim, Flux.relu, tanh)...,
            MMDLearning.CatScale([(-maxcorr, maxcorr), (noisebounds...,)], [n,n]),
        ) |> toT
    end

    # Discriminator
    if settings["arch"]["type"] âˆˆ ["gan", "hyb"]
        models["D"] = let
            hdim = settings["arch"]["discrim"]["hdim"]::Int
            nhidden = settings["arch"]["discrim"]["nhidden"]::Int
            MMDLearning.MLP(n => 1, nhidden, hdim, Flux.relu, Flux.sigmoid) |> toT
        end
    end

    # Initialize `nbandwidth` linearly spaced kernel bandwidths `logÏƒ` for each `n` channels strictly within the range (bwbounds[1], bwbounds[2])
    if settings["arch"]["type"] âˆˆ ["mmd", "hyb"]
        models["logsigma"] = let
            bwbounds = settings["arch"]["kernel"]["bwbounds"]::Vector{Float64}
            nbandwidth = settings["arch"]["kernel"]["nbandwidth"]::Int
            repeat(range(bwbounds...; length = nbandwidth+2)[2:end-1], 1, n) |> toT
        end
    end

    return models
end

phys = initialize!(
    ToyModel{Float32,true}();
    ntrain = settings["data"]["ntrain"]::Int,
    ntest = settings["data"]["ntest"]::Int,
    nval = settings["data"]["nval"]::Int,
)
models = make_models(phys)
ricegen = VectorRicianCorrector(models["G"]) # Generator produces ð‘^2n outputs parameterizing n Rician distributions
MMDLearning.model_summary(models)

# Generator and discriminator losses
D_Y_loss(Y) = models["D"](Y) # discrim on real data
D_G_X_loss(X) = models["D"](corrected_signal_instance(ricegen, X)) # discrim on genatr data
Dloss(X,Y) = -mean(log.(D_Y_loss(Y)) .+ log.(1 .- D_G_X_loss(X)))
Gloss(X) = mean(log.(1 .- D_G_X_loss(X)))
MMDloss(X,Y) = size(Y,2) * mmd_flux(models["logsigma"], corrected_signal_instance(ricegen, X), Y) # m*MMD^2 on genatr data

# Global state
const logger = DataFrame(
    :epoch   => Int[], # mandatory field
    :dataset => Symbol[], # mandatory field
    :time    => Union{Float64, Missing}[],
    :Gloss   => Union{eltype(phys), Missing}[],
    :Dloss   => Union{eltype(phys), Missing}[],
    :D_Y     => Union{eltype(phys), Missing}[],
    :D_G_X   => Union{eltype(phys), Missing}[],
    :MMDsq   => Union{eltype(phys), Missing}[],
    :MMDvar  => Union{eltype(phys), Missing}[],
    :tstat   => Union{eltype(phys), Missing}[],
    :c_alpha => Union{eltype(phys), Missing}[],
    :P_alpha => Union{eltype(phys), Missing}[],
    :rmse    => Union{eltype(phys), Missing}[],
    :theta_err => Union{Vector{eltype(phys)}, Missing}[],
    :Xhat_logL => Union{eltype(phys), Missing}[],
    :Xhat_rmse => Union{eltype(phys), Missing}[],
)
const optimizers = Dict{String,Any}(
    "G"   => Flux.ADAM(settings["opt"]["G"]["lr"]),
    "D"   => Flux.ADAM(settings["opt"]["D"]["lr"]),
    "mmd" => Flux.ADAM(settings["opt"]["mmd"]["lr"]),
)
const cb_state = Dict{String,Any}()

####
#### Training
####

const torch = pyimport("torch")
const logging = pyimport("logging")
const ignite = pyimport("ignite")
# const torchvision  pyimport("torchvision")
# const transforms = pyimport("torchvision.transforms")
# const datasets = pyimport("torchvision.datasets")
# const optim = pyimport("torch.optim")
# const nn = pyimport("torch.nn")
# const F = pyimport("torch.nn.functional")
# const ProgressBar = ignite.contrib.handlers.ProgressBar
# const Timer = ignite.handlers.Timer
# const RunningAverage = ignite.metrics.RunningAverage

function train_step(engine, batch)
    @unpack kernelrate, kernelsteps, GANrate, GANsucc, Dsteps = settings["train"]
    _, Xtrain, Ytrain = Ignite.array.(batch)

    @timeit "train batch" begin
        if settings["arch"]["type"] âˆˆ ["mmd", "hyb"]
            # Train MMD kernel bandwidths
            if mod(engine.state.iteration-1, kernelrate) == 0
                @timeit "MMD kernel" begin
                    @timeit "sample G(X)" XÌ‚train = corrected_signal_instance(ricegen, Xtrain)
                    for _ in 1:kernelsteps
                        success = train_kernel_bandwidth_flux!(models["logsigma"], XÌ‚train, Ytrain;
                            kernelloss = settings["opt"]["k"]["loss"], kernellr = settings["opt"]["k"]["lr"], bwbounds = settings["arch"]["kernel"]["bwbounds"]) # timed internally
                        !success && break
                    end
                end
            end
            # Train generator on MMD loss
            @timeit "MMD generator" begin
                @timeit "forward" _, back = Zygote.pullback(() -> MMDloss(Xtrain, Ytrain), Flux.params(models["G"]))
                @timeit "reverse" gs = back(one(eltype(phys)))
                @timeit "update!" Flux.Optimise.update!(optimizers["mmd"], Flux.params(models["G"]), gs)
            end
        end

        if settings["arch"]["type"] âˆˆ ["gan", "hyb"]
            if settings["arch"]["type"] == "gan" || mod((engine.state.iteration-1) Ã· GANsucc, GANrate) == 0
                @timeit "GAN discriminator" for _ in 1:Dsteps
                    @timeit "forward" _, back = Zygote.pullback(() -> Dloss(Xtrain, Ytrain), Flux.params(models["D"]))
                    @timeit "reverse" gs = back(one(eltype(phys)))
                    @timeit "update!" Flux.Optimise.update!(optimizers["D"], Flux.params(models["D"]), gs)
                end
                @timeit "GAN generator" begin
                    @timeit "forward" _, back = Zygote.pullback(() -> Gloss(Xtrain), Flux.params(models["G"]))
                    @timeit "reverse" gs = back(one(eltype(phys)))
                    @timeit "update!" Flux.Optimise.update!(optimizers["G"], Flux.params(models["G"]), gs)
                end
            end
        end
    end

    return nothing
end

function eval_metrics(engine, batch)
    @timeit "eval batch" begin
        # Update callback state
        @timeit "update cb state" let
            cb_state["Î¸"], cb_state["XÎ¸"], cb_state["Y"] = Ignite.array.(batch)
            if hasclosedform(phys)
                cb_state["YÎ¸"] = signal_model(ClosedForm(phys), cb_state["Î¸"])
                cb_state["YÎ¸hat"] = signal_model(ClosedForm(phys), cb_state["Î¸"], noiselevel(ClosedForm(phys)))
            end
            update_callback!(cb_state, phys, ricegen; ninfer = settings["eval"]["ninfer"], inferperiod = settings["eval"]["inferperiod"])
        end

        # Initialize metrics dictionary
        metrics = Dict{Any,Any}()
        metrics[:epoch]   = :val âˆ‰ logger.dataset ? 0 : logger.epoch[findlast(d -> d === :val, logger.dataset)] + 1
        metrics[:dataset] = :val
        metrics[:time]    = cb_state["curr_time"] - cb_state["last_time"]

        # Metrics computed in update_callback!
        metrics[:rmse] = cb_state["metrics"]["rmse"]
        metrics[:theta_err] = cb_state["metrics"]["theta_err"]
        metrics[:Xhat_logL] = cb_state["metrics"]["Xhat_logL"]
        metrics[:Xhat_rmse] = cb_state["metrics"]["Xhat_rmse"]

        # Perform permutation test
        if settings["arch"]["type"] âˆˆ ["mmd", "hyb"]
            @timeit "perm test" let
                m = settings["train"]["batchsize"] # training batch size, not size of val set
                cb_state["permtest"] = mmd_perm_test_power(models["logsigma"], MMDLearning.sample_columns(cb_state["XÎ¸hat"], m), MMDLearning.sample_columns(cb_state["Y"], m); nperms = settings["eval"]["nperms"])
                metrics[:MMDsq]   = m * cb_state["permtest"].MMDsq
                metrics[:MMDvar]  = m^2 * cb_state["permtest"].MMDvar
                metrics[:tstat]   = cb_state["permtest"].MMDsq / cb_state["permtest"].MMDÏƒ
                metrics[:c_alpha] = cb_state["permtest"].c_alpha
                metrics[:P_alpha] = cb_state["permtest"].P_alpha_approx
            end
        end

        # Compute GAN losses
        if settings["arch"]["type"] âˆˆ ["gan", "hyb"]
            @timeit "gan losses" let
                d_y = D_Y_loss(cb_state["Y"])
                d_g_x = D_G_X_loss(cb_state["XÎ¸hat"])
                metrics[:Gloss] = mean(log.(1 .- d_g_x))
                metrics[:Dloss] = -mean(log.(d_y) .+ log.(1 .- d_g_x))
                metrics[:D_Y]   = mean(d_y)
                metrics[:D_G_X] = mean(d_g_x)
            end
        end

        # Update logger dataframe
        push!(logger, metrics; cols = :subset)
    end

    return deepcopy(metrics) #TODO convert to PyDict?
end

function makeplots(;showplot = false)
    try
        Dict{Symbol, Any}(
            :gan       => settings["arch"]["type"] âˆˆ ["gan", "hyb"] ? MMDLearning.plot_gan_loss(logger, cb_state, phys; showplot = showplot, lrdroprate = settings["opt"]["lrrate"], lrdrop = settings["opt"]["lrdrop"]) : nothing,
            :ricemodel => MMDLearning.plot_rician_model(logger, cb_state, phys; showplot = showplot, bandwidths = haskey(models, "logsigma") ? permutedims(models["logsigma"]) : nothing),
            :signals   => MMDLearning.plot_rician_signals(logger, cb_state, phys; showplot = showplot),
            :mmd       => settings["arch"]["type"] âˆˆ ["mmd", "hyb"] ? MMDLearning.plot_mmd_losses(logger, cb_state, phys; showplot = showplot, lrdroprate = settings["opt"]["lrrate"], lrdrop = settings["opt"]["lrdrop"]) : nothing,
            :infer     => MMDLearning.plot_rician_inference(logger, cb_state, phys; showplot = showplot),
            :witness   => nothing, #mmd_witness(XÏµ, Y, sigma)
            :heat      => nothing, #mmd_heatmap(XÏµ, Y, sigma)
            :perm      => settings["arch"]["type"] âˆˆ ["mmd", "hyb"] ? mmd_perm_test_power_plot(cb_state["permtest"]; showplot = showplot) : nothing,
        )
    catch e
        handleinterrupt(e; msg = "Error plotting")
    end
end

function make_data_tuples(dataset)
    Î¸ = sampleÎ¸(phys, :all; dataset = dataset)
    X = sampleX(phys, :all; dataset = dataset)
    Y = sampleY(phys, :all; dataset = dataset)
    return [(Î¸[:,j], X[:,j], Y[:,j]) for j in 1:size(Y,2)]
end
train_loader = torch.utils.data.DataLoader(make_data_tuples(:train); batch_size = settings["train"]["batchsize"], shuffle = true, drop_last = true)
val_loader = torch.utils.data.DataLoader(make_data_tuples(:val); batch_size = settings["data"]["nval"], shuffle = false, drop_last = false)

trainer = ignite.engine.Engine(@j2p train_step)
evaluator = ignite.engine.Engine(@j2p eval_metrics)

# Compute callback metrics
trainer.add_event_handler(
    ignite.engine.Events.STARTED | ignite.engine.Events.EPOCH_COMPLETED,
    @j2p function (engine)
        evaluator.run(val_loader)
    end
)

# Checkpoint current model + logger + make plots
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED(event_filter = @j2p event_throttler(settings["eval"]["saveperiod"])),
    @j2p function (engine)
        @timeit "checkpoint" begin
            @timeit "save current model" saveprogress(@dict(models, optimizers, logger); savefolder = settings["data"]["out"], prefix = "current-")
            @timeit "make current plots" plothandles = makeplots()
            @timeit "save current plots" saveplots(plothandles; savefolder = settings["data"]["out"], prefix = "current-")
        end
    end
)

# Check for + save best model + logger + make plots
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED,
    @j2p function (engine)
        losses = logger.Xhat_logL[logger.dataset .=== :val] |> skipmissing |> collect
        if !isempty(losses) && (length(losses) == 1 || losses[end] < minimum(losses[1:end-1]))
            @timeit "save best progress" begin
                @timeit "save best model" saveprogress(@dict(models, optimizers, logger); savefolder = settings["data"]["out"], prefix = "best-")
                @timeit "make best plots" plothandles = makeplots()
                @timeit "save best plots" saveplots(plothandles; savefolder = settings["data"]["out"], prefix = "best-")
            end
        end
    end
)

# Drop learning rate
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED,
    @j2p function (engine)
        @unpack lrrate, lrdrop, lrthresh = settings["opt"]
        epoch = engine.state.epoch
        if epoch > 1 && mod(epoch-1, lrrate) == 0
            for (optname, opt) in optimizers
                new_eta = max(opt.eta / lrdrop, lrthresh)
                if new_eta > lrthresh
                    @info "$epoch: Dropping $optname optimizer learning rate to $new_eta"
                else
                    @info "$epoch: Learning rate reached minimum value $lrthresh for $optname optimizer"
                end
                opt.eta = new_eta
            end
        end
    end
)

# Print TimerOutputs timings
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED,
    @j2p function (engine)
        if mod(engine.state.epoch-1, settings["eval"]["showrate"]) == 0
            show(stdout, TimerOutputs.get_defaulttimer()); println("\n")
            show(stdout, last(logger[:, Not(:theta_err)], 10)); println("\n")
        end
        (engine.state.epoch == 1) && TimerOutputs.reset_timer!() # throw out compilation timings
    end
)

# Timeout
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED(event_filter = @j2p run_timeout(settings["train"]["timeout"])),
    @j2p function (engine)
        @info "Exiting: training time exceeded $(DECAES.pretty_time(settings["train"]["timeout"]))"
        engine.terminate()
    end
)

# Setup loggers
trainer.logger = ignite.utils.setup_logger("trainer")
evaluator.logger = ignite.utils.setup_logger("evaluator")

# Run trainer
trainer.run(train_loader, max_epochs = settings["train"]["epochs"])
