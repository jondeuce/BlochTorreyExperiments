####
#### Setup
####

using Pkg; Pkg.activate(joinpath(@__DIR__, ".."))
using MMDLearning
using PyCall
pyplot(size=(800,600))

const settings = TOML.parse("""
    [data]
        out    = "./output/ignite-cvae-tmp"
        ntrain = 102_400
        ntest  = 10_240
        nval   = 10_240

    [train]
        timeout   = 1e9
        epochs    = 999_999
        batchsize = 1024

    [eval]
        saveperiod = 30.0 # TODO
        showrate   = 1 # TODO

    [opt]
        lrdrop   = 1.0
        lrthresh = 1e-5
        lrrate   = 1000
        [opt.cvae]
            lr = 1e-4 # default for optimizers below

    [arch]
        physics = "toy" # "toy" or "mri"
        nlatent = 1 # number of latent variables Z
        zdim    = 4 # embedding dimension of z
        hdim    = 64 # default for models below
        nhidden = 2 # default for models below
        [arch.G]
            hdim        = 0
            nhidden     = 0
            maxcorr     = 0.0 # unset by default; correction amplitude
            noisebounds = [0.0, 0.0] # unset by default; noise amplitude
        [arch.E1]
            hdim    = 0
            nhidden = 0
        [arch.E2]
            hdim    = 0
            nhidden = 0
        [arch.D]
            hdim    = 0
            nhidden = 0
""")

Ignite.parse_command_line!(settings)
Ignite.compare_and_set!(settings["arch"]["G"], "maxcorr",            0.0, settings["arch"]["physics"] == "toy" ?          0.1 :       0.025 )
Ignite.compare_and_set!(settings["arch"]["G"], "noisebounds", [0.0, 0.0], settings["arch"]["physics"] == "toy" ? [-8.0, -2.0] : [-6.0, -3.0])
Ignite.compare_and_set!.([settings["arch"][k] for k in ("G","E1","E2","D")], "hdim",    0, settings["arch"]["hdim"])
Ignite.compare_and_set!.([settings["arch"][k] for k in ("G","E1","E2","D")], "nhidden", 0, settings["arch"]["nhidden"])
Ignite.save_and_print(settings; outpath = settings["data"]["out"], filename = "settings.toml")

# Initialize generator + discriminator + kernel
function make_models(phys)
    models = Dict{String, Any}()
    n   = nsignal(phys) # input signal length
    nÎ¸  = ntheta(phys) # number of physics variables
    Î¸bd = Î¸bounds(phys)
    k   = settings["arch"]["nlatent"]::Int # number of latent variables Z
    nz  = settings["arch"]["zdim"]::Int # embedding dimension
    toT(m) = Flux.paramtype(eltype(phys), m)

    # Rician generator. First `n` elements for `Î´X` scaled to (-Î´, Î´), second `n` elements for `logÏµ` scaled to (noisebounds[1], noisebounds[2])
    models["G"] = let
        hdim = settings["arch"]["G"]["hdim"]::Int
        nhidden = settings["arch"]["G"]["nhidden"]::Int
        maxcorr = settings["arch"]["G"]["maxcorr"]::Float64
        noisebounds = settings["arch"]["G"]["noisebounds"]::Vector{Float64}
        Flux.Chain(
            MMDLearning.MLP(n + k => 2n, nhidden, hdim, Flux.relu, tanh)...,
            # MMDLearning.MLP(k => 2n, nhidden, hdim, Flux.relu, tanh)..., #TODO
            MMDLearning.CatScale([(-maxcorr, maxcorr), (noisebounds...,)], [n,n]),
        ) |> toT
    end

    # Encoders
    models["E1"] = let
        hdim = settings["arch"]["E1"]["hdim"]::Int
        nhidden = settings["arch"]["E1"]["nhidden"]::Int
        MMDLearning.MLP(n => 2*nz, nhidden, hdim, Flux.relu, identity) |> toT
    end

    models["E2"] = let
        hdim = settings["arch"]["E2"]["hdim"]::Int
        nhidden = settings["arch"]["E2"]["nhidden"]::Int
        MMDLearning.MLP(n + nÎ¸ + k => 2*nz, nhidden, hdim, Flux.relu, identity) |> toT
    end

    # Decoder
    models["D"] = let
        hdim = settings["arch"]["D"]["hdim"]::Int
        nhidden = settings["arch"]["D"]["nhidden"]::Int
        Flux.Chain(
            MMDLearning.MLP(n + nz => 2*(nÎ¸ + k), nhidden, hdim, Flux.relu, identity)...,
            MMDLearning.CatScale(eltype(Î¸bd)[Î¸bd; (-1, 1)], [ones(Int, nÎ¸); k + nÎ¸ + k]),
        ) |> toT
    end

    return models
end

phys = initialize!(
    ToyModel{Float32,true}();
    ntrain = settings["data"]["ntrain"]::Int,
    ntest = settings["data"]["ntest"]::Int,
    nval = settings["data"]["nval"]::Int,
)
models = make_models(phys)
ricegen = VectorRicianCorrector(models["G"]) # Generator produces ð‘^2n outputs parameterizing n Rician distributions
# ricegen = LatentVectorRicianCorrector(models["G"])
MMDLearning.model_summary(models)

# Helpers
split_mean_std(Î¼::Matrix) = (Î¼[1:endÃ·2,:], Î¼[endÃ·2+1:end,:])
split_theta_latent(Î¼::Matrix) = (Î¼[1:ntheta(phys),:], Î¼[ntheta(phys)+1:end,:])
sample_mv_normal(Î¼0::Matrix{T}, Ïƒ::Matrix{T}) where {T} = Î¼0 .+ Ïƒ .* randn(T, max.(size(Î¼0), size(Ïƒ)))
@inline square(x) = x*x

function InvertY(Y)
    Î¼r = models["E1"](Y)
    zr = sample_mv_normal(split_mean_std(Î¼r)...)
    Î¼x = models["D"](vcat(Y,zr))
    x  = sample_mv_normal(split_mean_std(Î¼x)...)
    Î¸, Z = split_theta_latent(x)
    return Î¸, Z
end

function DataConsistency(Y, Î¼G0, ÏƒG)
    # Rician negative log likelihood
    ÏƒG2 = square.(ÏƒG)
    YlogL = -sum(@. log(Y / ÏƒG2) + MMDLearning._logbesseli0(Y * Î¼G0 / ÏƒG2) - (Y^2 + Î¼G0^2) / (2 * ÏƒG2))
    # YlogL = sum(@. log(ÏƒG2) + square(Y - Î¼G0) / ÏƒG2) / 2 # Gaussian likelihood for testing
    return YlogL
end

function KLdivergence(Î¼q0, Ïƒq, Î¼r0, Ïƒr)
    Ïƒr2, Ïƒq2 = square.(Ïƒr), square.(Ïƒq)
    KLdiv = sum(@. (Ïƒq2 + square(Î¼r0 - Î¼q0)) / Ïƒr2 + log(Ïƒr2 / Ïƒq2)) / 2 # KL-divergence contribution to cross-entropy (Note: dropped constant -Zdim/2 term)
    return KLdiv
end

function EvidenceLowerBound(x, Î¼x0, Ïƒx)
    Ïƒx2 = square.(Ïƒx)
    ELBO = sum(@. square(x - Î¼x0) / Ïƒx2 + log(Ïƒx2)) / 2 # Negative log-likelihood/ELBO contribution to cross-entropy (Note: dropped constant +Zdim*log(2Ï€)/2 term)
    return ELBO
end

# Self-supervised CVAE loss
function SelfCVAEloss(Y; recover_Z = false)
    # Invert Y
    Î¸, Z = InvertY(Y)

    # Limit information capacity of Z with â„“2 regularization
    #   - Equivalently, as 1/2||Z||^2 is the negative log likelihood of Z ~ N(0,1) (dropping normalization factor)
    Zreg = recover_Z ?
        sum(abs2, Z) / 2 :
        zero(eltype(Z))

    # Drop gradients for Î¸ and Z, and compute uncorrected X from physics model
    Î¸ = Zygote.dropgrad(Î¸)
    Z = Zygote.dropgrad(Z)
    # X = Zygote.ignore() do
    #     signal_model(phys, Î¸)
    # end
    X = signal_model(phys, Î¸)

    # Corrected XÌ‚ instance
    Î¼G0, ÏƒG = rician_params(ricegen, X, Z)
    XÌ‚ = add_noise_instance(ricegen, Î¼G0, ÏƒG)

    # Rician negative log likelihood
    YlogL = DataConsistency(Y, Î¼G0, ÏƒG)

    # Cross-entropy loss function
    Î¼r0, Ïƒr = split_mean_std(models["E1"](Y)) #TODO XÌ‚ or Y?
    Î¼q0, Ïƒq = split_mean_std(models["E2"](vcat(XÌ‚,Î¸,Z)))
    zq = sample_mv_normal(Î¼q0, Ïƒq)
    Î¼x0, Ïƒx = split_mean_std(models["D"](vcat(Y,zq))) #TODO XÌ‚ or Y?

    KLdiv = KLdivergence(Î¼q0, Ïƒq, Î¼r0, Ïƒr)
    ELBO = recover_Z ?
        EvidenceLowerBound(vcat(Î¸,Z), Î¼x0, Ïƒx) :
        EvidenceLowerBound(Î¸, Î¼x0[1:ntheta(phys),:], Ïƒx[1:ntheta(phys),:])

    Nbatch = size(Y,2)
    â„“ = (Zreg + YlogL + KLdiv + ELBO) / Nbatch

    return â„“
end

# Global state
const logger = DataFrame(
    :epoch   => Int[], # mandatory field
    :dataset => Symbol[], # mandatory field
    :time    => Union{Float64, Missing}[],
    :loss    => Union{eltype(phys), Missing}[],
    :Zreg    => Union{eltype(phys), Missing}[],
    :YlogL   => Union{eltype(phys), Missing}[],
    :KLdiv   => Union{eltype(phys), Missing}[],
    :ELBO    => Union{eltype(phys), Missing}[],
    :rmse    => Union{eltype(phys), Missing}[],
    :theta_fit_err   => Union{Vector{eltype(phys)}, Missing}[],
    :Z_fit_err       => Union{Vector{eltype(phys)}, Missing}[],
    :signal_fit_logL => Union{eltype(phys), Missing}[],
    :signal_fit_rmse => Union{eltype(phys), Missing}[],
)
const optimizers = Dict{String,Any}(
    "cvae" => Flux.ADAM(settings["opt"]["cvae"]["lr"]),
)
const cb_state = Dict{String,Any}()

####
#### Training
####

const torch = pyimport("torch")
const logging = pyimport("logging")
const ignite = pyimport("ignite")
# const torchvision  pyimport("torchvision")
# const transforms = pyimport("torchvision.transforms")
# const datasets = pyimport("torchvision.datasets")
# const optim = pyimport("torch.optim")
# const nn = pyimport("torch.nn")
# const F = pyimport("torch.nn.functional")
# const ProgressBar = ignite.contrib.handlers.ProgressBar
# const Timer = ignite.handlers.Timer
# const RunningAverage = ignite.metrics.RunningAverage

function train_step(engine, batch)
    Ytrain = Ignite.array(only(batch))

    @timeit "train batch" begin
        ps = Flux.params(values(models)...)
        @timeit "forward" _, back = Zygote.pullback(() -> SelfCVAEloss(Ytrain), ps)
        @timeit "reverse" gs = back(one(eltype(phys)))
        @timeit "update!" Flux.Optimise.update!(optimizers["cvae"], ps, gs)
    end

    return nothing
end

function eval_metrics(engine, batch)
    @timeit "eval batch" begin
        # Update callback state
        cb_state["last_time"] = get!(cb_state, "curr_time", time())
        cb_state["curr_time"] = time()
        cb_state["metrics"] = Dict{String,Any}()

        # Invert Y and make Xs
        Y = Ignite.array(only(batch))
        Nbatch = size(Y,2)
        Î¸, Z = InvertY(Y)
        X = signal_model(phys, Î¸)
        Î´G0, ÏƒG = correction_and_noiselevel(ricegen, X, Z)
        Î¼G0 = @. abs(X + Î´G0)
        XÌ‚ = add_noise_instance(ricegen, Î¼G0, ÏƒG)

        # Cross-entropy loss function
        Î¼r0, Ïƒr = split_mean_std(models["E1"](Y)) #TODO XÌ‚ or Y?
        Î¼q0, Ïƒq = split_mean_std(models["E2"](vcat(XÌ‚,Î¸,Z)))
        zq = sample_mv_normal(Î¼q0, Ïƒq)
        Î¼x0, Ïƒx = split_mean_std(models["D"](vcat(Y,zq))) #TODO XÌ‚ or Y?

        Zreg = sum(abs2, Z) / (2*Nbatch)
        YlogL = DataConsistency(Y, Î¼G0, ÏƒG) / Nbatch
        KLdiv = KLdivergence(Î¼q0, Ïƒq, Î¼r0, Ïƒr) / Nbatch
        ELBO = EvidenceLowerBound(vcat(Î¸,Z), Î¼x0, Ïƒx) / Nbatch
        loss = Zreg + YlogL + KLdiv + ELBO
        @pack! cb_state["metrics"] = Zreg, YlogL, KLdiv, ELBO, loss

        # Compute signal correction, noise instances, etc.
        cb_state["Î¸"], cb_state["XÎ¸"], cb_state["Y"] = Î¸, X, Y
        cb_state["YÎ¸"] = hasclosedform(phys) ? signal_model(ClosedForm(phys), cb_state["Î¸"]) : missing
        cb_state["YÎ¸hat"] = hasclosedform(phys) ? signal_model(ClosedForm(phys), cb_state["Î¸"], noiselevel(ClosedForm(phys))) : missing
        let
            Î´Î¸, ÏµÎ¸, XÎ¸Î´, XÎ¸hat = Î´G0, ÏƒG, Î¼G0, XÌ‚
            @pack! cb_state = Î´Î¸, ÏµÎ¸, XÎ¸Î´, XÎ¸hat
        end

        # Compute signal correction, noise instances, etc.
        let
            Yfit, Î¸fit, Zfit = Y, Î¸, Z
            XÎ¸fit = signal_model(phys, Î¸fit)
            Î´Î¸fit, ÏµÎ¸fit = correction_and_noiselevel(ricegen, XÎ¸fit, Zfit)
            XÎ¸Î´fit = abs.(XÎ¸fit .+ Î´Î¸fit)
            XÎ¸hatfit = add_noise_instance(ricegen, XÎ¸Î´fit, ÏµÎ¸fit)
            YÎ¸fit = hasclosedform(phys) ? signal_model(ClosedForm(phys), Î¸) : missing
            YÎ¸hatfit = hasclosedform(phys) ? signal_model(ClosedForm(phys), Î¸, noiselevel(ClosedForm(phys))) : missing
            @pack! cb_state = Yfit, Î¸fit, Zfit, XÎ¸fit, Î´Î¸fit, ÏµÎ¸fit, XÎ¸Î´fit, XÎ¸hatfit, YÎ¸fit, YÎ¸hatfit
        end

        # Compute error metrics
        let
            @unpack Yfit, Î¸fit, Zfit, YÎ¸fit, XÎ¸hatfit, XÎ¸Î´fit, ÏµÎ¸fit = cb_state
            rmse = hasclosedform(phys) ? sqrt(mean(abs2, YÎ¸fit - XÎ¸Î´fit)) : missing
            all_signal_fit_rmse = sqrt.(mean(abs2, Yfit .- XÎ¸hatfit; dims = 1)) |> vec
            all_signal_fit_logL = .-sum(logpdf.(Rician.(XÎ¸Î´fit, ÏµÎ¸fit), Yfit); dims = 1) |> vec
            signal_fit_rmse = mean(all_signal_fit_rmse)
            signal_fit_logL = mean(all_signal_fit_logL)
            Î¸samp, Zsamp = split_theta_latent(sample_mv_normal(Î¼x0, Ïƒx))
            Î¸_fit_err = mean(Î¸error(phys, Î¸, Î¸samp); dims = 2) |> vec |> copy
            Z_fit_err = mean(abs, Z .- Zsamp; dims = 2) |> vec |> copy
            @pack! cb_state["metrics"] = rmse, all_signal_fit_rmse, all_signal_fit_logL, signal_fit_rmse, signal_fit_logL, Î¸_fit_err, Z_fit_err
        end

        # Initialize output metrics dictionary
        metrics = Dict{Any,Any}()
        metrics[:epoch]   = :val âˆ‰ logger.dataset ? 0 : logger.epoch[findlast(d -> d === :val, logger.dataset)] + 1
        metrics[:dataset] = :val
        metrics[:time]    = cb_state["curr_time"] - cb_state["last_time"]

        # Metrics computed in update_callback!
        metrics[:loss]  = cb_state["metrics"]["loss"]
        metrics[:Zreg]  = cb_state["metrics"]["Zreg"]
        metrics[:YlogL] = cb_state["metrics"]["YlogL"]
        metrics[:KLdiv] = cb_state["metrics"]["KLdiv"]
        metrics[:ELBO]  = cb_state["metrics"]["ELBO"]
        metrics[:rmse]  = cb_state["metrics"]["rmse"]
        metrics[:theta_fit_err]   = cb_state["metrics"]["Î¸_fit_err"]
        metrics[:Z_fit_err]       = cb_state["metrics"]["Z_fit_err"]
        metrics[:signal_fit_logL] = cb_state["metrics"]["signal_fit_logL"]
        metrics[:signal_fit_rmse] = cb_state["metrics"]["signal_fit_rmse"]

        # Update logger dataframe
        push!(logger, metrics; cols = :subset)

        return deepcopy(metrics) #TODO convert to PyDict?
    end
end

function makeplots(;showplot = false)
    try
        Dict{Symbol, Any}(
            :ricemodel => MMDLearning.plot_rician_model(logger, cb_state, phys; showplot = showplot, bandwidths = haskey(models, "logsigma") ? permutedims(models["logsigma"]) : nothing),
            :signals   => MMDLearning.plot_rician_signals(logger, cb_state, phys; showplot = showplot),
            :infer     => MMDLearning.plot_rician_inference(logger, cb_state, phys; showplot = showplot),
        )
    catch e
        handleinterrupt(e; msg = "Error plotting")
    end
end

make_data_tuples(dataset) = tuple.(copy.(eachcol(sampleY(phys, :all; dataset = dataset))))
train_loader = torch.utils.data.DataLoader(make_data_tuples(:train); batch_size = settings["train"]["batchsize"], shuffle = true, drop_last = true)
val_loader = torch.utils.data.DataLoader(make_data_tuples(:val); batch_size = settings["data"]["nval"], shuffle = false, drop_last = false)

trainer = ignite.engine.Engine(@j2p train_step)
evaluator = ignite.engine.Engine(@j2p eval_metrics)

# Compute callback metrics
trainer.add_event_handler(
    ignite.engine.Events.STARTED | ignite.engine.Events.EPOCH_COMPLETED,
    @j2p function (engine)
        evaluator.run(val_loader)
    end
)

# Checkpoint current model + logger + make plots
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED(event_filter = @j2p event_throttler(settings["eval"]["saveperiod"])),
    @j2p function (engine)
        @timeit "checkpoint" begin
            @timeit "save current model" saveprogress(@dict(models, optimizers, logger); savefolder = settings["data"]["out"], prefix = "current-")
            @timeit "make current plots" plothandles = makeplots()
            @timeit "save current plots" saveplots(plothandles; savefolder = settings["data"]["out"], prefix = "current-")
        end
    end
)

# Check for + save best model + logger + make plots
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED,
    @j2p function (engine)
        losses = logger.signal_fit_logL[logger.dataset .=== :val] |> skipmissing |> collect
        if !isempty(losses) && (length(losses) == 1 || losses[end] < minimum(losses[1:end-1]))
            @timeit "save best progress" begin
                @timeit "save best model" saveprogress(@dict(models, optimizers, logger); savefolder = settings["data"]["out"], prefix = "best-")
                @timeit "make best plots" plothandles = makeplots()
                @timeit "save best plots" saveplots(plothandles; savefolder = settings["data"]["out"], prefix = "best-")
            end
        end
    end
)

# Drop learning rate
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED,
    @j2p function (engine)
        @unpack lrrate, lrdrop, lrthresh = settings["opt"]
        epoch = engine.state.epoch
        if epoch > 1 && mod(epoch-1, lrrate) == 0
            for (optname, opt) in optimizers
                new_eta = opt.eta / lrdrop
                if new_eta >= lrthresh
                    @info "$epoch: Dropping $optname optimizer learning rate to $new_eta"
                    opt.eta = new_eta
                else
                    @info "$epoch: Learning rate dropped below $lrthresh for $optname optimizer, exiting..."
                    throw(InterruptException())
                end
            end
        end
    end
)

# Print TimerOutputs timings
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED,
    @j2p function (engine)
        if mod(engine.state.epoch-1, settings["eval"]["showrate"]) == 0
            show(stdout, TimerOutputs.get_defaulttimer()); println("\n")
            show(stdout, last(logger, 10)); println("\n")
        end
        (engine.state.epoch == 1) && TimerOutputs.reset_timer!() # throw out compilation timings
    end
)

# Timeout
trainer.add_event_handler(
    ignite.engine.Events.EPOCH_COMPLETED(event_filter = @j2p run_timeout(settings["train"]["timeout"])),
    @j2p function (engine)
        @info "Exiting: training time exceeded $(DECAES.pretty_time(settings["train"]["timeout"]))"
        engine.terminate()
    end
)

# Setup loggers
trainer.logger = ignite.utils.setup_logger("trainer")
evaluator.logger = ignite.utils.setup_logger("evaluator")

# Run trainer
trainer.run(train_loader, max_epochs = settings["train"]["epochs"])
