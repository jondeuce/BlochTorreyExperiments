dir     = "."
prec    = 32
gpu     = false
save    = true
timeout = 47.5 # Training timeout (hours)

[data]
val_data    = ""
test_data   = "/project/st-arausch-1/jcd1994/ismrm2020/diff-med-test"
train_data  = "/project/st-arausch-1/jcd1994/ismrm2020/diff-med-train"
val_batch   = "auto" # Auto treats entire validation set as one batch
test_batch  = "auto" # Auto treats entire test set as one batch
train_batch = 250
[data.info]
nfeatures   = "auto" # Height of input data
nchannels   = "auto" # Number of input data channels
nlabels     = "auto" # Number of output labels
labmean     = "auto" # Label means
labwidth    = "auto" # Label distribution widths
# labnames  = ["cosd(alpha)", "log(TE*K)", "gratio", "mwf", "T2mw/TE", "T2iew/TE", "iwf", "ewf", "iewf", "T2iw/TE", "T2ew/TE", "T1mw/TE", "T1iw/TE", "T1ew/TE", "T1iew/TE"]
# labinfer  = ["cosd(alpha)", "log(TE*K)", "gratio", "mwf", "T2mw/TE", "T2iew/TE"]
# labweights  = [1.0, 1.0, 1.0,   1.0,   1.0, 1.0] # Weights for loss function
# labscale    = [1.0, 1.0, 100.0, 100.0, 1.0, 1.0] # scale labels before plotting
# labunits    = ["cosd(°)", "log(um)", "%", "%", "ms/ms", "ms/ms"] # label units, after scaling
labnames    = ["cosd(alpha)", "gratio", "mwf", "T2mw/TE", "T2iew/TE", "log(TE*K)", "iwf", "ewf", "iewf", "T2iw/TE", "T2ew/TE", "T1mw/TE", "T1iw/TE", "T1ew/TE", "T1iew/TE"]
labinfer    = ["cosd(alpha)", "gratio", "mwf", "T2mw/TE", "T2iew/TE", "log(TE*K)"]
labweights  = [1.0, 1.0,   1.0,   1.0, 1.0, 1.0] # Weights for loss function
labscale    = [1.0, 100.0, 100.0, 1.0, 1.0, 1.0] # scale labels before plotting
labunits    = ["cosd(°)", "%", "%", "ms/ms", "ms/ms", "log(um)"] # label units, after scaling
# labnames    = ["log(TE*K)", "cosd(alpha)", "gratio", "mwf", "T2mw/TE", "T2iew/TE", "iwf", "ewf", "iewf", "T2iw/TE", "T2ew/TE", "T1mw/TE", "T1iw/TE", "T1ew/TE", "T1iew/TE"]
# labinfer    = ["log(TE*K)"]
# labweights  = [1.0] # Weights for loss function
# labscale    = [1.0] # scale labels before plotting
# labunits    = ["log(um)"] # label units, after scaling
[data.filter]
labnames    = [ "nTE" ] # "K"
lower       = [  32.0 ] # 0.00
upper       = [  64.0 ] # 1.00
[data.preprocess]
shuffle     = true
SNR         = [0.0] # Gaussian noise on complex signal (0 == no noise)
[data.preprocess.chunk]
apply       = true
size        = 32
[data.postprocess]
SNR         = 100.0 # Rician noise on magnitude signal (0 == no noise)

[model]
loss      = "l2"
acc       = "rmse"
# [model.load]
# path    = "/home/jdoucette/Documents/code/BlochTorreyResults/Experiments/MyelinWaterLearning/ismrm2020/pretrained-dense-model-with-perm/2019-11-01-T-16-12-14-111.acc=rmse_loss=l2_DenseLIGOCVAE_Ndense1=128_Ndense2=128_Ndense3=128_Ndense4=128_Ndense5=128_Ndense6=128_Xout=6_Zdim=20_act=leakyrelu.model-checkpoint.bson"
[model.DenseLIGOCVAE]
# Xout    = 1 # Number of learned labels
# Xout    = 5 # Number of learned labels
Xout      = 6 # Number of learned labels
Zdim      = 20 # Latent variable dimension
Ndense    = [32, 32] # Sizes of inner dense layers
# Ndense    = [128, 128, 128, 128] # Sizes of inner dense layers
# Ndense    = [128, 128, 128, 128, 128, 128] # Sizes of inner dense layers
act       = "leakyrelu"
# [model.ConvLIGOCVAE]
# Xout      = 6  # Number of learned labels
# Zdim      = 15 # Latent variable dimension
# Nfeat     = 8  # Number of convolutional channels
# Ndown     = 1  # Optional striding for downsampling
# act       = "leakyrelu"
# [model.RDNLIGOCVAE]
# Xout      = 6  # Number of learned labels
# Zdim      = 20 # Latent variable dimension
# Nrdn      = 4  # Number of RDN + downsampling blocks
# Ncat      = 2  # Number of concat blocks within RDN
# Nfeat     = 16 # Number of convolutional channels + RDN growth rate
# act       = "leakyrelu"

[optimizer]
epochs  = 1_000_000
savemod = 1
testmod = 1
[optimizer.ADAM]
lr      = 1e-5
beta    = [0.9, 0.999]
decay   = 1e-5
[optimizer.SGD]
lr      = 1e-5
rho     = 0.9
decay   = 1e-5
